我们来**从头到尾、不省略任何步骤地完整推导**马尔可夫模型中过滤（filtering）的核心公式：

$$
P(X_{t+1} | e_{1:t+1}) = \alpha \cdot P(e_{t+1} | X_{t+1}) \cdot \sum_{x_t} P(X_{t+1} | x_t) \cdot P(x_t | e_{1:t})
$$

---

### 🎯 目标：
推导出在时间 $ t+1 $ 观测到证据 $ e_{1:t+1} $ 后，系统状态 $ X_{t+1} $ 的后验概率 $ P(X_{t+1} | e_{1:t+1}) $。

---

## 第一步：使用贝叶斯定理（Bayes' Theorem）

我们想要求的是：
$$
P(X_{t+1} | e_{1:t+1})
$$

根据**贝叶斯定理**的基本形式：
$$
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
$$

令：
- $ A = X_{t+1} $
- $ B = e_{1:t+1} $

代入得：
$$
P(X_{t+1} | e_{1:t+1}) = \frac{P(e_{1:t+1} | X_{t+1}) \cdot P(X_{t+1})}{P(e_{1:t+1})}
\tag{1}
$$

但这里的问题是：我们不知道 $ P(X_{t+1}) $ 和 $ P(e_{1:t+1} | X_{t+1}) $ 的具体值。我们需要用**已知的过去信息**来表达这些项。

---

## 第二步：分解联合观测 $ e_{1:t+1} $

注意：$ e_{1:t+1} = e_{1:t} \cup e_{t+1} $，即全部观测分为两部分：
- 过去的观测 $ e_{1:t} $
- 当前的新观测 $ e_{t+1} $

我们可以将联合条件概率 $ P(e_{1:t+1} | X_{t+1}) $ 拆开。

利用**条件概率的链式法则（Chain Rule）**：
$$
P(e_{1:t+1} | X_{t+1}) = P(e_{t+1}, e_{1:t} | X_{t+1}) = P(e_{t+1} | e_{1:t}, X_{t+1}) \cdot P(e_{1:t} | X_{t+1})
\tag{2}
$$

这一步是关键！它把一个长序列的观测条件概率拆成了两个部分。

---

## 第三步：引入传感器马尔可夫假设（Sensor Markov Assumption）

这个假设说：
> **当前观测 $ e_{t+1} $ 只依赖于当前状态 $ X_{t+1} $，不依赖于过去的观测或状态。**

数学表达为：
$$
P(e_{t+1} | e_{1:t}, X_{t+1}) = P(e_{t+1} | X_{t+1})
\tag{3}
$$

代入 (2) 式：
$$
P(e_{1:t+1} | X_{t+1}) = P(e_{t+1} | X_{t+1}) \cdot P(e_{1:t} | X_{t+1})
\tag{4}
$$

---

## 第四步：处理 $ P(X_{t+1}) $ 和 $ P(e_{1:t} | X_{t+1}) $

我们现在回到 (1) 式：
$$
P(X_{t+1} | e_{1:t+1}) = \frac{P(e_{1:t+1} | X_{t+1}) \cdot P(X_{t+1})}{P(e_{1:t+1})}
$$

把 (4) 代入分子：
$$
= \frac{P(e_{t+1} | X_{t+1}) \cdot P(e_{1:t} | X_{t+1}) \cdot P(X_{t+1})}{P(e_{1:t+1})}
$$

注意：$ P(e_{1:t} | X_{t+1}) \cdot P(X_{t+1}) = P(e_{1:t}, X_{t+1}) $

所以：
$$
P(X_{t+1} | e_{1:t+1}) = \frac{P(e_{t+1} | X_{t+1}) \cdot P(e_{1:t}, X_{t+1})}{P(e_{1:t+1})}
\tag{5}
$$

---

## 第五步：展开 $ P(e_{1:t}, X_{t+1}) $

使用全概率公式（对 $ x_t $ 求和）：
$$
P(e_{1:t}, X_{t+1}) = \sum_{x_t} P(e_{1:t}, x_t, X_{t+1})
$$

再拆解联合概率：
$$
= \sum_{x_t} P(X_{t+1} | x_t, e_{1:t}) \cdot P(x_t, e_{1:t})
$$

根据**马尔可夫假设**（当前状态只依赖前一状态）：
$$
P(X_{t+1} | x_t, e_{1:t}) = P(X_{t+1} | x_t)
\tag{6}
$$

所以：
$$
P(e_{1:t}, X_{t+1}) = \sum_{x_t} P(X_{t+1} | x_t) \cdot P(x_t, e_{1:t})
$$

而 $ P(x_t, e_{1:t}) = P(x_t | e_{1:t}) \cdot P(e_{1:t}) $，所以：
$$
= \sum_{x_t} P(X_{t+1} | x_t) \cdot P(x_t | e_{1:t}) \cdot P(e_{1:t})
\tag{7}
$$

---

## 第六步：代回原式

把 (7) 代入 (5)：
$$
P(X_{t+1} | e_{1:t+1}) = \frac{P(e_{t+1} | X_{t+1}) \cdot \left[ \sum_{x_t} P(X_{t+1} | x_t) \cdot P(x_t | e_{1:t}) \cdot P(e_{1:t}) \right]}{P(e_{1:t+1})}
$$

注意到 $ P(e_{1:t}) $ 和 $ P(e_{1:t+1}) $ 都不依赖于 $ X_{t+1} $，我们可以把它们合并成一个常数。

令：
$$
\alpha = \frac{P(e_{1:t})}{P(e_{1:t+1})}
$$

但注意：这个 $ \alpha $ 实际上也依赖于 $ e_{1:t+1} $，但它对所有 $ X_{t+1} $ 是相同的，所以可以作为归一化因子。

更准确地说，我们写成：
$$
P(X_{t+1} | e_{1:t+1}) = \alpha \cdot P(e_{t+1} | X_{t+1}) \cdot \sum_{x_t} P(X_{t+1} | x_t) \cdot P(x_t | e_{1:t})
\tag{8}
$$

其中：
$$
\alpha = \frac{1}{\sum_{X_{t+1}} P(e_{t+1} | X_{t+1}) \cdot \sum_{x_t} P(X_{t+1} | x_t) \cdot P(x_t | e_{1:t})}
$$

是为了保证所有状态的概率之和为 1。

---

## 第七步：最终公式整理

我们得到最终的递推公式：

$$
\boxed{P(X_{t+1} | e_{1:t+1}) = \alpha \cdot P(e_{t+1} | X_{t+1}) \cdot \sum_{x_t} P(X_{t+1} | x_t) \cdot P(x_t | e_{1:t})}}
$$

其中：
- $ P(x_t | e_{1:t}) $：上一时刻的过滤结果（已知）
- $ \sum_{x_t} P(X_{t+1} | x_t) \cdot P(x_t | e_{1:t}) $：预测步骤（prediction），得到 $ P(X_{t+1} | e_{1:t}) $
- $ P(e_{t+1} | X_{t+1}) $：传感器模型（likelihood）
- $ \alpha $：归一化常数，确保概率和为 1

---

## 🔁 总结：每一步的意义

| 步骤 | 操作 | 目的 |
|------|------|------|
| 1 | 贝叶斯定理 | 把后验概率表示为似然 × 先验 / 证据 |
| 2 | 链式法则 | 拆分观测序列 |
| 3 | 传感器马尔可夫假设 | 简化新观测的条件概率 |
| 4 | 全概率公式 | 引入前一状态 $ x_t $ |
| 5 | 马尔可夫假设 | 简化状态转移 |
| 6 | 归一化 | 引入 $ \alpha $，使结果为合法概率分布 |

---

✅ **一句话总结**：  
这个公式是通过**贝叶斯定理 + 马尔可夫假设 + 链式法则 + 全概率公式**，一步步推导出的**递归更新规则**，用于在时间序列中不断更新状态的信念。

