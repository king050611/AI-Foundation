这份完整的学习笔记基于阿伯丁大学 JC3001 课程 **Lecture 41, 42, 43** 的全套内容整合而成。

笔记涵盖了从**神经元基础**到**感知机训练手算**，再到**多层网络的前向/反向传播**的所有核心知识点。

---

# 📘 人工智能基础：神经网络完整学习笔记
**课程：** JC3001 (Lecture 41-43)
**主题：** Neural Networks (Foundation, Training, Deep Learning)

---

## 🟢 第一部分：基础概念与单层感知机 (Lecture 41)

### 1. 什么是人工神经网络 (ANN)?
*   **定位：** 属于机器学习中的**监督学习 (Supervised Learning)**。
*   **灵感：** 模仿生物大脑的结构，但请注意，这只是**数学模型**，不要过度神化生物学类比。
    *   **输入 ($x$):** 对应树突 (Dendrites)，接收信号。
    *   **权重 ($w$):** 对应突触 (Synapse)，决定信号的重要性。
    *   **激活 ($f$):** 对应细胞体/轴突，决定是否“点火”输出。

### 2. 感知机 (Perceptron) 模型
这是最简单的神经元结构。

**核心公式：**
$$ h_\theta(x) = \text{Activation}(\sum_{i=0}^{n} w_i x_i) $$

*   **线性部分 (Linear Combination):**
    $$ z = \underbrace{w_0 \cdot 1}_{\text{偏置}} + w_1 x_1 + w_2 x_2 + \dots $$
*   **非线性部分 (Activation):**
    *   Lecture 41 使用**阶跃函数 (Step Function)**：
    *   若 $z \ge \text{阈值} \sigma$，输出 1。
    *   若 $z < \text{阈值} \sigma$，输出 0。

### 3. 感知机能解决什么？(逻辑门)
我们可以通过“人肉”设定权重来实现逻辑运算：

*   **AND 门 (与):** 只有全为1才输出1。
    *   权重: $0.5, 0.5$, 阈值: $0.75$。
*   **OR 门 (或):** 只要有1就输出1。
    *   权重: $1, 1$, 阈值: $0.5$。
*   **NOT 门 (非):** 输入1变0。
    *   权重: $-1$, 阈值: $0$。

### 4. ❌ 致命弱点：XOR (异或) 问题
*   **XOR 定义:** 相同为0，不同为1。
    *   $(0,0) \to 0$
    *   $(1,1) \to 0$
    *   $(0,1) \to 1$
    *   $(1,0) \to 1$
*   **结论:** 单个感知机画出的决策边界永远是**直线**（线性）。XOR 是非线性问题，单层感知机**无法解决**。这引出了多层网络的需求。

---

## 🟡 第二部分：感知机训练算法 (Lecture 42)

如果我们不知道权重是多少，如何让机器自动学习？答案是：**试错法 (Error-Correction Learning)**。

### 1. 训练设定
*   **标签变化:** 为了计算方便，Lecture 42 将输出定义为 **$\{+1, -1\}$**。
*   **预测规则:**
    *   $Sum \ge 0 \rightarrow \text{预测 } +1$
    *   $Sum < 0 \rightarrow \text{预测 } -1$

### 2. ⚡ 核心公式：权重更新规则 (必背)
**仅当预测错误时**，才更新权重。

$$ w_{new} = w_{old} + \Delta w $$
$$ \Delta w = \alpha \times (y_{真实} - \hat{y}_{预测}) \times x_{输入} $$

*   **$\alpha$ (学习率):** 控制步长 (例如 0.4)。
*   **$(y - \hat{y})$ (误差项):**
    *   **误报 (Target -1, Predict +1):** 误差 = $-2$ $\rightarrow$ 权重减小。
    *   **漏报 (Target +1, Predict -1):** 误差 = $+2$ $\rightarrow$ 权重增加。
    *   **正确:** 误差 = $0$ $\rightarrow$ 权重不变。

### 3. 📝 手算训练全过程 (考试重点)

**初始条件:**
*   权重 $W = [-0.5, 0.4, -0.6, 0.6]$ (对应 $w_0, w_1, w_2, w_3$)。
*   注意：$w_0$ 是偏置，对应输入恒为 **1**。
*   学习率 $\alpha = 0.4$。

#### 🔹 样本 1 训练
*   **输入:** $x=[1, 0, 0, 1]$ (第1位是偏置输入), **目标:** $y=-1$
*   **Step 1: 预测**
    $$ z = (-0.5 \times 1) + (0.4 \times 0) + (-0.6 \times 0) + (0.6 \times 1) $$
    $$ z = -0.5 + 0 + 0 + 0.6 = \mathbf{0.1} $$
    因为 $0.1 \ge 0$，机器预测 **$\hat{y} = +1$**。
*   **Step 2: 找错**
    $$ \text{Error} = y - \hat{y} = -1 - 1 = \mathbf{-2} $$
    (预测太大，需要把权重调小)
*   **Step 3: 更新** (公式: $\Delta w = 0.4 \times (-2) \times x_i = -0.8 \times x_i$)
    *   $w_0$: $-0.5 + (-0.8 \times 1) = \mathbf{-1.3}$ **(偏置被惩罚)**
    *   $w_1$: $0.4 + (-0.8 \times 0) = \mathbf{0.4}$ (输入是0，不背锅)
    *   $w_2$: $-0.6 + (-0.8 \times 0) = \mathbf{-0.6}$ (输入是0，不背锅)
    *   $w_3$: $0.6 + (-0.8 \times 1) = \mathbf{-0.2}$ **(被惩罚)**
    *   **当前新权重:** $[-1.3, 0.4, -0.6, -0.2]$

#### 🔹 样本 2 训练
*   **输入:** $x=[1, 1, 1, 0]$, **目标:** $y=+1$
*   **Step 1: 预测 (用刚才的新权重)**
    $$ z = (-1.3 \times 1) + (0.4 \times 1) + (-0.6 \times 1) + (-0.2 \times 0) $$
    $$ z = -1.3 + 0.4 - 0.6 = \mathbf{-1.5} $$
    因为 $-1.5 < 0$，机器预测 **$\hat{y} = -1$**。
*   **Step 2: 找错**
    $$ \text{Error} = y - \hat{y} = 1 - (-1) = \mathbf{+2} $$
    (预测太小，需要把权重调大)
*   **Step 3: 更新** (公式: $\Delta w = 0.4 \times (2) \times x_i = 0.8 \times x_i$)
    *   $w_0$: $-1.3 + (0.8 \times 1) = \mathbf{-0.5}$
    *   $w_1$: $0.4 + (0.8 \times 1) = \mathbf{1.2}$
    *   $w_2$: $-0.6 + (0.8 \times 1) = \mathbf{0.2}$
    *   $w_3$: $-0.2 + (0.8 \times 0) = \mathbf{-0.2}$
    *   **最终权重:** $[-0.5, 1.2, 0.2, -0.2]$

---

## 🟠 第三部分：多层感知机与深度学习 (Lecture 43)

为了解决 XOR 等复杂非线性问题，我们将多个神经元连接成网，并引入**隐藏层 (Hidden Layer)**。

### 1. 架构与激活函数
*   **架构:** 输入层 $\to$ 隐藏层 $\to$ 输出层。
*   **激活函数:** 不再使用生硬的阶跃函数，而是使用平滑可导的 **Sigmoid 函数**:
    $$ g(z) = \frac{1}{1+e^{-z}} $$
    这使得我们可以使用微积分来计算梯度。

### 2. 📝 前向传播手算 (Forward Propagation Example)
**场景:** 2层网络，输入 $[x_1, x_2] = [1, 0.8]$。
**权重矩阵 $W^{(1)}$ (输入$\to$隐藏):** $\begin{bmatrix} -0.5 & 0.2 & 0.4 \\ 0.1 & -0.3 & 0.8 \end{bmatrix}$
**权重矩阵 $W^{(2)}$ (隐藏$\to$输出):** $\begin{bmatrix} -0.2 & 0.6 & 0.5 \end{bmatrix}$

**Step 1: 计算隐藏层 ($a^{(2)}$)**
*   **神经元 1:**
    $z_1 = (-0.5 \times 1) + (0.2 \times 1) + (0.4 \times 0.8) = 0.02$
    $a_1 = \text{sigmoid}(0.02) \approx \mathbf{0.505}$
*   **神经元 2:**
    $z_2 = (0.1 \times 1) + (-0.3 \times 1) + (0.8 \times 0.8) = 0.44$
    $a_2 = \text{sigmoid}(0.44) \approx \mathbf{0.608}$
*   **添加偏置:** $a^{(2)} = [1, 0.505, 0.608]^T$

**Step 2: 计算输出层 ($h(x)$)**
*   $z_{out} = (-0.2 \times 1) + (0.6 \times 0.505) + (0.5 \times 0.608) = \mathbf{0.407}$
*   **最终预测:** $h(x) = \text{sigmoid}(0.407) \approx \mathbf{0.60}$

### 3. 代价函数 (Cost Function)
衡量整个网络的误差。公式基于逻辑回归的交叉熵：
$$ Loss = -\left[ y \log(h(x)) + (1-y) \log(1-h(x)) \right] $$

**手算 Loss (若真实 $y=1$):**
$$ Loss = -[1 \cdot \ln(0.60) + 0] \approx -(-0.51) = \mathbf{0.51} $$

### 4. 反向传播 (Backpropagation)
这是多层网络训练的核心。
*   **思想:** 既然我们知道了 Loss (0.51)，我们需要知道**每一个权重对这个错误贡献了多少**。
*   **方法:** 链式法则 (Chain Rule)。
    1.  算输出层误差 $\delta^{(L)} = a^{(L)} - y$。
    2.  利用权重矩阵 $W$ 和导数 $g'(z)$ 把误差往回推：$\delta^{(l)} = (W^{(l)})^T \delta^{(l+1)} \cdot g'(z^{(l)})$。
    3.  根据误差算梯度，更新权重。

### 5. 训练中的挑战
*   **过拟合 (Overfitting):** 模型死记硬背。
    *   *解法:* 正则化 (Regularization, 惩罚大权重), Dropout, 早停。
*   **局部最优 (Local Minima):** 卡在半山腰。
    *   *解法:* 随机梯度下降 (SGD)。
*   **梯度消失:** 层数太深时，误差传不回去。

---

## 🟣 第四部分：深度学习架构概览
当层数变多，就叫深度学习。不同的结构处理不同的数据：
*   **MLP (多层感知机):** 通用，处理表格数据。
*   **CNN (卷积神经网络):** 处理**图片** (Image)。
*   **RNN/LSTM/GRU:** 处理**序列/时间**数据 (Text, Audio)。
*   **Transformers:** 现代自然语言处理 (NLP)。

---

## ⚡ 极速复习 Cheat Sheet (必考公式)

1.  **偏置 (Bias):** 永远记作 $w_0$，其输入 $x_0$ 永远是 **1**。
2.  **感知机更新公式:**
    $$ w = w + \alpha(y_{true} - y_{pred})x $$
    *(口诀：错哪改哪，猜大了减，猜小了加)*
3.  **Sigmoid 函数:**
    $$ g(z) = \frac{1}{1+e^{-z}} $$
4.  **前向传播逻辑:**
    $X \xrightarrow{W^{(1)}} \text{Hidden} \xrightarrow{\text{Sigmoid}} \xrightarrow{W^{(2)}} \text{Output} \xrightarrow{\text{Sigmoid}} \text{Prediction}$
5.  **反向传播逻辑:**
    $\text{Loss} \xrightarrow{\text{Gradient}} \text{Output Error} \xrightarrow{W^T} \text{Hidden Error} \xrightarrow{\text{Update}} W$