å¥½çš„ï¼Œè¿™æ˜¯æ•´åˆäº†æ‰€æœ‰ä¼ªä»£ç å¹¶ç¡®ä¿æ ¼å¼æ•´é½ã€æ³¨é‡Šæ¸…æ™°çš„å®Œæ•´ç‰ˆ Markdown ç¬”è®°ã€‚

# Artificial Intelligence Foundation - Course Notes
# äººå·¥æ™ºèƒ½åŸºç¡€ - è¯¾ç¨‹ç¬”è®°

---

## Lecture 1-3: Intelligent Agents
## è®²åº§ 1-3ï¼šæ™ºèƒ½ä½“

### Definition of Agent
### æ™ºèƒ½ä½“å®šä¹‰
- An agent is a computer system that is capable of **independent action** on behalf of its user or owner. / æ™ºèƒ½ä½“æ˜¯ä¸€ä¸ªèƒ½å¤Ÿä»£è¡¨å…¶ç”¨æˆ·æˆ–æ‰€æœ‰è€…è¿›è¡Œ**ç‹¬ç«‹è¡ŒåŠ¨**çš„è®¡ç®—æœºç³»ç»Ÿã€‚
- It figures out what needs to be done to satisfy design objectives, rather than constantly being told. / å®ƒèƒ½è‡ªè¡Œåˆ¤æ–­éœ€è¦åšä»€ä¹ˆæ¥æ»¡è¶³è®¾è®¡ç›®æ ‡ï¼Œè€Œä¸æ˜¯ä¸€ç›´è¢«æŒ‡ä»¤ã€‚

### Multi-Agent Systems (MAS)
### å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (MAS)
- A multiagent system consists of a number of agents which **interact with one-another**. / å¤šæ™ºèƒ½ä½“ç³»ç»Ÿç”±å¤šä¸ªç›¸äº’**äº¤äº’**çš„æ™ºèƒ½ä½“ç»„æˆã€‚
- Agents act on behalf of users with **different goals and motivations**. / æ™ºèƒ½ä½“ä»£è¡¨å…·æœ‰**ä¸åŒç›®æ ‡å’ŒåŠ¨æœº**çš„ç”¨æˆ·è¡Œäº‹ã€‚
- To successfully interact, they require the ability to **cooperate, coordinate, and negotiate**. / ä¸ºäº†æˆåŠŸäº¤äº’ï¼Œå®ƒä»¬éœ€è¦å…·å¤‡**åˆä½œã€åè°ƒå’Œåå•†**çš„èƒ½åŠ›ã€‚

#### Two Key Problems in MAS
#### å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„ä¸¤ä¸ªå…³é”®é—®é¢˜
1.  **Agent Design**: How to build agents capable of independent, autonomous action? / **æ™ºèƒ½ä½“è®¾è®¡**ï¼šå¦‚ä½•æ„å»ºèƒ½å¤Ÿç‹¬ç«‹è‡ªä¸»è¡ŒåŠ¨çš„æ™ºèƒ½ä½“ï¼Ÿ
2.  **Society Design**: How to build agents capable of interacting with other agents, especially when they don't share the same interests/goals? / **ç¤¾ä¼šè®¾è®¡**ï¼šå¦‚ä½•æ„å»ºèƒ½å¤Ÿä¸å…¶ä»–æ™ºèƒ½ä½“äº¤äº’çš„æ™ºèƒ½ä½“ï¼Œç‰¹åˆ«æ˜¯åœ¨ç›®æ ‡åˆ©ç›Šä¸ä¸€è‡´æ—¶ï¼Ÿ

### Intelligent Agent
### æ™ºèƒ½ä½“
- Agents are **autonomous**: capable of acting independently and exhibiting control over their internal state. / æ™ºèƒ½ä½“æ˜¯**è‡ªä¸»çš„**ï¼šèƒ½å¤Ÿç‹¬ç«‹è¡ŒåŠ¨å¹¶æ§åˆ¶å…¶å†…éƒ¨çŠ¶æ€ã€‚
- Typically exhibit three types of behaviour: / é€šå¸¸è¡¨ç°å‡ºä¸‰ç§è¡Œä¸ºï¼š
    - **Reactive**: Responds to changes in the environment. / **ååº”å¼**ï¼šå¯¹ç¯å¢ƒå˜åŒ–åšå‡ºååº”ã€‚
    - **Pro-active**: Takes initiative to pursue goals. / **ä¸»åŠ¨å¼**ï¼šä¸»åŠ¨è¿½æ±‚ç›®æ ‡ã€‚
    - **Social**: Interacts and cooperates with other agents/humans. / **ç¤¾ä¼šæ€§**ï¼šä¸å…¶ä»–æ™ºèƒ½ä½“/äººäº¤äº’åˆä½œã€‚

#### Agents vs. Objects
#### æ™ºèƒ½ä½“ vs. å¯¹è±¡
| Feature / ç‰¹æ€§        | Objects / å¯¹è±¡                                      | Agents / æ™ºèƒ½ä½“                                     |
| :-------------------- | :-------------------------------------------------- | :-------------------------------------------------- |
| **Autonomy** <br>è‡ªä¸»æ€§   | Lower autonomy <br>è‡ªä¸»æ€§è¾ƒä½                         | **High autonomy**, decides on actions <br>**é«˜è‡ªä¸»æ€§**ï¼Œè‡ªè¡Œå†³å®šè¡ŒåŠ¨ |
| **Intelligence** <br>æ™ºèƒ½ | Encapsulates state <br>å°è£…çŠ¶æ€                       | **Flexible behavior** (reactive, proactive, social) <br>**çµæ´»è¡Œä¸º**ï¼ˆååº”ã€ä¸»åŠ¨ã€ç¤¾äº¤ï¼‰ |
| **Initiative** <br>ä¸»åŠ¨æ€§ | Passive service providers <br>è¢«åŠ¨æœåŠ¡æä¾›è€…         | **Proactive**, not just passive <br>**ä¸»åŠ¨**ï¼Œè€Œéä»…ä»…è¢«åŠ¨ |

### Environment Types & Dimensions
### ç¯å¢ƒç±»å‹ä¸ç»´åº¦
| Dimension <br>ç»´åº¦        | Type 1 <br>ç±»å‹1                          | Type 2 <br>ç±»å‹2                              |
| :------------------------ | :---------------------------------------- | :-------------------------------------------- |
| **Observability** <br>å¯è§‚æµ‹æ€§ | Accessible/Fully Observable <br>å¯è®¿é—®/å®Œå…¨å¯è§‚æµ‹ | Inaccessible/Partially Observable <br>ä¸å¯è®¿é—®/éƒ¨åˆ†å¯è§‚æµ‹ |
| **Determinism** <br>ç¡®å®šæ€§   | Deterministic <br>ç¡®å®šæ€§                  | Non-deterministic <br>éç¡®å®šæ€§                |
| **Dynamicity** <br>åŠ¨æ€æ€§    | Static <br>é™æ€                          | Dynamic <br>åŠ¨æ€                              |
| **Discreteness** <br>ç¦»æ•£æ€§  | Discrete <br>ç¦»æ•£                        | Continuous <br>è¿ç»­                           |
| **Episodic** <br>æƒ…æ™¯æ€§      | Sequential <br>åºåˆ—å¼                    | Episodic <br>æƒ…æ™¯å¼                           |

---

## Lecture 4: Search I â€“ Uninformed Search
## è®²åº§ 4ï¼šæœç´¢ I â€“ æ— ä¿¡æ¯æœç´¢

### Problem-Solving Agent
### é—®é¢˜æ±‚è§£æ™ºèƒ½ä½“
1.  Update state based on percept and previous action. / æ ¹æ®æ„ŸçŸ¥å’Œå…ˆå‰åŠ¨ä½œæ›´æ–°çŠ¶æ€ã€‚
2.  If no current plan: / è‹¥å½“å‰æ— è®¡åˆ’ï¼š
    - Formulate a goal. / åˆ¶å®šç›®æ ‡ã€‚
    - Define the problem. / å®šä¹‰é—®é¢˜ã€‚
    - Search for a solution. / æœç´¢è§£å†³æ–¹æ¡ˆã€‚
3.  Execute the first action in the plan. / æ‰§è¡Œè®¡åˆ’ä¸­çš„ç¬¬ä¸€ä¸ªåŠ¨ä½œã€‚

### Problem Formulation
### é—®é¢˜å½¢å¼åŒ–
A problem is defined by five components: / é—®é¢˜ç”±äº”ä¸ªè¦ç´ å®šä¹‰ï¼š
1.  **Initial State** / **åˆå§‹çŠ¶æ€**
2.  **Actions** / **åŠ¨ä½œé›†åˆ**
3.  **Successor Function** / **åç»§å‡½æ•°**
4.  **Goal Test** / **ç›®æ ‡æµ‹è¯•**
5.  **Path Cost** / **è·¯å¾„ä»£ä»·**

### Tree Search Algorithm (Generic Framework)
### æ ‘æœç´¢ç®—æ³•ï¼ˆé€šç”¨æ¡†æ¶ï¼‰
```python
# Initialize the frontier using the initial state of the problem
# ä½¿ç”¨é—®é¢˜çš„åˆå§‹çŠ¶æ€åˆå§‹åŒ–å‰æ²¿
frontier = [INITIAL-STATE]

while frontier is not empty:
    # Choose a node from the frontier based on the search strategy (BFS, DFS, etc.)
    # æ ¹æ®æœç´¢ç­–ç•¥ï¼ˆBFSã€DFSç­‰ï¼‰ä»å‰æ²¿ä¸­é€‰æ‹©ä¸€ä¸ªèŠ‚ç‚¹
    node = frontier.remove_choice()

    # If the node contains a goal state, return the solution
    # å¦‚æœèŠ‚ç‚¹åŒ…å«ç›®æ ‡çŠ¶æ€ï¼Œè¿”å›è§£å†³æ–¹æ¡ˆ
    if node.STATE is goal:
        return SOLUTION(node)

    # Otherwise, expand the node (generate all successors) and add them to the frontier
    # å¦åˆ™ï¼Œæ‰©å±•è¯¥èŠ‚ç‚¹ï¼ˆç”Ÿæˆæ‰€æœ‰åç»§èŠ‚ç‚¹ï¼‰å¹¶å°†å…¶æ·»åŠ åˆ°å‰æ²¿
    for each successor in EXPAND(node, problem):
        frontier.add(successor)

# If the frontier is empty and no solution is found, return failure
# å¦‚æœå‰æ²¿ä¸ºç©ºä¸”æœªæ‰¾åˆ°è§£ï¼Œè¿”å›å¤±è´¥
return failure
```

### Search Strategies Evaluation
### æœç´¢ç­–ç•¥è¯„ä¼°æ ‡å‡†
| Criterion <br>æ ‡å‡†        | Description <br>æè¿°                                                                               |
| :------------------------ | :------------------------------------------------------------------------------------------------- |
| **Completeness** <br>å®Œå¤‡æ€§ | Guaranteed to find a solution if one exists? <br>æ˜¯å¦èƒ½ä¿è¯æ‰¾åˆ°è§£ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Ÿ                      |
| **Optimality** <br>æœ€ä¼˜æ€§   | Guaranteed to find the least-cost solution? <br>æ˜¯å¦èƒ½ä¿è¯æ‰¾åˆ°æœ€ä½ä»£ä»·çš„è§£ï¼Ÿ                         |
| **Time Complexity** <br>æ—¶é—´å¤æ‚åº¦ | Number of nodes generated/expanded (often using \(b\), \(d\), \(m\)). <br>ç”Ÿæˆ/æ‰©å±•çš„èŠ‚ç‚¹æ•°ï¼ˆå¸¸ç”¨åˆ†æ”¯å› å­b, è§£æ·±åº¦d, æœ€å¤§æ·±åº¦mï¼‰ã€‚ |
| **Space Complexity** <br>ç©ºé—´å¤æ‚åº¦ | Maximum number of nodes stored in memory. <br>å†…å­˜ä¸­å­˜å‚¨çš„æœ€å¤§èŠ‚ç‚¹æ•°ã€‚                             |

### Uninformed Search Algorithms Summary
### æ— ä¿¡æ¯æœç´¢ç®—æ³•æ€»ç»“
| Algorithm <br>ç®—æ³•       | Completeness <br>å®Œå¤‡æ€§ | Optimality <br>æœ€ä¼˜æ€§ | Time Complexity <br>æ—¶é—´å¤æ‚åº¦ | Space Complexity <br>ç©ºé—´å¤æ‚åº¦ | Notes <br>å¤‡æ³¨                                 |
| :----------------------- | :---------------------- | :-------------------- | :----------------------------- | :------------------------------ | :--------------------------------------------- |
| **BFS** <br>å¹¿åº¦ä¼˜å…ˆæœç´¢   | âœ… Yes                    | âœ… Yes*                | \(O(b^d)\)                     | \(O(b^d)\)                      | \*If step costs are identical <br>\*å‡è®¾æ¯æ­¥ä»£ä»·ç›¸åŒ |
| **UCS** <br>ä¸€è‡´ä»£ä»·æœç´¢   | âœ… Yes                    | âœ… Yes                 | \(O(b^{1 + \lfloor C^*/\epsilon \rfloor})\) | Same as Time <br>åŒæ—¶é—´å¤æ‚åº¦    | Optimal for varying step costs <br>é€‚ç”¨äºä¸åŒä»£ä»·çš„æƒ…å†µ |
| **DFS** <br>æ·±åº¦ä¼˜å…ˆæœç´¢   | âŒ No                     | âŒ No                  | \(O(b^m)\)                     | \(O(bm)\)                       | Low memory, but not optimal <br>å†…å­˜éœ€æ±‚ä½ï¼Œä½†ä¸ä¿è¯æœ€ä¼˜ |
| **DLS** <br>æ·±åº¦å—é™æœç´¢   | âŒ No (if \(L < d\))      | âŒ No                  | \(O(b^L)\)                     | \(O(bL)\)                       | Requires depth limit \(L\) <br>éœ€è¦æ·±åº¦é™åˆ¶L     |
| **IDS** <br>è¿­ä»£åŠ æ·±æœç´¢   | âœ… Yes                    | âœ… Yes*                | \(O(b^d)\)                     | \(O(bd)\)                       | Combines benefits of BFS & DFS <br>ç»“åˆBFSå’ŒDFSçš„ä¼˜ç‚¹ |

---

## Lecture 7: Search II â€“ Informed Search II
## è®²åº§ 7ï¼šæœç´¢ II â€“ æœ‰ä¿¡æ¯æœç´¢ II

### A* Search Algorithm (Graph Search Version)
### A* æœç´¢ç®—æ³•ï¼ˆå›¾æœç´¢ç‰ˆæœ¬ï¼‰
*å‡è®¾å¯å‘å‡½æ•° h(n) æ˜¯ä¸€è‡´çš„ï¼Œä¿è¯æœ€ä¼˜æ€§ã€‚å¦‚æœ h(n) ä¸ä¸€è‡´ï¼Œå¯èƒ½éœ€è¦é‡æ–°æ‰“å¼€å·²å…³é—­èŠ‚ç‚¹ã€‚ / Assumes heuristic h(n) is consistent, guaranteeing optimality. If h(n) is inconsistent, reopening closed nodes might be needed.*
```text
function A*-GRAPH-SEARCH(problem, h):
    // Initialize: Create the start node and frontier (priority queue by f = g + h)
    // åˆå§‹åŒ–ï¼šåˆ›å»ºèµ·å§‹èŠ‚ç‚¹å’Œå‰æ²¿ï¼ˆæŒ‰ f = g + h æ’åºçš„ä¼˜å…ˆé˜Ÿåˆ—ï¼‰
    node = a node with STATE = problem.INITIAL-STATE, PATH-COST = 0, PARENT = null
    frontier = a priority queue ordered by f = g + h, with node as the only element
    // 'reached' tracks the best g-cost found so far for each state
    // 'reached' è®°å½•åˆ°ç›®å‰ä¸ºæ­¢æ‰¾åˆ°çš„æ¯ä¸ªçŠ¶æ€çš„æœ€ä½³ g ä»£ä»·
    reached = a lookup table; key=state, value=lowest g-cost found so far
    Add node.STATE to reached with value node.PATH-COST

    while frontier is not empty:
        node = frontier.pop() // Remove node with smallest f-value / ç§»é™¤å…·æœ‰æœ€å° f å€¼çš„èŠ‚ç‚¹
        if problem.GOAL-TEST(node.STATE):
            return SOLUTION(node) // Reconstruct path by following parent pointers / é€šè¿‡çˆ¶æŒ‡é’ˆå›æº¯è·¯å¾„
        for each child in EXPAND(problem, node): // Generate all successors / ç”Ÿæˆæ‰€æœ‰åç»§èŠ‚ç‚¹
            s = child.STATE
            g_child = node.PATH-COST + problem.ACTION-COST(node.STATE, child.ACTION, s)
            // Check if this is a new state or a cheaper path to a known state
            // æ£€æŸ¥è¿™æ˜¯æ–°çŠ¶æ€è¿˜æ˜¯åˆ°å·²çŸ¥çŠ¶æ€çš„æ›´ä¾¿å®œè·¯å¾„
            if s is not in reached or g_child < reached[s]:
                reached[s] = g_child // Update best g-cost for this state / æ›´æ–°è¯¥çŠ¶æ€çš„æœ€ä½³ g ä»£ä»·
                child.PATH-COST = g_child
                child.PARENT = node // Essential for path reconstruction / è·¯å¾„å›æº¯æ‰€å¿…éœ€
                f_child = g_child + h(s)
                add child to frontier with priority f_child
    return failure
```

### Iterative Deepening A* (IDA*) Algorithm
### è¿­ä»£åŠ æ·± A* (IDA*) ç®—æ³•
```text
function IDA*(problem, h):
    root = a node with STATE = problem.INITIAL-STATE, g = 0
    f_limit = h(root.STATE) // Initial f-cost limit / åˆå§‹ f ä»£ä»·é™åˆ¶

    while true:
        // Perform a depth-first search within the current f-cost contour
        // åœ¨å½“å‰ f ä»£ä»·ç­‰é«˜çº¿å†…æ‰§è¡Œæ·±åº¦ä¼˜å…ˆæœç´¢
        solution, new_f_limit = CONTOUR-SEARCH(root, problem, h, f_limit)
        if solution is not failure:
            return solution
        if new_f_limit is infinity: // No nodes exceeded the current limit? No solution exists.
            return failure           // æ²¡æœ‰èŠ‚ç‚¹è¶…è¿‡å½“å‰é™åˆ¶ï¼Ÿåˆ™æ— è§£ã€‚
        f_limit = new_f_limit // Increase the f-cost limit for the next iteration / å¢åŠ  f ä»£ä»·é™åˆ¶ä»¥è¿›è¡Œä¸‹ä¸€æ¬¡è¿­ä»£

function CONTOUR-SEARCH(node, problem, h, f_limit):
    f_node = node.g + h(node.STATE)
    if f_node > f_limit:
        return (failure, f_node) // Return failure and the exceeded f-value / è¿”å›å¤±è´¥å’Œè¶…å‡ºçš„ f å€¼
    if problem.GOAL-TEST(node.STATE):
        return (SOLUTION(node), f_limit)
    next_f_limit = infinity // Track the smallest exceedance / è·Ÿè¸ªæœ€å°çš„è¶…å‡ºå€¼
    for each child in EXPAND(problem, node):
        child.g = node.g + problem.ACTION-COST(node.STATE, child.ACTION, child.STATE)
        solution, new_f = CONTOUR-SEARCH(child, problem, h, f_limit) // Recursive DFS / é€’å½’DFS
        if solution is not failure:
            return (solution, f_limit)
        next_f_limit = min(next_f_limit, new_f) // Update the next limit / æ›´æ–°ä¸‹ä¸€æ¬¡çš„é™åˆ¶
    return (failure, next_f_limit)
```

### Heuristic Function Properties
### å¯å‘å¼å‡½æ•°å±æ€§
| Property <br>å±æ€§      | Description <br>æè¿°                                                                 | Importance <br>é‡è¦æ€§                                                          |
| :--------------------- | :----------------------------------------------------------------------------------- | :---------------------------------------------------------------------------- |
| **Admissible** <br>å¯é‡‡çº³æ€§ | Never overestimates the true cost to the goal (\(h(n) \leq h^*(n)\)). <br>ä»ä¸é«˜ä¼°åˆ°è¾¾ç›®æ ‡çš„çœŸå®ä»£ä»·ã€‚ | Guarantees optimality for A* (Tree Search) <br>ä¿è¯A*ï¼ˆæ ‘æœç´¢ï¼‰çš„æœ€ä¼˜æ€§         |
| **Consistent** <br>ä¸€è‡´æ€§ | For every node \(n\) and successor \(n'\): \(h(n) \leq c(n,a,n') + h(n')\). <br>å¯¹ä»»æ„èŠ‚ç‚¹nåŠå…¶åç»§n'ï¼š\(h(n) \leq c(n,a,n') + h(n')\)ã€‚ | Guarantees optimality for A* (Graph Search) <br>ä¿è¯A*ï¼ˆå›¾æœç´¢ï¼‰çš„æœ€ä¼˜æ€§         |
| **Domination** <br>æ”¯é…æ€§ | If \(h_2(n) \geq h_1(n)\) for all \(n\), \(h_2\) dominates \(h_1\) and is better. <br>è‹¥å¯¹æ‰€æœ‰næœ‰\(h_2(n) \geq h_1(n)\)ï¼Œåˆ™h2æ”¯é…h1ä¸”æ›´ä¼˜ã€‚ | Tighter heuristics expand fewer nodes. <br>æ›´ç´§çš„å¯å‘å¼æ‰©å±•çš„èŠ‚ç‚¹æ›´å°‘ã€‚           |

---

## Lecture 8: Search III â€“ Local Search I
## è®²åº§ 8ï¼šæœç´¢ III â€“ å±€éƒ¨æœç´¢ I

### Hill Climbing (Steepest Ascent)
### çˆ¬å±±ç®—æ³•ï¼ˆæœ€é™¡ä¸Šå‡ï¼‰
```text
function HILL-CLIMBING(problem):
    current = problem.INITIAL-STATE
    while true:
        // Find the best among all neighbors (successor states)
        // æ‰¾å‡ºæ‰€æœ‰é‚»å±…ï¼ˆåç»§çŠ¶æ€ï¼‰ä¸­æœ€å¥½çš„ä¸€ä¸ª
        neighbor = a highest-valued successor state of current // "Steepest ascent" / "æœ€é™¡ä¸Šå‡"
        // For minimization problems, choose the lowest-valued successor
        // å¯¹äºæœ€å°åŒ–é—®é¢˜ï¼Œé€‰æ‹©å€¼æœ€ä½çš„åç»§

        // If no neighbor is better than the current state, stop
        // å¦‚æœæ²¡æœ‰é‚»å±…æ¯”å½“å‰çŠ¶æ€æ›´å¥½ï¼Œåˆ™åœæ­¢
        if VALUE(neighbor) <= VALUE(current):
            return current // Return the local optimum / è¿”å›å±€éƒ¨æœ€ä¼˜è§£

        current = neighbor // Move to the better neighbor / ç§»åŠ¨åˆ°æ›´å¥½çš„é‚»å±…
```

### Simulated Annealing
### æ¨¡æ‹Ÿé€€ç«
```text
function SIMULATED-ANNEALING(problem, schedule):
    // schedule(t) returns the "temperature" T at time step t, decreasing over time
    // schedule(t) è¿”å›æ—¶é—´æ­¥ t çš„"æ¸©åº¦" Tï¼Œéšæ—¶é—´é€’å‡
    current = problem.INITIAL-STATE
    for t = 1 to âˆ:
        T = schedule(t)
        if T == 0:
            return current
        // Randomly select a neighbor / éšæœºé€‰æ‹©ä¸€ä¸ªé‚»å±…
        next = a randomly selected successor state of current
        // Calculate the change in value (Î”E). Assume maximization problem.
        // è®¡ç®—å€¼çš„å˜åŒ– (Î”E)ã€‚å‡è®¾æ˜¯æœ€å¤§åŒ–é—®é¢˜ã€‚
        Î”E = VALUE(next) - VALUE(current)

        // Always accept improving moves; probabilistically accept worse moves
        // æ€»æ˜¯æ¥å—æ”¹è¿›çš„ç§»åŠ¨ï¼›ä»¥ä¸€å®šæ¦‚ç‡æ¥å—æ›´å·®çš„ç§»åŠ¨
        if Î”E > 0:
            current = next // Accept improving move / æ¥å—æ”¹è¿›çš„ç§»åŠ¨
        else:
            // Acceptance probability decreases as T decreases and as the move is worse
            // æ¥å—æ¦‚ç‡éšç€ T çš„é™ä½å’Œç§»åŠ¨å˜å·®è€Œå‡å°
            acceptance_probability = exp(Î”E / T) // Note: Î”E is negative / æ³¨æ„ï¼šÎ”E ä¸ºè´Ÿ
            if random(0, 1) < acceptance_probability:
                current = next // Accept worse move probabilistically / ä»¥ä¸€å®šæ¦‚ç‡æ¥å—æ›´å·®ç§»åŠ¨
```

---

## Lecture 9: Search III â€“ Local Search II
## è®²åº§ 9ï¼šæœç´¢ III â€“ å±€éƒ¨æœç´¢ II

### (Local) Beam Search
### ï¼ˆå±€éƒ¨ï¼‰æŸæœç´¢
```text
function BEAM-SEARCH(problem, k): // k is the beam width / k æ˜¯æŸå®½åº¦
    // Initialize population with k states (could be random or just the initial state)
    // ç”¨ k ä¸ªçŠ¶æ€åˆå§‹åŒ–ç§ç¾¤ï¼ˆå¯ä»¥æ˜¯éšæœºçš„ï¼Œæˆ–è€…åªæ˜¯åˆå§‹çŠ¶æ€ï¼‰
    population = [problem.INITIAL-STATE] // Or k random states / æˆ–è€…æ˜¯ k ä¸ªéšæœºçŠ¶æ€

    while true:
        all_candidates = [] // List to store all generated successors / å­˜å‚¨æ‰€æœ‰ç”Ÿæˆçš„åç»§çš„åˆ—è¡¨
        // Generate all successors for each state in the current population
        // ä¸ºå½“å‰ç§ç¾¤ä¸­çš„æ¯ä¸ªçŠ¶æ€ç”Ÿæˆæ‰€æœ‰åç»§
        for each state in population:
            for each successor in SUCCESSORS(state):
                all_candidates.append(successor)

        // Check if any candidate is a solution
        // æ£€æŸ¥æ˜¯å¦æœ‰å€™é€‰è€…æ˜¯è§£
        for each candidate in all_candidates:
            if problem.GOAL-TEST(candidate):
                return candidate

        // If no solution, select the k best candidates based on their VALUE
        // å¦‚æœæ— è§£ï¼Œæ ¹æ® VALUE é€‰æ‹© k ä¸ªæœ€ä½³å€™é€‰è€…
        sort all_candidates by VALUE descending // For maximization / å¯¹äºæœ€å¤§åŒ–é—®é¢˜é™åºæ’åº
        population = the first k states from sorted all_candidates // New population / æ–°çš„ç§ç¾¤
```

### Standard Genetic Algorithm
### æ ‡å‡†é—ä¼ ç®—æ³•
```text
function GENETIC-ALGORITHM(population, FITNESS-FN):
    // population: a set of individuals (e.g., strings representing states)
    // population: ä¸€ç»„ä¸ªä½“ï¼ˆä¾‹å¦‚ï¼Œä»£è¡¨çŠ¶æ€çš„å­—ç¬¦ä¸²ï¼‰
    // FITNESS-FN: a function that measures the fitness of an individual (higher is better)
    // FITNESS-FN: è¡¡é‡ä¸ªä½“é€‚åº”åº¦çš„å‡½æ•°ï¼ˆå€¼è¶Šé«˜è¶Šå¥½ï¼‰

    while termination condition not met (e.g., time, max generations):
        new_population = []
        for i = 1 to SIZE(population):
            // 1. Selection: Choose two parents based on fitness
            // 1. é€‰æ‹©ï¼šæ ¹æ®é€‚åº”åº¦é€‰æ‹©ä¸¤ä¸ªçˆ¶ä»£
            x = RANDOM-SELECTION(population, FITNESS-FN) // e.g., Roulette Wheel / ä¾‹å¦‚è½®ç›˜èµŒ
            y = RANDOM-SELECTION(population, FITNESS-FN)

            // 2. Crossover: With probability P_c, create child by combining x and y
            // 2. äº¤å‰ï¼šä»¥æ¦‚ç‡ P_cï¼Œé€šè¿‡ç»„åˆ x å’Œ y åˆ›å»ºå­ä»£
            child = REPRODUCE(x, y, P_c) // Might return a parent if no crossover / è‹¥æ— äº¤å‰å¯èƒ½è¿”å›ä¸€ä¸ªçˆ¶ä»£

            // 3. Mutation: With small probability P_m, mutate the child
            // 3. å˜å¼‚ï¼šä»¥è¾ƒå°çš„æ¦‚ç‡ P_m å˜å¼‚å­ä»£
            if random(0,1) < P_m:
                child = MUTATE(child)

            new_population.append(child)

        population = new_population

    // Return the best individual found after all generations
    // åœ¨æ‰€æœ‰ä¸–ä»£åè¿”å›æ‰¾åˆ°çš„æœ€ä½³ä¸ªä½“
    return the individual in population with the highest FITNESS-FN(individual)

// Helper function for Roulette Wheel Selection
// è½®ç›˜èµŒé€‰æ‹©çš„è¾…åŠ©å‡½æ•°
function RANDOM-SELECTION(population, FITNESS-FN):
    total_fitness = sum(FITNESS-FN(individual) for individual in population)
    pick = random(0, total_fitness) // Spin the wheel / è½¬åŠ¨è½®ç›˜
    current = 0
    for individual in population:
        current += FITNESS-FN(individual)
        if current > pick:
            return individual // This individual's segment was selected / é€‰æ‹©äº†ä¸ªä½“å¯¹åº”çš„ç‰‡æ®µ

// Helper function for Single-Point Crossover
// å•ç‚¹äº¤å‰çš„è¾…åŠ©å‡½æ•°
function REPRODUCE(x, y, P_c):
    if random(0,1) > P_c: // With probability 1-P_c, no crossover
        return random_choice([x, y])
    n = LENGTH(x)
    c = random integer between 1 and n-1 // Crossover point / äº¤å‰ç‚¹
    child = SUBSTRING(x, 1, c) + SUBSTRING(y, c+1, n) // Combine genetic material / ç»„åˆé—ä¼ ç‰©è´¨
    return child
```

### Local Search Summary
### å±€éƒ¨æœç´¢æ€»ç»“
| Algorithm <br>ç®—æ³•          | Key Idea <br>æ ¸å¿ƒæ€æƒ³                                                                 | Pros & Cons <br>ä¼˜ç¼ºç‚¹                                                      |
| :-------------------------- | :------------------------------------------------------------------------------------ | :-------------------------------------------------------------------------- |
| **Hill Climbing** <br>çˆ¬å±±ç®—æ³•   | Move to the best neighbor. <br>ç§»åŠ¨åˆ°æœ€ä½³é‚»å±…ã€‚                                       | Simple, fast. Gets stuck in local optima. <br>ç®€å•ã€å¿«é€Ÿã€‚æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚     |
| **Simulated Annealing** <br>æ¨¡æ‹Ÿé€€ç« | Allow worse moves initially (probabilistically). <br>åˆå§‹æ—¶ï¼ˆæ¦‚ç‡æ€§åœ°ï¼‰å…è®¸æ›´å·®çš„ç§»åŠ¨ã€‚ | Can escape local optima. Slower. <br>èƒ½é€ƒç¦»å±€éƒ¨æœ€ä¼˜ã€‚è¾ƒæ…¢ã€‚                   |
| **Beam Search** <br>æŸæœç´¢      | Keep k best states at each step. <br>æ¯æ­¥ä¿ç•™kä¸ªæœ€ä½³çŠ¶æ€ã€‚                             | Parallel-like, limited memory. May lack diversity. <br>ç±»å¹¶è¡Œï¼Œå†…å­˜æœ‰é™ã€‚å¯èƒ½ç¼ºä¹å¤šæ ·æ€§ã€‚ |
| **Genetic Algorithms** <br>é—ä¼ ç®—æ³• | Evolve population using selection, crossover, mutation. <br>ä½¿ç”¨é€‰æ‹©ã€äº¤å‰ã€å˜å¼‚è¿›åŒ–ç§ç¾¤ã€‚ | Good for complex spaces. Complex to tune. <br>é€‚ç”¨äºå¤æ‚ç©ºé—´ã€‚å‚æ•°è°ƒæ•´å¤æ‚ã€‚   |

---

## Lecture 10: Search IV: Adversarial Search I
## è®²åº§ 10ï¼šæœç´¢ IVï¼šå¯¹æŠ—æœç´¢ I

### æœ¬è®²æ¦‚è§ˆ (Overview)
*   **æ ¸å¿ƒä¸»é¢˜**ï¼šä»**å•äºº**é—®é¢˜æ±‚è§£è½¬å‘**å¤šäºº**ç¯å¢ƒä¸‹çš„å†³ç­–ï¼Œç‰¹åˆ«æ˜¯**å¯¹æŠ—æ€§ç¯å¢ƒ**ã€‚
*   **ä¸»è¦å†…å®¹**ï¼š
    *   ä»‹ç»**åšå¼ˆ (Games)** ä½œä¸ºå¤šæ™ºèƒ½ä½“ç¯å¢ƒçš„åŸºæœ¬æ¨¡å‹ã€‚
    *   å­¦ä¹ åšå¼ˆçš„ä¸¤ç§æ ‡å‡†è¡¨ç¤ºå½¢å¼ï¼š**æ ‡å‡†å¼ (Normal Form)** å’Œ**æ‰©å±•å¼ (Extensive Form)**ã€‚
    *   é€šè¿‡ç»å…¸ä¾‹å­ï¼ˆå¦‚å›šå¾’å›°å¢ƒã€åè°ƒåšå¼ˆç­‰ï¼‰ç†è§£åšå¼ˆè®ºçš„åŸºæœ¬æ¦‚å¿µã€‚
    *   ä¸ºä¸‹ä¸€è®²å­¦ä¹ å…·ä½“çš„å¯¹æŠ—æœç´¢ç®—æ³•ï¼ˆå¦‚ Minimaxï¼‰æ‰“ä¸‹åŸºç¡€ã€‚

---

### 1. ä»å•æ™ºèƒ½ä½“åˆ°å¤šæ™ºèƒ½ä½“ (From Single-Agent to Multi-Agent)
*   ä¹‹å‰çš„æœç´¢ç®—æ³•ï¼ˆæ— ä¿¡æ¯æœç´¢ã€æœ‰ä¿¡æ¯æœç´¢ã€å±€éƒ¨æœç´¢ï¼‰ä¸»è¦é’ˆå¯¹**å•æ™ºèƒ½ä½“**ç¯å¢ƒï¼Œæ™ºèƒ½ä½“ä¸ç¯å¢ƒäº’åŠ¨ï¼Œç›®æ ‡æ˜¯æ‰¾åˆ°åˆ°è¾¾ç›®æ ‡çŠ¶æ€çš„è·¯å¾„æˆ–æœ€ä¼˜é…ç½®ã€‚
*   åœ¨**å¤šæ™ºèƒ½ä½“ç¯å¢ƒ**ä¸­ï¼š
    *   æ¯ä¸ªæ™ºèƒ½ä½“å¿…é¡»è€ƒè™‘**å…¶ä»–æ™ºèƒ½ä½“çš„è¡Œä¸º**ã€‚
    *   æ™ºèƒ½ä½“ä¹‹é—´å¯èƒ½éœ€è¦**åä½œ (Coordination)** ä»¥ä¿è¯è¡ŒåŠ¨çš„ä¸€è‡´æ€§ã€‚
    *   æ™ºèƒ½ä½“ä¹‹é—´ä¹Ÿå¯èƒ½æ˜¯**ç«äº‰ (Competitive)** å…³ç³»ï¼Œå³ä¸€ä¸ªæ™ºèƒ½ä½“çš„æˆåŠŸå¯èƒ½æ„å‘³ç€å¦ä¸€ä¸ªçš„å¤±è´¥ã€‚è¿™æ˜¯**åšå¼ˆè®º (Game Theory)** ç ”ç©¶çš„æ ¸å¿ƒã€‚

---

### 2. åšå¼ˆ (Games)
#### 2.1 ä»€ä¹ˆæ˜¯åšå¼ˆï¼Ÿ(What is a Game?)
*   åœ¨AIå’Œåšå¼ˆè®ºä¸­ï¼Œâ€œåšå¼ˆâ€æ˜¯ä¸€ä¸ªå¹¿ä¹‰æ¦‚å¿µï¼ŒæŒ‡ä»»ä½•æ¶‰åŠå¤šä¸ªå†³ç­–è€…ï¼ˆç©å®¶ï¼‰äº’åŠ¨çš„åœºæ™¯ã€‚
*   **ä¾‹å­**ï¼š
    *   ä¼ ç»Ÿæ¸¸æˆï¼šè±¡æ£‹ã€å›´æ£‹ã€äº•å­—æ£‹ã€‚
    *   ç°å®åœºæ™¯ï¼šæµ·ç›—æµ·åŸŸçš„æµ·å†›å·¡é€»ã€è­¦åŠ›éƒ¨ç½²ã€é‡‘èå¸‚åœºäº¤æ˜“ã€äººç¾¤ç®¡ç†ã€‚
*   AI æ—©æœŸå°¤å…¶å…³æ³¨**ç¡®å®šæ€§ã€è½®æµè¡ŒåŠ¨ã€åŒäººã€é›¶å’Œã€å®Œå…¨ä¿¡æ¯**çš„åšå¼ˆã€‚
    *   **ç¡®å®šæ€§ (Deterministic)**ï¼šè¡ŒåŠ¨ç»“æœç¡®å®šã€‚
    *   **è½®æµè¡ŒåŠ¨ (Turn-taking)**ï¼šç©å®¶äº¤æ›¿è¡ŒåŠ¨ã€‚
    *   **åŒäºº (Two-player)**ï¼šé€šå¸¸ç®€åŒ–ä¸º MAXï¼ˆæˆ‘æ–¹ï¼‰å’Œ MINï¼ˆå¯¹æ‰‹ï¼‰ã€‚
    *   **é›¶å’Œ (Zero-sum)**ï¼šä¸€æ–¹çš„æ”¶ç›Šç­‰äºå¦ä¸€æ–¹çš„æŸå¤±ï¼Œæ€»æ•ˆç”¨ä¸ºé›¶ã€‚
    *   **å®Œå…¨ä¿¡æ¯ (Perfect Information)**ï¼šæ‰€æœ‰ç©å®¶å¯¹å½“å‰æ¸¸æˆçŠ¶æ€å®Œå…¨äº†è§£ï¼ˆå¦‚è±¡æ£‹ï¼Œæ‰€æœ‰æ£‹å­å¯è§ï¼‰ã€‚

#### 2.2 ä¸ºä»€ä¹ˆç ”ç©¶åšå¼ˆï¼Ÿ(Why Study Games?)
*   åšå¼ˆå¯¹æœç´¢ç®—æ³•çš„**æ•ˆç‡è¦æ±‚æé«˜**ã€‚ä¾‹å¦‚ï¼Œå›½é™…è±¡æ£‹çš„åˆ†æ”¯å› å­çº¦ä¸º35ï¼Œä¸€åœºå¯¹å±€å¯èƒ½æŒç»­50å›åˆï¼Œäº§ç”Ÿçš„æœç´¢æ ‘èŠ‚ç‚¹æ•°é‡å·¨å¤§ï¼ˆ\(35^{100}\)çº§åˆ«ï¼‰ã€‚
*   åœ¨å®æ—¶å¯¹æŠ—ä¸­ï¼Œ**å†³ç­–è¿‡æ…¢ä¼šå¯¼è‡´å¤±è´¥**ã€‚

---

### 3. åšå¼ˆçš„å½¢å¼åŒ–è¡¨ç¤º (Game Formalism)

#### 3.1 åšå¼ˆçš„å…³é”®è¦ç´  (Key Ingredients of a Game)
1.  **ç©å®¶ (Players)**ï¼šå†³ç­–è€…ï¼Œå¦‚ä¸ªäººã€æ”¿åºœã€å…¬å¸ã€‚
2.  **è¡ŒåŠ¨ (Actions)**ï¼šç©å®¶åœ¨æ¯ä¸ªå†³ç­–ç‚¹å¯ä»¥åšä»€ä¹ˆï¼Œå¦‚å‡ºä»·ã€æŠ•ç¥¨ã€å–å‡ºè‚¡ç¥¨ã€‚
3.  **æ”¶ç›Š (Payoffs/Utility)**ï¼šæ¿€åŠ±ç©å®¶çš„å› ç´ ï¼Œå³ä»–ä»¬å…³å¿ƒä»€ä¹ˆï¼ˆå¦‚åˆ©æ¶¦ã€å…¶ä»–ç©å®¶çš„æ•ˆç”¨ï¼‰ã€‚

#### 3.2 æ ‡å‡†å¼ï¼ˆæ­£è§„å½¢å¼ï¼‰(Normal Form / Strategic Form)
*   ç”¨äºæè¿°**é™æ€åšå¼ˆ**ï¼ˆç©å®¶ä¼¼ä¹åŒæ—¶è¡ŒåŠ¨ï¼‰ã€‚
*   å®šä¹‰ä¸ºä¸€ä¸ªä¸‰å…ƒç»„ \(\langle N, A, u \rangle\)ï¼š
    *   \(N = \{1, \ldots, n\}\)ï¼šæœ‰é™çš„ç©å®¶é›†åˆã€‚
    *   \(A_i\)ï¼šç©å®¶ \(i\) çš„è¡ŒåŠ¨é›†åˆã€‚æ‰€æœ‰è¡ŒåŠ¨çš„ç»„åˆæ„æˆ**è¡ŒåŠ¨å‰–é¢ (Action Profile)** \(a = (a_1, \ldots, a_n) \in A = A_1 \times \cdots \times A_n\)ã€‚
    *   \(u_i : A \mapsto \mathbb{R}\)ï¼šç©å®¶ \(i\) çš„**æ•ˆç”¨å‡½æ•° (Utility Function)**ï¼Œè¡¨ç¤ºåœ¨æŸä¸ªè¡ŒåŠ¨å‰–é¢ä¸‹çš„æ”¶ç›Šã€‚\(u = (u_1, \ldots, u_n)\) æ˜¯æ•ˆç”¨å‡½æ•°å‰–é¢ã€‚
*   **è¡¨ç¤º**ï¼šé€šå¸¸ç”¨**çŸ©é˜µ (Matrix)** è¡¨ç¤ºï¼Œå°¤å…¶é€‚ç”¨äºåŒäººåšå¼ˆã€‚è¡Œæ˜¯ç©å®¶1çš„è¡ŒåŠ¨ï¼Œåˆ—æ˜¯ç©å®¶2çš„è¡ŒåŠ¨ï¼Œå•å…ƒæ ¼å†…æ˜¯ï¼ˆç©å®¶1æ”¶ç›Šï¼Œç©å®¶2æ”¶ç›Šï¼‰ã€‚

#### 3.3 æ‰©å±•å¼ï¼ˆæ‰©å±•å½¢å¼ï¼‰(Extensive Form)
*   ç”¨äºæè¿°**åŠ¨æ€åšå¼ˆ**ï¼ˆç©å®¶åºè´¯è¡ŒåŠ¨ï¼‰ï¼ŒåŒ…å«äº†è¡ŒåŠ¨çš„**æ—¶åº**å’Œç©å®¶çš„**ä¿¡æ¯é›†**ï¼ˆçŸ¥é“ä»€ä¹ˆå†å²ä¿¡æ¯ï¼‰ã€‚
*   å®šä¹‰ä¸ºä¸€ä¸ªæœç´¢é—®é¢˜ï¼ŒåŒ…å«ä»¥ä¸‹å…ƒç´ ï¼š
    *   \(S_0\)ï¼šåˆå§‹çŠ¶æ€ã€‚
    *   \(Player(s)\)ï¼šåœ¨çŠ¶æ€ \(s\) è¡ŒåŠ¨çš„ç©å®¶ã€‚
    *   \(Actions(s)\)ï¼šåœ¨çŠ¶æ€ \(s\) å¯ç”¨çš„è¡ŒåŠ¨ã€‚
    *   \(Result(s, a)\)ï¼šè½¬ç§»æ¨¡å‹ï¼Œæ‰§è¡Œè¡ŒåŠ¨åçš„ç»“æœçŠ¶æ€ã€‚
    *   \(TerminalTest(s)\)ï¼šåˆ¤æ–­çŠ¶æ€ \(s\) æ˜¯å¦ä¸ºç»ˆæ­¢çŠ¶æ€ï¼ˆæ¸¸æˆç»“æŸï¼‰ã€‚
    *   \(Utility(s, p)\)ï¼šåœ¨ç»ˆæ­¢çŠ¶æ€ \(s\) ä¸‹ï¼Œç©å®¶ \(p\) çš„æ•ˆç”¨ï¼ˆé›¶å’Œåšå¼ˆä¸­ \(Utility(s, p_1) = -Utility(s, p_2)\)ï¼‰ã€‚
*   **è¡¨ç¤º**ï¼šé€šå¸¸ç”¨**åšå¼ˆæ ‘ (Game Tree)** è¡¨ç¤ºã€‚ä¾‹å¦‚äº•å­—æ£‹çš„åšå¼ˆæ ‘ã€‚

---

### 4. åšå¼ˆè®ºä¸ç¤ºä¾‹ (Game Theory & Examples)
è¯¾ä»¶é€šè¿‡å¤šä¸ªç»å…¸åšå¼ˆç¤ºä¾‹è¯´æ˜äº†å…³é”®æ¦‚å¿µã€‚ä¸‹è¡¨æ€»ç»“äº†è¿™äº›ä¾‹å­åŠå…¶ç‰¹ç‚¹ï¼š

| åšå¼ˆåç§° <br>Game Name       | ç©å®¶ç›®æ ‡ <br>Player Objectives                               | å…³é”®æ¦‚å¿µ <br>Key Concepts Illustrated                               | æ”¶ç›ŠçŸ©é˜µç¤ºä¾‹ <br>Payoff Matrix Example (P1, P2)                                                                       |
| :-------------------------- | :---------------------------------------------------------- | :----------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------ |
| **å›šå¾’å›°å¢ƒ (Prisoner's Dilemma)** | ä¸ªä½“ç†æ€§ä¸é›†ä½“ç†æ€§çš„å†²çª <br>Conflict between individual and collective rationality | **å ä¼˜ç­–ç•¥ (Dominant Strategy)**: æ— è®ºå¯¹æ–¹é€‰ä»€ä¹ˆï¼ŒæŸä¸ªç­–ç•¥æ€»æ˜¯æœ€ä¼˜çš„ï¼ˆæ­¤å¤„æ˜¯èƒŒå›`D`ï¼‰ã€‚ <br>**çº³ä»€å‡è¡¡ (Nash Equilibrium)**: ç»™å®šå¯¹æ–¹ç­–ç•¥ï¼Œè‡ªå·±æ²¡æœ‰åŠ¨æœºå•æ–¹é¢æ”¹å˜ç­–ç•¥ï¼ˆ(`D`, `D`)æ˜¯å‡è¡¡ç‚¹ï¼Œä½†éé›†ä½“æœ€ä¼˜ï¼‰ã€‚ <br>**å¸•ç´¯æ‰˜æœ€ä¼˜ (Pareto Optimality)**: æ— æ³•åœ¨ä¸æŸå®³ä»–äººçš„æƒ…å†µä¸‹ä½¿æŸäººå—ç›Šï¼ˆ(`C`, `C`)æ˜¯å¸•ç´¯æ‰˜æœ€ä¼˜ï¼Œä½†éå‡è¡¡ï¼‰ã€‚ | <code>C | (4, 4) | (0, 6) </code><br><code>D | (6, 0) | (2, 2) </code><br>**å‡è¡¡/å ä¼˜**: (`D`, `D`) |
| **åè°ƒåšå¼ˆ (Coordination Game)** | åŒæ–¹éƒ½å¸Œæœ›è¡ŒåŠ¨ä¸€è‡´ <br>Both players want to coordinate their actions | **å¤šé‡çº³ä»€å‡è¡¡ (Multiple Nash Equilibria)**: å­˜åœ¨å¤šä¸ªç¨³å®šç»“æœï¼ˆ(`Left`, `Left`) å’Œ (`Right`, `Right`)ï¼‰ã€‚ | <code>Left | (1, 1) | (0, 0) </code><br><code>Right | (0, 0) | (1, 1) </code><br>**å‡è¡¡**: (`L`,`L`) å’Œ (`R`,`R`) |
| **åŒ¹é…åšå¼ˆ (Matching Game)** | åŒæ–¹éƒ½å¸Œæœ›â€œåŒ¹é…â€ <br>Both players want to "match"           | ç±»ä¼¼äºåè°ƒåšå¼ˆï¼Œä½†æ”¶ç›Šå¯èƒ½ä¸å¯¹ç§°ã€‚å­˜åœ¨å¤šä¸ªçº³ä»€å‡è¡¡ã€‚ | <code>B | (2, 1) | (0, 0) </code><br><code>F | (0, 0) | (1, 2) </code><br>**å‡è¡¡**: (`B`,`B`) å’Œ (`F`,`F`) |
| **çŒœç¡¬å¸ (Matching Pennies)** | ä¸€æ–¹æƒ³åŒ¹é…ï¼Œå¦ä¸€æ–¹æƒ³ä¸åŒ¹é… <br>One wants to match, the other to mismatch | **é›¶å’Œåšå¼ˆ (Zero-Sum Game)** çš„å…¸å‹ä¾‹å­ã€‚**æ··åˆç­–ç•¥çº³ä»€å‡è¡¡ (Mixed Strategy Nash Equilibrium)**: ä¸å­˜åœ¨çº¯ç­–ç•¥å‡è¡¡ï¼Œç©å®¶å¿…é¡»éšæœºåŒ–è¡ŒåŠ¨ã€‚ | <code>Heads | (1, -1) | (-1, 1) </code><br><code>Tails | (-1, 1) | (1, -1) </code><br>**å‡è¡¡**: åŒæ–¹ä»¥50%/50%çš„æ¦‚ç‡éšæœºé€‰æ‹©ã€‚ |
| **çŸ³å¤´å‰ªåˆ€å¸ƒ (RPS)**        | ä¸€æ–¹æƒ³èµ¢è¿‡å¯¹æ–¹ <br>One wants to beat the other              | åŒæ ·æ˜¯**é›¶å’Œåšå¼ˆ**å’Œ**æ··åˆç­–ç•¥å‡è¡¡**çš„ä¾‹å­ã€‚                         | <code>Rock | (0, 0) | (-1, 1) | (1, -1) </code><br><code>Paper | (1, -1) | (0, 0) | (-1, 1) </code><br><code>Scissors | (-1, 1) | (1, -1) | (0, 0) </code><br>**å‡è¡¡**: åŒæ–¹ä»¥1/3çš„æ¦‚ç‡éšæœºé€‰æ‹©æ¯ç§è¡ŒåŠ¨ã€‚ |
| **é€€å‡ºåšå¼ˆ (Quitting Game)** | ç±»ä¼¼äºâ€œèƒ†å°é¬¼æ¸¸æˆâ€ï¼Œæ˜¯ä¸€ç§**ååè°ƒåšå¼ˆ (Anti-coordination Game)** | ç©å®¶å¸Œæœ›é¿å…ä¸å¯¹æ–¹é€‰æ‹©ç›¸åŒçš„è¡ŒåŠ¨ã€‚å­˜åœ¨ä¸¤ä¸ªçº¯ç­–ç•¥çº³ä»€å‡è¡¡ï¼ˆ(`C`,`D`) å’Œ (`D`,`C`)ï¼‰ï¼Œä»¥åŠä¸€ä¸ªæ··åˆç­–ç•¥å‡è¡¡ã€‚ | <code>C | (0, 0) | (-1, 1) </code><br><code>D | (1, -1) | (-2, -2) </code><br>**å‡è¡¡**: (`C`,`D`) å’Œ (`D`,`C`) |

---

### 5. æœ¬è®²æ€»ç»“ (Summary of This Lecture)
1.  **å¯¹æŠ—æœç´¢**å¤„ç†çš„æ˜¯**å¤šæ™ºèƒ½ä½“ã€ç«äº‰æ€§**ç¯å¢ƒä¸‹çš„å†³ç­–é—®é¢˜ã€‚
2.  åšå¼ˆè®ºä¸ºè¿™ç±»é—®é¢˜æä¾›äº†å½¢å¼åŒ–æ¡†æ¶ã€‚
3.  åšå¼ˆæœ‰ä¸¤ç§ä¸»è¦è¡¨ç¤ºæ³•ï¼š
    *   **æ ‡å‡†å¼ (Normal Form)**ï¼šé€‚ç”¨äºé™æ€ï¼ˆåŒæ—¶è¡ŒåŠ¨ï¼‰åšå¼ˆï¼Œç”¨çŸ©é˜µè¡¨ç¤ºã€‚
    *   **æ‰©å±•å¼ (Extensive Form)**ï¼šé€‚ç”¨äºåŠ¨æ€ï¼ˆåºè´¯è¡ŒåŠ¨ï¼‰åšå¼ˆï¼Œç”¨åšå¼ˆæ ‘è¡¨ç¤ºï¼Œæ˜¯ä¸‹ä¸€è®²ç®—æ³•ï¼ˆå¦‚Minimaxï¼‰çš„åŸºç¡€ã€‚
4.  é€šè¿‡ç»å…¸åšå¼ˆç¤ºä¾‹å¼•å…¥äº†**å ä¼˜ç­–ç•¥ã€çº³ä»€å‡è¡¡ã€å¸•ç´¯æ‰˜æœ€ä¼˜ã€é›¶å’Œåšå¼ˆã€æ··åˆç­–ç•¥**ç­‰æ ¸å¿ƒæ¦‚å¿µã€‚
5.  ä¸‹ä¸€è®²å°†æ·±å…¥æ¢è®¨ç”¨äº**æ‰©å±•å¼åšå¼ˆ**çš„å…·ä½“æœç´¢ç®—æ³•ï¼Œå¦‚ **Minimax ç®—æ³•** å’Œ **Alpha-Beta å‰ªæ**ã€‚

---
### å…³é”®æœ¯è¯­ (Key Terms)
*   **Adversarial Search / å¯¹æŠ—æœç´¢**
*   **Game Theory / åšå¼ˆè®º**
*   **Players / ç©å®¶**
*   **Actions / è¡ŒåŠ¨**
*   **Payoffs / Utility / æ”¶ç›Š / æ•ˆç”¨**
*   **Normal Form / æ ‡å‡†å¼ / æ­£è§„å½¢å¼**
*   **Extensive Form / æ‰©å±•å¼ / æ‰©å±•å½¢å¼**
*   **Game Tree / åšå¼ˆæ ‘**
*   **Zero-Sum Game / é›¶å’Œåšå¼ˆ**
*   **Perfect Information / å®Œå…¨ä¿¡æ¯**
*   **Dominant Strategy / å ä¼˜ç­–ç•¥**
*   **Nash Equilibrium / çº³ä»€å‡è¡¡**
*   **Pareto Optimality / å¸•ç´¯æ‰˜æœ€ä¼˜**
*   

---

## Lecture 13: CSP I - å®šä¹‰ä¸åŸºç¡€

### 1. CSP çš„å®šä¹‰
çº¦æŸæ»¡è¶³é—®é¢˜ï¼ˆCSPï¼‰æ˜¯ä¸€ç§è¯†åˆ«é—®é¢˜ï¼Œå…¶ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ç»„æ»¡è¶³ç»™å®šçº¦æŸçš„å˜é‡èµ‹å€¼ã€‚
CSPç”±ä¸‰ä¸ªç»„æˆéƒ¨åˆ†æ„æˆï¼š
- **å˜é‡ï¼ˆXï¼‰**: \( X = \{X_1, X_2, ..., X_n\} \)ï¼Œè¡¨ç¤ºé—®é¢˜ä¸­çš„æœªçŸ¥æ•°ã€‚
- **åŸŸï¼ˆDï¼‰**: \( D = \{D_1, D_2, ..., D_n\} \)ï¼Œæ¯ä¸ªå˜é‡ \( X_i \) æœ‰å…¶å¯¹åº”çš„å¯èƒ½å–å€¼é›†åˆ \( D_i \)ã€‚
- **çº¦æŸï¼ˆCï¼‰**: è§„å®šäº†å˜é‡ä¹‹é—´å…è®¸çš„å–å€¼ç»„åˆã€‚

**æ ¸å¿ƒæ¦‚å¿µ**:
- **èµ‹å€¼**: ä¸ºå˜é‡æŒ‡å®šå€¼çš„è¿‡ç¨‹ã€‚
- **å®Œå…¨èµ‹å€¼**: æ¯ä¸ªå˜é‡éƒ½è¢«èµ‹äºˆä¸€ä¸ªå€¼ã€‚
- **éƒ¨åˆ†èµ‹å€¼**: ä»…éƒ¨åˆ†å˜é‡è¢«èµ‹å€¼ã€‚
- **è§£**: ä¸€ä¸ªæ»¡è¶³æ‰€æœ‰çº¦æŸçš„**å®Œå…¨èµ‹å€¼**ã€‚
- **éƒ¨åˆ†è§£**: ä¸€ä¸ªæ»¡è¶³æ‰€æœ‰çº¦æŸçš„**éƒ¨åˆ†èµ‹å€¼**ã€‚

### 2. CSP ä¸æ ‡å‡†æœç´¢é—®é¢˜çš„åŒºåˆ«
| ç‰¹å¾ | æ ‡å‡†æœç´¢é—®é¢˜ | CSP |
| :--- | :--- | :--- |
| **çŠ¶æ€** | ä¸é€æ˜çš„â€œé»‘ç®±â€æ•°æ®ç»“æ„ | ç”±å˜é‡åŠå…¶èµ‹å€¼æ˜ç¡®å®šä¹‰ |
| **ç›®æ ‡æµ‹è¯•** | é’ˆå¯¹æ•´ä¸ªçŠ¶æ€çš„å•ä¸€å‡½æ•° | ä¸€ç»„çº¦æŸæ¡ä»¶ï¼Œæ£€æŸ¥å˜é‡é—´çš„å€¼ç»„åˆ |
| **ä¼˜åŠ¿** | é€šç”¨æ€§å¼º | åˆ©ç”¨ç»“æ„ä¿¡æ¯ï¼Œæœ‰æ›´å¼ºå¤§ã€é«˜æ•ˆçš„ä¸“ç”¨ç®—æ³• |

### 3. ç¤ºä¾‹ï¼šåœ°å›¾ç€è‰²é—®é¢˜
- **å˜é‡**: WA, NT, Q, NSW, V, SA, T (æ¾³å¤§åˆ©äºšçš„å·å’Œåœ°åŒº)
- **åŸŸ**: \( D_i = \{red, green, blue\} \) (æ¯ä¸ªå˜é‡éƒ½å¯å–ä¸‰ç§é¢œè‰²)
- **çº¦æŸ**: ç›¸é‚»çš„åŒºåŸŸå¿…é¡»é¢œè‰²ä¸åŒï¼Œä¾‹å¦‚ \( WA \neq NT \)ï¼Œæˆ–æ›´å½¢å¼åŒ–åœ°è¡¨ç¤ºä¸ºå…è®¸çš„(WA, NT)é¢œè‰²å¯¹é›†åˆã€‚
- **è§£ç¤ºä¾‹**: `{WA=red, NT=green, Q=red, NSW=green, V=red, SA=blue, T=green}`

### 4. çº¦æŸå›¾
- **äºŒå…ƒCSP**: æ‰€æœ‰çº¦æŸæœ€å¤šæ¶‰åŠä¸¤ä¸ªå˜é‡ã€‚
- **çº¦æŸå›¾**: èŠ‚ç‚¹ä»£è¡¨å˜é‡ï¼Œè¾¹ä»£è¡¨çº¦æŸå…³ç³»ã€‚
    - **ä»·å€¼**: å›¾ç»“æ„å¯ä»¥æå¤§åœ°åŠ é€Ÿæœç´¢ã€‚ä¾‹å¦‚ï¼Œåœ¨æ¾³å¤§åˆ©äºšåœ°å›¾ä¸­ï¼Œå¡”æ–¯é©¬å°¼äºšå²›(T)ä¸å¤§é™†æ˜¯ç‹¬ç«‹çš„å­é—®é¢˜ï¼Œå¯ä»¥åˆ†å¼€æ±‚è§£ã€‚

### 5. CSP çš„åˆ†ç±»
#### a. æŒ‰å˜é‡ç±»å‹
- **ç¦»æ•£å˜é‡**
    - **æœ‰é™åŸŸ**: ä¾‹å¦‚å¸ƒå°”CSPï¼ˆå‘½é¢˜å¯æ»¡è¶³æ€§ï¼Œæ˜¯NPå®Œå…¨é—®é¢˜ï¼‰ã€‚å…±æœ‰ \(d^n\) ç§å®Œå…¨èµ‹å€¼ã€‚
    - **æ— é™åŸŸ**: ä¾‹å¦‚ä½œä¸šè°ƒåº¦ï¼ˆå˜é‡æ˜¯å¼€å§‹/ç»“æŸæ—¥æœŸï¼‰ã€‚éœ€è¦çº¦æŸè¯­è¨€ï¼ˆå¦‚ \(StartJob_1 + 5 \leq StartJob_3\)ï¼‰ã€‚çº¿æ€§çº¦æŸå¯è§£ï¼Œéçº¿æ€§çº¦æŸä¸å¯åˆ¤å®šã€‚
- **è¿ç»­å˜é‡**
    - ä¾‹å¦‚å“ˆå‹ƒæœ›è¿œé•œè§‚æµ‹è°ƒåº¦ã€‚çº¿æ€§çº¦æŸå¯é€šè¿‡çº¿æ€§è§„åˆ’æ–¹æ³•åœ¨å¤šé¡¹å¼æ—¶é—´å†…æ±‚è§£ã€‚

#### b. æŒ‰çº¦æŸç±»å‹
- **ä¸€å…ƒçº¦æŸ**: æ¶‰åŠå•ä¸ªå˜é‡ï¼ˆå¦‚ \( SA \neq green \)ï¼‰ã€‚
- **äºŒå…ƒçº¦æŸ**: æ¶‰åŠä¸¤ä¸ªå˜é‡ï¼ˆå¦‚ \( SA \neq WA \)ï¼‰ã€‚
- **é«˜é˜¶çº¦æŸ**: æ¶‰åŠä¸‰ä¸ªåŠä»¥ä¸Šå˜é‡ï¼ˆå¦‚å¯†ç ç®—æœ¯é¢˜ä¸­çš„åˆ—çº¦æŸï¼‰ã€‚
- **è½¯çº¦æŸï¼ˆåå¥½ï¼‰**: å¹¶éå¿…é¡»æ»¡è¶³ï¼Œä½†æ›´å¥½çš„èµ‹å€¼å…·æœ‰æ›´é«˜ä»·å€¼ï¼ˆå¦‚çº¢è‰²æ¯”ç»¿è‰²å¥½ï¼‰ï¼Œå¯è½¬åŒ–ä¸ºçº¦æŸä¼˜åŒ–é—®é¢˜ã€‚

### 6. ç°å®ä¸–ç•Œçš„ CSP åº”ç”¨
- ä»»åŠ¡åˆ†é…ï¼ˆè°æ•™å“ªé—¨è¯¾ï¼‰
- æ—¶é—´è¡¨ç¼–æ’ï¼ˆè¯¾ç¨‹å®‰æ’åœ¨ä½•æ—¶ä½•åœ°ï¼‰
- ç¡¬ä»¶é…ç½®
- è¿è¾“è°ƒåº¦
- å·¥å‚è°ƒåº¦
- å¹³é¢å¸ƒå±€è§„åˆ’

### 7. æ¡ˆä¾‹ç ”ç©¶ï¼šML vs. GOFAI (ä¼ ç»ŸAI)
- **Flatland æŒ‘æˆ˜** (NeurIPS 2020): ä¸€ä¸ªå¤§å‹é“è·¯ç½‘ç»œè°ƒåº¦é—®é¢˜ã€‚
- **ç»“æœ**: å°½ç®¡å¸å¼•äº†å¤§é‡æœºå™¨å­¦ä¹ æ–¹æ¡ˆï¼Œä½†æœ€ç»ˆå¤šæ•°è·èƒœæ–¹æ¡ˆä½¿ç”¨äº†åŸºäºCSPçš„ä¼ ç»Ÿç¬¦å·AIæ–¹æ³•ï¼ˆGOFAIï¼‰ã€‚
- **å¯ç¤º**: å¯¹äºé«˜åº¦ç»“æ„åŒ–çš„ä¼˜åŒ–é—®é¢˜ï¼ŒCSP ç­‰ä¼ ç»Ÿæ–¹æ³•ç›®å‰ä»å¸¸ä¼˜äºçº¯ç²¹çš„æœºå™¨å­¦ä¹ æ–¹æ³•ã€‚

---

## Lecture 14: CSP II - å›æº¯æœç´¢

### 1. å›æº¯æœç´¢åŸºç¡€
- **æ ¸å¿ƒè§‚å¯Ÿ**: åœ¨CSPä¸­ï¼Œå˜é‡èµ‹å€¼çš„**é¡ºåºä¸å½±å“ç»“æœ**ï¼ˆ[WA=red then NT=green] ä¸ [NT=green then WA=red] ç›¸åŒï¼‰ã€‚
- **å›æº¯æœç´¢**: ä¸€ç§ä¸ºCSPè®¾è®¡çš„æ·±åº¦ä¼˜å…ˆæœç´¢ã€‚
    - æ¯ä¸ªèŠ‚ç‚¹ä¸ºä¸€ä¸ªå˜é‡èµ‹å€¼ã€‚
    - åˆ†æ”¯å› å­ \(b = d\) (åŸŸå¤§å°)ï¼Œå¶å­èŠ‚ç‚¹æ•°ä¸º \(d^n\)ã€‚
    - æ˜¯CSPçš„åŸºæœ¬**æ— ä¿¡æ¯**ç®—æ³•ï¼Œå¯è§£å†³ nâ‰ˆ25 çš„nçš‡åé—®é¢˜ã€‚

### 2. å›æº¯æœç´¢ç®—æ³•ï¼ˆä¼ªä»£ç ï¼‰

```python
# ä¸»å‡½æ•°ï¼šå¯åŠ¨å›æº¯æœç´¢è¿‡ç¨‹
function BACKTRACKING-SEARCH(csp):
    # ä»ç©ºèµ‹å€¼å¼€å§‹é€’å½’æœç´¢
    return BACKTRACK({}, csp)

# æ ¸å¿ƒé€’å½’å›æº¯å‡½æ•°
function BACKTRACK(assignment, csp):
    # åŸºç¡€æƒ…å†µï¼šå¦‚æœèµ‹å€¼å·²ç»åŒ…å«æ‰€æœ‰å˜é‡ï¼Œè¯´æ˜æ‰¾åˆ°è§£
    if assignment is complete:
        return assignment

    # å¯å‘å¼ï¼šé€‰æ‹©ä¸‹ä¸€ä¸ªè¦èµ‹å€¼çš„å˜é‡ï¼ˆå¦‚ä½¿ç”¨MRVï¼‰
    var = SELECT-UNASSIGNED-VARIABLE(csp, assignment)
    
    # å¯å‘å¼ï¼šæŒ‰ç‰¹å®šé¡ºåºå°è¯•è¯¥å˜é‡çš„æ‰€æœ‰å¯èƒ½å€¼ï¼ˆå¦‚ä½¿ç”¨æœ€å°‘çº¦æŸå€¼ï¼‰
    for value in ORDER-DOMAIN-VALUES(csp, var, assignment):
        # æ£€æŸ¥è¯¥å€¼æ˜¯å¦ä¸å½“å‰èµ‹å€¼ä¸€è‡´ï¼ˆæ»¡è¶³æ‰€æœ‰çº¦æŸï¼‰
        if value is consistent with assignment:
            # æš‚æ—¶å°†è¯¥èµ‹å€¼åŠ å…¥å½“å‰è§£
            add {var = value} to assignment
            
            # å¯é€‰ï¼šè¿›è¡Œæ¨ç†ï¼ˆå¦‚å‰å‘æ£€æŸ¥ã€å¼§ç›¸å®¹ï¼‰ï¼Œå¯èƒ½æ¨å¯¼å‡ºæ–°çš„çº¦æŸæˆ–å‡å°‘å€¼åŸŸ
            inferences = INFERENCE(csp, var, assignment)
            
            # å¦‚æœæ¨ç†æ²¡æœ‰å‘ç°çŸ›ç›¾
            if inferences != failure:
                # å°†æ¨ç†ç»“æœåŠ å…¥èµ‹å€¼
                add inferences to assignment
                
                # é€’å½’è°ƒç”¨ï¼Œç»§ç»­æ·±åº¦æœç´¢
                result = BACKTRACK(assignment, csp)
                
                # å¦‚æœé€’å½’è°ƒç”¨è¿”å›è§£ï¼Œåˆ™é€å±‚è¿”å›è§£
                if result != failure:
                    return result
                
                # å¦‚æœé€’å½’å¤±è´¥ï¼Œå›æº¯ï¼šç§»é™¤æ¨ç†ç»“æœ
                remove inferences from assignment
            
            # å›æº¯ï¼šç§»é™¤å½“å‰å°è¯•çš„èµ‹å€¼ï¼Œå‡†å¤‡å°è¯•ä¸‹ä¸€ä¸ªå€¼
            remove {var = value} from assignment
    
    # å¦‚æœæ‰€æœ‰å€¼éƒ½å°è¯•å¤±è´¥ï¼Œåˆ™è¿”å›å¤±è´¥ä¿¡å·ç»™ä¸Šä¸€å±‚
    return failure
```

### 3. æé«˜å›æº¯æ•ˆç‡çš„é€šç”¨ç­–ç•¥
1.  **ä¸‹ä¸€æ­¥é€‰æ‹©å“ªä¸ªå˜é‡ï¼Ÿ** (`SELECT-UNASSIGNED-VARIABLE`)
2.  **æŒ‰ä»€ä¹ˆé¡ºåºå°è¯•å˜é‡çš„å€¼ï¼Ÿ** (`ORDER-DOMAIN-VALUES`)
3.  **èƒ½å¦å°½æ—©æ£€æµ‹åˆ°å¿…ç„¶å¤±è´¥ï¼Ÿ** (`INFERENCE`)
4.  **èƒ½å¦åˆ©ç”¨é—®é¢˜ç»“æ„ï¼Ÿ** (è§L15)
5.  **èƒ½å¦ä¿å­˜å’Œé‡ç”¨éƒ¨åˆ†æœç´¢ç»“æœï¼Ÿ**

### 4. å˜é‡é€‰æ‹©å¯å‘å¼
- **æœ€å°å‰©ä½™å€¼ (MRV)**:
    - **ç­–ç•¥**: é€‰æ‹©**åˆæ³•å€¼æœ€å°‘çš„å˜é‡**ã€‚
    - **åŸç†**: ä¼˜å…ˆå¤„ç†çº¦æŸæœ€ç´§çš„å˜é‡ï¼Œèƒ½æœ€å¿«åœ°å¯¼è‡´æˆåŠŸæˆ–å¤±è´¥ï¼ˆ"å¤±è´¥ä¼˜å…ˆ"ï¼‰ã€‚

- **åº¦å¯å‘å¼ (Degree Heuristic)**:
    - **ç­–ç•¥**: ä½œä¸ºMRVçš„å¹³å±€å†³èƒœå™¨ï¼Œé€‰æ‹©**å¯¹æœªèµ‹å€¼å˜é‡çº¦æŸæœ€å¤š**çš„å˜é‡ã€‚
    - **åŸç†**: çº¦æŸæœ€å¤šçš„å˜é‡æ›´å¯èƒ½å¾ˆå¿«å¯¼è‡´å¤±è´¥ï¼Œé¿å…åœ¨æ— å…³åˆ†æ”¯ä¸Šæµªè´¹æ—¶é—´ã€‚

### 5. å€¼é€‰æ‹©å¯å‘å¼
- **æœ€å°‘çº¦æŸå€¼ (Least Constraining Value)**:
    - **ç­–ç•¥**: ç»™å®šä¸€ä¸ªå˜é‡ï¼Œä¼˜å…ˆé€‰æ‹©**æ·˜æ±°å…¶ä»–å˜é‡å€¼æœ€å°‘**çš„å€¼ã€‚
    - **åŸç†**: ä¿ç•™å…¶ä»–å˜é‡çš„çµæ´»æ€§ï¼Œå‡å°‘æœªæ¥å›æº¯çš„å¯èƒ½æ€§ã€‚

### 6. æ¨ç†ä¸çº¦æŸä¼ æ’­
- **å‰å‘æ£€æŸ¥ (Forward Checking)**:
    - **æ€æƒ³**: å½“ä¸€ä¸ªå˜é‡è¢«èµ‹å€¼åï¼Œç«‹å³æ£€æŸ¥å¹¶åˆ é™¤å…¶æœªèµ‹å€¼é‚»å±…å˜é‡å€¼åŸŸä¸­ä¸ä¹‹å†²çªçš„å€¼ã€‚
    - **ä½œç”¨**: èƒ½å°½æ—©å‘ç°æŸä¸ªå˜é‡å€¼åŸŸä¸ºç©ºçš„æƒ…å†µï¼Œç«‹å³å›æº¯ã€‚

- **çº¦æŸä¼ æ’­ (Constraint Propagation)**:
    - **æ€æƒ³**: æ¯”å‰å‘æ£€æŸ¥æ›´æ·±å…¥ã€‚ä¸ä»…æ£€æŸ¥ç›´æ¥é‚»å±…ï¼Œè¿˜é€šè¿‡çº¦æŸç½‘ç»œä¼ æ’­å½±å“ã€‚
    - **ç›®æ ‡**: é€šè¿‡å±€éƒ¨ç›¸å®¹æ€§æ£€æŸ¥ï¼Œè¿›ä¸€æ­¥ç¼©å°å€¼åŸŸï¼Œæå‰å‘ç°ä¸ä¸€è‡´ã€‚

### 7. å¼§ç›¸å®¹ (Arc Consistency) ä¸ AC-3 ç®—æ³•
- **å¼§ç›¸å®¹å®šä¹‰**: å¯¹äºçº¦æŸå›¾ä¸­çš„ä¸€æ¡å¼§ (X, Y)ï¼ˆä»£è¡¨çº¦æŸï¼‰ï¼Œå¦‚æœå¯¹äºXå€¼åŸŸä¸­çš„**æ¯ä¸€ä¸ª**å€¼ï¼ŒYå€¼åŸŸä¸­éƒ½**è‡³å°‘å­˜åœ¨ä¸€ä¸ª**å€¼èƒ½æ»¡è¶³è¯¥çº¦æŸï¼Œåˆ™ç§°å¼§ (X, Y) æ˜¯ç›¸å®¹çš„ã€‚
- **AC-3ç®—æ³•æ€æƒ³**: ç»´æŠ¤ä¸€ä¸ªå¼§çš„é˜Ÿåˆ—ã€‚ä¸æ–­ä»é˜Ÿåˆ—ä¸­å–å‡ºå¼§è¿›è¡Œæ£€æŸ¥ï¼Œå¦‚æœä¸ºäº†ä½¿å…¶ç›¸å®¹è€Œåˆ é™¤äº†Xçš„æŸäº›å€¼ï¼Œé‚£ä¹ˆæ‰€æœ‰æŒ‡å‘Xçš„å¼§ï¼ˆå³ (Z, X)ï¼‰å¯èƒ½å˜å¾—ä¸ç›¸å®¹ï¼Œéœ€è¦é‡æ–°åŠ å…¥é˜Ÿåˆ—æ£€æŸ¥ã€‚ç›´åˆ°é˜Ÿåˆ—ä¸ºç©ºæˆ–æŸä¸ªå˜é‡çš„å€¼åŸŸä¸ºç©ºã€‚

```python
# AC-3 ç®—æ³•ï¼šå®ç°å¼§ç›¸å®¹
function AC-3(csp):
    # åˆå§‹åŒ–é˜Ÿåˆ—ï¼ŒåŒ…å«cspä¸­çš„æ‰€æœ‰å¼§ï¼ˆåŒå‘ï¼‰
    queue = a queue of arcs, initially all the arcs in csp

    while queue is not empty:
        # ä»é˜Ÿåˆ—ä¸­å–å‡ºä¸€æ¡å¼§ (X_i, X_j)
        (X_i, X_j) = POP(queue)

        # ä¿®è®¢X_içš„å€¼åŸŸï¼šåˆ é™¤æ‰€æœ‰æ²¡æœ‰å¯¹åº”åˆæ³•å€¼in X_jçš„å€¼
        if REVISE(csp, X_i, X_j):
            # å¦‚æœX_içš„å€¼åŸŸè¢«ä¿®è®¢åä¸ºç©ºï¼Œåˆ™é—®é¢˜æ— è§£
            if size of D_i == 0:
                return false
            # å¦åˆ™ï¼Œå› ä¸ºX_içš„å€¼åŸŸå˜äº†ï¼Œéœ€è¦é‡æ–°æ£€æŸ¥æ‰€æœ‰æŒ‡å‘X_içš„å¼§ï¼ˆé™¤äº†X_jï¼‰
            for each X_k in X_i.Neighbors - {X_j}:
                add (X_k, X_i) to queue  # æ³¨æ„æ–¹å‘æ˜¯ (é‚»å±… -> X_i)

    return true  # CSPæ˜¯å¼§ç›¸å®¹çš„

# è¾…åŠ©å‡½æ•°ï¼šä¿®è®¢X_içš„å€¼åŸŸä»¥ä½¿å…¶ç›¸å¯¹äºX_jæ»¡è¶³å¼§ç›¸å®¹
function REVISE(csp, X_i, X_j):
    revised = false
    for each x in D_i:  # éå†X_içš„å½“å‰å€¼åŸŸä¸­çš„æ¯ä¸ªå€¼
        # æ£€æŸ¥åœ¨X_jçš„å€¼åŸŸä¸­æ˜¯å¦å­˜åœ¨ä¸€ä¸ªå€¼yï¼Œä½¿å¾—(x, y)æ»¡è¶³X_iå’ŒX_jä¹‹é—´çš„çº¦æŸ
        if no value y in D_j allows (x, y) to satisfy the constraint:
            delete x from D_i  # å¦‚æœæ²¡æœ‰è¿™æ ·çš„yï¼Œåˆ™xæ˜¯éæ³•çš„ï¼Œä»å€¼åŸŸä¸­åˆ é™¤
            revised = true
    return revised  # è¿”å›æ˜¯å¦å¯¹å€¼åŸŸè¿›è¡Œäº†ä¿®è®¢
```
- **å¤æ‚åº¦**: æœ€åæƒ…å†µ \(O(n^2d^3)\)ï¼Œå¯ä¼˜åŒ–è‡³ \(O(n^2d^2)\)ã€‚å®ç°å®Œå…¨ç›¸å®¹æ˜¯NPéš¾çš„ï¼Œä½†å¼§ç›¸å®¹ä½œä¸ºé¢„å¤„ç†æˆ–æœç´¢ä¸­æ¯ä¸€æ­¥çš„æ¨ç†éå¸¸æœ‰æ•ˆã€‚

---

## Lecture 15: CSP III - å±€éƒ¨æœç´¢ä¸é—®é¢˜ç»“æ„

### 1. å±€éƒ¨æœç´¢ç”¨äº CSP
- **å®Œå…¨çŠ¶æ€å½¢å¼åŒ–**: ä¸å›æº¯æœç´¢ä»ç©ºèµ‹å€¼å¼€å§‹ä¸åŒï¼Œå±€éƒ¨æœç´¢ä»**ä¸€ä¸ªå®Œå…¨èµ‹å€¼**ï¼ˆå¯èƒ½è¿åçº¦æŸï¼‰å¼€å§‹ã€‚
- **åŸºæœ¬æ“ä½œ**: é€šè¿‡**æ”¹å˜ä¸€ä¸ªå˜é‡çš„å€¼**æ¥ç”Ÿæˆé‚»å±…çŠ¶æ€ã€‚
- **ç›®æ ‡å‡½æ•°**: é€šå¸¸å®šä¹‰ä¸º**è¢«è¿åçº¦æŸçš„æ•°é‡**ã€‚ç›®æ ‡æ˜¯ä½¿è¿™ä¸ªæ•°é‡æœ€å°åŒ–ä¸º0ã€‚

### 2. æœ€å°å†²çªç®—æ³•ï¼ˆä¼ªä»£ç ï¼‰

```python
# æœ€å°å†²çªç®—æ³•ï¼šä½¿ç”¨å±€éƒ¨æœç´¢è§£å†³CSP
function MIN-CONFLICTS(csp, max_steps):
    # 1. åˆå§‹åŒ–ï¼šéšæœºç”Ÿæˆä¸€ä¸ªå®Œå…¨èµ‹å€¼ï¼ˆå¯èƒ½åŒ…å«å†²çªï¼‰
    current = an initial complete assignment for csp

    # 2. è¿­ä»£æ”¹è¿›ï¼šæœ€å¤šå°è¯• max_steps æ­¥
    for i = 1 to max_steps:
        # å¦‚æœå½“å‰èµ‹å€¼å·²ç»æ»¡è¶³æ‰€æœ‰çº¦æŸï¼ˆå†²çªæ•°ä¸º0ï¼‰ï¼Œåˆ™è¿”å›è§£
        if current is a solution for csp:
            return current

        # 3. å˜é‡é€‰æ‹©ï¼šéšæœºé€‰æ‹©ä¸€ä¸ªå½“å‰å¤„äºå†²çªä¸­çš„å˜é‡ï¼ˆå³å…¶å€¼è¿åäº†è‡³å°‘ä¸€ä¸ªçº¦æŸï¼‰
        var = a randomly chosen conflicted variable from csp.Variables

        # 4. å€¼é€‰æ‹©ï¼šåœ¨varçš„æ‰€æœ‰å¯èƒ½å€¼ä¸­ï¼Œé€‰æ‹©ä½¿æ€»å†²çªæ•°æœ€å°çš„é‚£ä¸ªå€¼
        #    æ³¨æ„ï¼šè¿™é‡Œæ¯”è¾ƒçš„æ˜¯å°†varèµ‹å€¼ä¸ºvåï¼Œæ•´ä¸ªèµ‹å€¼çš„å†²çªæ•°ï¼Œè€Œä¸ä»…ä»…æ˜¯varå¼•èµ·çš„å†²çª
        value = the value v in var.domain that minimizes CONFLICTS(csp, var, v, current)

        # 5. èµ‹å€¼ï¼šå°†é€‰å®šçš„å€¼èµ‹ç»™å˜é‡ï¼Œç§»åŠ¨åˆ°é‚»å±…çŠ¶æ€
        set var = value in current

    # å¦‚æœè¶…è¿‡æœ€å¤§æ­¥æ•°ä»æœªæ‰¾åˆ°è§£ï¼Œåˆ™è¿”å›å¤±è´¥
    return failure
```

### 3. å±€éƒ¨æœç´¢çš„ç‰¹æ€§ä¸ä¼˜åŒ–
- **æœ€å°å†²çªå¯å‘å¼**: é€‰æ‹©ä½¿æ–°çŠ¶æ€æ€»å†²çªæ•°æœ€å°çš„å€¼ã€‚è¿™æ˜¯ä¸€ç§**çˆ¬å±±æ³•**ã€‚
- **å¹³å°é—®é¢˜**: æœç´¢å¯èƒ½é™·å…¥é«˜åŸï¼ˆè®¸å¤šçŠ¶æ€å†²çªæ•°ç›¸åŒï¼‰ã€‚å…è®¸**ä¾§å‘ç§»åŠ¨**ï¼ˆç§»åŠ¨åˆ°å†²çªæ•°ç›¸ç­‰çš„çŠ¶æ€ï¼‰æœ‰åŠ©äºé€ƒç¦»é«˜åŸã€‚
- **çº¦æŸåŠ æƒ**: ä¸ºæ¯ä¸ªçº¦æŸèµ‹äºˆæƒé‡ã€‚å½“çº¦æŸè¢«è¿åæ—¶å¢åŠ å…¶æƒé‡ã€‚æœç´¢ä¼šå€¾å‘äºè§£å†³æƒé‡é«˜çš„çº¦æŸï¼Œä»è€Œå°†ç²¾åŠ›é›†ä¸­åœ¨éš¾é¢˜éƒ¨åˆ†ã€‚
- **æ€§èƒ½**: å¯¹è®¸å¤šCSPï¼ˆå¦‚n-çš‡åï¼‰æ•ˆæœæƒŠäººï¼Œå³ä½¿nå¾ˆå¤§ï¼ˆå¦‚1000ä¸‡ï¼‰ï¼Œä¹Ÿèƒ½ä»¥é«˜æ¦‚ç‡åœ¨è¿‘ä¼¼å¸¸æ•°æ—¶é—´å†…æ‰¾åˆ°è§£ã€‚ä½†åœ¨çº¦æŸæ•°é‡ä¸å˜é‡æ•°é‡çš„æ¯”ç‡ï¼ˆRï¼‰å¤„äºä¸´ç•ŒåŒºåŸŸæ—¶å¯èƒ½å¤±æ•ˆã€‚

### 4. é—®é¢˜ç»“æ„çš„é‡è¦æ€§
- **ç‹¬ç«‹å­é—®é¢˜**: å¦‚æœçº¦æŸå›¾å¯åˆ†è§£ä¸ºè¿é€šåˆ†é‡ï¼ˆå¦‚æ¾³å¤§åˆ©äºšåœ°å›¾ä¸­çš„å¤§é™†å’Œå¡”æ–¯é©¬å°¼äºšå²›ï¼‰ï¼Œè¿™äº›å­é—®é¢˜å¯ç‹¬ç«‹æ±‚è§£ã€‚
    - **å¤æ‚åº¦æ”¶ç›Š**: ä» \(O(d^n)\) é™ä¸º \(O(\frac{n}{c} \cdot d^c)\)ï¼Œå…¶ä¸­cæ˜¯å­é—®é¢˜å¤§å°ã€‚ä¾‹å¦‚ï¼Œn=80, d=2, c=20ï¼Œæ—¶é—´ä»40äº¿å¹´é™è‡³0.4ç§’ï¼ˆå‡è®¾æ¯ç§’1000ä¸‡æ¬¡æ“ä½œï¼‰ã€‚

### 5. æ ‘ç»“æ„ CSPs
- **å®šç†**: å¦‚æœçº¦æŸå›¾æ˜¯æ— ç¯çš„ï¼ˆä¸€æ£µæ ‘ï¼‰ï¼Œåˆ™CSPå¯ä»¥åœ¨ \(O(n d^2)\) æ—¶é—´å†…è§£å†³ï¼Œä¸ä¸€èˆ¬CSPçš„ \(O(d^n)\) å½¢æˆé²œæ˜å¯¹æ¯”ã€‚
- **æ±‚è§£ç®—æ³•**:
    1.  **æ’åº**: é€‰ä¸€ä¸ªæ ¹å˜é‡ï¼Œè¿›è¡Œæ‹“æ‰‘æ’åºï¼Œä½¿æ¯ä¸ªèŠ‚ç‚¹çš„çˆ¶èŠ‚ç‚¹æ’åœ¨å…¶ä¹‹å‰ã€‚
    2.  **å‘åä¼ é€’ï¼ˆå»é™¤ä¸ç›¸å®¹ï¼‰**: ä»å¶å­åˆ°æ ¹ï¼Œç¡®ä¿æ¯ä¸ªèŠ‚ç‚¹ç›¸å¯¹äºå…¶çˆ¶èŠ‚ç‚¹æ˜¯å¼§ç›¸å®¹çš„ã€‚
    3.  **å‘å‰ä¼ é€’ï¼ˆèµ‹å€¼ï¼‰**: ä»æ ¹åˆ°å¶å­ï¼Œä¸ºæ¯ä¸ªå˜é‡é€‰æ‹©ä¸€ä¸ªä¸å…¶çˆ¶èŠ‚ç‚¹å€¼ç›¸å®¹çš„å€¼ã€‚
- **æ„ä¹‰**: å±•ç¤ºäº†è¯­æ³•ï¼ˆç»“æ„ï¼‰é™åˆ¶ä¸æ¨ç†å¤æ‚åº¦ä¹‹é—´çš„é‡è¦å…³ç³»ã€‚

### 6. è¿‘ä¼¼æ ‘ç»“æ„ CSPsï¼šå‰²é›†æ¡ä»¶
- **æ€æƒ³**: å¦‚æœå›¾ä¸æ˜¯æ ‘ï¼Œå¯ä»¥é€šè¿‡å®ä¾‹åŒ–ä¸€ä¸ªå˜é‡å­é›†ï¼ˆ**å‰²é›†**ï¼‰ï¼Œä½¿å¾—å‰©ä½™å›¾å˜æˆæ ‘ã€‚
- **è¿‡ç¨‹**:
    1.  ä»¥æ‰€æœ‰å¯èƒ½çš„æ–¹å¼å®ä¾‹åŒ–å‰²é›†ï¼ˆ\(d^{|cutset|}\) ç§æ–¹å¼ï¼‰ã€‚
    2.  å¯¹äºæ¯ç§å®ä¾‹åŒ–ï¼Œæ±‚è§£å‰©ä½™çš„æ ‘ç»“æ„CSPã€‚
- **å¤æ‚åº¦**: \(O(d^c \cdot (n-c) d^2)\)ï¼Œå…¶ä¸­cæ˜¯å‰²é›†å¤§å°ã€‚åªè¦å‰²é›†cå¾ˆå°ï¼Œè¿™å°±éå¸¸é«˜æ•ˆã€‚

### 7. CSP æ€»ç»“
- CSPæ˜¯ä¸€ç§çŠ¶æ€ç”±å˜é‡èµ‹å€¼å®šä¹‰çš„ç‰¹æ®Šé—®é¢˜ã€‚
- **å›æº¯æœç´¢**æ˜¯åŸºç¡€ç³»ç»Ÿæ–¹æ³•ï¼Œç»“åˆå˜é‡/å€¼æ’åºå¯å‘å¼ã€å‰å‘æ£€æŸ¥ã€çº¦æŸä¼ æ’­ï¼ˆå¦‚å¼§ç›¸å®¹ï¼‰å¯æå¤§æå‡æ•ˆç‡ã€‚
- **å±€éƒ¨æœç´¢**ï¼ˆå¦‚æœ€å°å†²çªï¼‰é€‚ç”¨äºå¤§è§„æ¨¡CSPï¼Œé€šå¸¸ä¸ä¿è¯å®Œå¤‡æ€§ä½†é€Ÿåº¦å¾ˆå¿«ã€‚
- **åˆ†æé—®é¢˜ç»“æ„**ï¼ˆå¦‚æ ‘å®½åº¦ã€å‰²é›†ï¼‰èƒ½å¸¦æ¥æŒ‡æ•°çº§çš„æ•ˆç‡æå‡ã€‚
- å¯¹äºè®¸å¤šç»“æ„åŒ–çš„ç»„åˆä¼˜åŒ–é—®é¢˜ï¼Œ**CSPæ–¹æ³•ç›®å‰ä»ä¼˜äºæœ€å…ˆè¿›çš„æœºå™¨å­¦ä¹ æ–¹æ³•**ã€‚

ä»¥ä¸‹æ˜¯æ ¹æ®ä½ æä¾›çš„ä¸‰ä¸ªè¯¾ä»¶ï¼ˆL16, L17, L18ï¼‰æ•´ç†çš„**è¯¦ç»†åŒè¯­å­¦ä¹ ç¬”è®°**ï¼Œæ¶µç›–æ‰€æœ‰çŸ¥è¯†ç‚¹ï¼Œå¹¶é…æœ‰ä¾‹å­å’Œé€šä¿—è§£é‡Šï¼Œå¸®åŠ©ä½ ç†è§£é€»è¾‘ä»£ç†ï¼ˆLogic Agentsï¼‰åœ¨äººå·¥æ™ºèƒ½ä¸­çš„åŸºç¡€ä¸åº”ç”¨ã€‚

---

# ğŸ“˜ Lecture 16: Logic Agents I â€“ é€»è¾‘ä»£ç†ï¼ˆä¸€ï¼‰

## 1. çŸ¥è¯†å‹ä»£ç† | Knowledge-based Agents

- **çŸ¥è¯†åº“ï¼ˆKnowledge Base, KBï¼‰**ï¼šä¸€ä¸ªç”¨å½¢å¼è¯­è¨€è¡¨è¾¾çš„å¥å­é›†åˆã€‚
  - A set of sentences in a formal language.

- **æ¨ç†å¼•æ“ï¼ˆInference Engineï¼‰**ï¼šæ‰§è¡Œæ¨ç†çš„ç®—æ³•ï¼Œä¸é¢†åŸŸæ— å…³ã€‚
  - Domain-independent algorithms.

- **å£°æ˜å¼æ–¹æ³•ï¼ˆDeclarative Approachï¼‰**ï¼š
  - å‘Šè¯‰ä»£ç†å®ƒéœ€è¦çŸ¥é“çš„å†…å®¹ï¼ˆTellï¼‰ï¼Œä»£ç†è‡ªè¡Œæ¨ç†è¯¥åšä»€ä¹ˆï¼ˆAskï¼‰ã€‚
  - Tell it what it needs to know, so it can Ask itself what to do.

---

## 2. é€»è¾‘ä»£ç†çš„ç»“æ„ | Structure of a Logic Agent

- **æ„ŸçŸ¥ â†’ æ›´æ–°çŸ¥è¯†åº“ â†’ æŸ¥è¯¢åŠ¨ä½œ â†’ æ‰§è¡Œ**
  - **Percept â†’ Update KB â†’ Query Action â†’ Execute**

- **ä¼ªä»£ç ç¤ºä¾‹**ï¼š

```lua
function KBAgent(percept):
  static KB, t = 0
  tell(KB, makePerceptSentence(percept, t))
  action = ask(KB, makeActionQuery(t))
  tell(KB, makeActionSentence(action, t))
  t = t + 1
  return action
```

---

## 3. Wumpus World ç¯å¢ƒ | Wumpus World Environment

### ç¯å¢ƒè§„åˆ™ï¼š

- **Stench**ï¼šä¸Wumpusç›¸é‚»çš„æ ¼å­æœ‰è‡­å‘³ã€‚
- **Breeze**ï¼šä¸Pitç›¸é‚»çš„æ ¼å­æœ‰å¾®é£ã€‚
- **Glitter**ï¼šé‡‘å­æ‰€åœ¨çš„æ ¼å­é—ªå…‰ã€‚
- **Shoot**ï¼šå°„æ€é¢å‰çš„Wumpusï¼ˆåªæœ‰ä¸€æ”¯ç®­ï¼‰ã€‚
- **Grab**ï¼šæ¡èµ·é‡‘å­ï¼ˆè‹¥åœ¨åŒä¸€æ ¼ï¼‰ã€‚

### æ€§èƒ½è¯„ä¼°ï¼š

- æ‰¾åˆ°é‡‘å­ï¼š+1000
- æ­»äº¡ï¼š-1000
- æ¯èµ°ä¸€æ­¥ï¼š-1
- ä½¿ç”¨ç®­ï¼š-10

---

### ç¯å¢ƒç‰¹æ€§ï¼š

- **å¯è§‚å¯Ÿæ€§**ï¼šå¦ï¼Œåªæœ‰å±€éƒ¨æ„ŸçŸ¥ã€‚
- **ç¡®å®šæ€§**ï¼šæ˜¯ï¼Œç»“æœæ˜ç¡®ã€‚
- **é™æ€æ€§**ï¼šæ˜¯ï¼ŒWumpuså’ŒPitä¸åŠ¨ã€‚
- **ç¦»æ•£æ€§**ï¼šæ˜¯ã€‚
- **å•ä»£ç†**ï¼šæ˜¯ï¼ŒWumpusè§†ä¸ºè‡ªç„¶ç‰¹å¾ã€‚

---

# ğŸ“˜ Lecture 17: Logic Agents II â€“ é€»è¾‘ä»£ç†ï¼ˆäºŒï¼‰

## 1. é€»è¾‘åŸºç¡€ | Logic Basics

### è¯­æ³•ä¸è¯­ä¹‰ï¼š

- **è¯­æ³•ï¼ˆSyntaxï¼‰**ï¼šå®šä¹‰è¯­è¨€ä¸­çš„åˆæ³•å¥å­ã€‚
- **è¯­ä¹‰ï¼ˆSemanticsï¼‰**ï¼šå®šä¹‰å¥å­çš„çœŸå€¼æ¡ä»¶ã€‚

> ä¾‹ï¼š  
> \( x + 2 \geq y \) æ˜¯åˆæ³•å¥å­ï¼›  
> \( x^2 + y > \) ä¸æ˜¯åˆæ³•å¥å­ã€‚

---

## 2. è•´å«ä¸æ¨¡å‹ | Entailment and Models

- **è•´å«ï¼ˆEntailmentï¼‰**ï¼š  
  \( KB \models \alpha \) è¡¨ç¤ºåœ¨æ‰€æœ‰ \( KB \) ä¸ºçœŸçš„ä¸–ç•Œä¸­ï¼Œ\( \alpha \) ä¹Ÿä¸ºçœŸã€‚

- **æ¨¡å‹ï¼ˆModelï¼‰**ï¼šä¸€ä¸ªä½¿å¥å­ä¸ºçœŸçš„ä¸–ç•Œç»“æ„ã€‚

- **æ¨¡å‹é›†åˆ**ï¼š  
  \( M(KB) \subseteq M(\alpha) \) è¡¨ç¤º \( KB \) è•´å« \( \alpha \)ã€‚

---

## 3. Wumpus World ä¸­çš„æ¨ç† | Inference in Wumpus World

### ä½¿ç”¨å‘½é¢˜é€»è¾‘ï¼š

- \( P_{i,j} \)ï¼šæ ¼å­ \([i,j]\) æœ‰é™·é˜±ã€‚
- \( B_{i,j} \)ï¼šæ ¼å­ \([i,j]\) æœ‰å¾®é£ã€‚

è§„åˆ™ç¤ºä¾‹ï¼š

- \( B_{1,1} \Leftrightarrow (P_{1,2} \lor P_{2,1}) \)
- \( B_{2,1} \Leftrightarrow (P_{1,1} \lor P_{2,2} \lor P_{3,1}) \)

---

## 4. æ¨ç†æ–¹æ³• | Inference Methods

### a. æšä¸¾æ³• | Inference by Enumeration

- éå†æ‰€æœ‰å¯èƒ½çš„çœŸå€¼èµ‹å€¼ï¼Œæ£€æŸ¥ \( KB \) ä¸ºçœŸæ—¶ \( \alpha \) æ˜¯å¦ä¹Ÿä¸ºçœŸã€‚
- å¤æ‚åº¦ï¼š\( O(2^n) \)ï¼ŒNPå®Œå…¨é—®é¢˜ã€‚

### b. å‰å‘é“¾æ¥ | Forward Chaining

- ä»å·²çŸ¥äº‹å®å‡ºå‘ï¼Œåº”ç”¨è§„åˆ™ç›´åˆ°æ¨å‡ºç›®æ ‡ã€‚
- é€‚ç”¨äº Horn å­å¥ï¼ˆæœ€å¤šä¸€ä¸ªæ­£æ–‡å­—ï¼‰ã€‚

> ä¾‹ï¼š  
> è§„åˆ™ï¼š\( A \land B \Rightarrow L \)ï¼Œå·²çŸ¥ \( A \) å’Œ \( B \)ï¼Œåˆ™æ¨å‡º \( L \)ã€‚

### c. åå‘é“¾æ¥ | Backward Chaining

- ä»ç›®æ ‡å‡ºå‘ï¼Œåå‘å¯»æ‰¾æ”¯æŒç›®æ ‡çš„å­ç›®æ ‡ã€‚
- é€‚ç”¨äºç›®æ ‡å¯¼å‘çš„æ¨ç†ã€‚

---

## 5. å‰å‘ vs åå‘é“¾æ¥ | Forward vs Backward Chaining

| ç±»å‹ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| å‰å‘é“¾æ¥ | æ•°æ®é©±åŠ¨ï¼Œè‡ªåŠ¨å¤„ç† | å¯¹è±¡è¯†åˆ«ã€å¸¸è§„å†³ç­– |
| åå‘é“¾æ¥ | ç›®æ ‡é©±åŠ¨ï¼Œæœç´¢å¯¼å‘ | é—®é¢˜è§£å†³ã€è§„åˆ’ |

---

# ğŸ“˜ Lecture 18: Logic Agents III â€“ é€»è¾‘ä»£ç†ï¼ˆä¸‰ï¼‰

## 1. ä¸€é˜¶é€»è¾‘ | First-Order Logic (FOL)

### åŸºæœ¬å…ƒç´ ï¼š

- **å¸¸é‡**ï¼šKingJohn, 2, CMU, ...
- **è°“è¯**ï¼šBrother, >, ...
- **å‡½æ•°**ï¼šSqrt, LeftLegOf, ...
- **å˜é‡**ï¼šx, y, a, b, ...
- **é‡è¯**ï¼šâˆ€ï¼ˆå…¨ç§°ï¼‰ï¼Œâˆƒï¼ˆå­˜åœ¨ï¼‰

---

## 2. è°“è¯ä¸åŸå­å¥ | Predicates and Atomic Sentences

- è°“è¯è¡¨ç¤ºå…³ç³»æˆ–å±æ€§ã€‚
- åŸå­å¥ï¼š\( P(x) \), \( Q(x, y) \)

> ä¾‹ï¼š  
> \( P(Wumpus) \) è¡¨ç¤ºâ€œWumpus æ˜¯è‡­çš„â€ã€‚

---

## 3. åˆä¸€ | Unification

- ä½¿ä¸¤ä¸ªè¡¨è¾¾å¼åœ¨è¯­æ³•ä¸Šä¸€è‡´çš„è¿‡ç¨‹ã€‚

> ä¾‹ï¼š  
> \( UNIFY(Knows(John, x), Knows(John, Jane)) = \{x/Jane\} \)

---

## 4. å¹¿ä¹‰Modus Ponens | Generalized Modus Ponens (GMP)

- ç”¨äºä¸€é˜¶é€»è¾‘çš„æ¨ç†è§„åˆ™ï¼š

\[
\frac{p_1', \ldots, p_n', \quad (p_1 \land \cdots \land p_n \Rightarrow q)}{q\theta}
\]

> ä¾‹ï¼š  
> è§„åˆ™ï¼š\( King(x) \land Greedy(x) \Rightarrow Evil(x) \)  
> äº‹å®ï¼š\( King(John) \), \( Greedy(John) \)  
> ç»“è®ºï¼š\( Evil(John) \)

---

## 5. å‰å‘ä¸åå‘é“¾æ¥åœ¨ä¸€é˜¶é€»è¾‘ä¸­ | Forward and Backward Chaining in FOL

### å‰å‘é“¾æ¥ï¼š

- ä»å·²çŸ¥äº‹å®å‡ºå‘ï¼Œåº”ç”¨è§„åˆ™ç›´åˆ°æ¨å‡ºç›®æ ‡ã€‚
- é€‚ç”¨äº Datalogï¼ˆæ— å‡½æ•°çš„ä¸€é˜¶å®šå¥å­å¥ï¼‰ã€‚

### åå‘é“¾æ¥ï¼š

- ä»ç›®æ ‡å‡ºå‘ï¼Œåå‘å¯»æ‰¾æ”¯æŒç›®æ ‡çš„å­ç›®æ ‡ã€‚
- ç”¨äºé€»è¾‘ç¼–ç¨‹ï¼ˆå¦‚ Prologï¼‰ã€‚

---

## 6. ç¤ºä¾‹çŸ¥è¯†åº“ï¼šæ­¦å™¨è´©å–æ¡ˆä¾‹ | Example KB: Weapons Sale

### è§„åˆ™ï¼š

- \( American(x) \land Weapon(y) \land Sells(x, y, z) \land Hostile(z) \Rightarrow Criminal(x) \)
- \( Missile(x) \Rightarrow Weapon(x) \)
- \( Enemy(x, America) \Rightarrow Hostile(x) \)

### äº‹å®ï¼š

- \( American(West) \)
- \( Missile(M1) \)
- \( Owns(Nono, M1) \)
- \( Sells(West, M1, Nono) \)
- \( Enemy(Nono, America) \)

### ç»“è®ºï¼š

- \( Criminal(West) \)

---

## 7. å‰å‘ä¸åå‘é“¾æ¥çš„æ€§è´¨ | Properties of Forward and Backward Chaining

| æ–¹æ³• | å®Œå¤‡æ€§ | ç»ˆæ­¢æ€§ | é€‚ç”¨åœºæ™¯ |
|------|--------|--------|----------|
| å‰å‘é“¾æ¥ | æ˜¯ï¼ˆå®šå¥å­å¥ï¼‰ | æ˜¯ï¼ˆDatalogï¼‰ | æ•°æ®é©±åŠ¨ |
| åå‘é“¾æ¥ | æ˜¯ï¼ˆå®šå¥å­å¥ï¼‰ | å¯èƒ½ä¸ç»ˆæ­¢ | ç›®æ ‡é©±åŠ¨ |

---

# âœ… æ€»ç»“ | Summary

| ä¸»é¢˜ | å†…å®¹ |
|------|------|
| çŸ¥è¯†å‹ä»£ç† | ä½¿ç”¨ KB å’Œæ¨ç†å¼•æ“è¿›è¡Œæ¨ç† |
| å‘½é¢˜é€»è¾‘ | è¯­æ³•ã€è¯­ä¹‰ã€è•´å«ã€æ¨¡å‹ |
| Wumpus World | ç¯å¢ƒè§„åˆ™ä¸é€»è¾‘å»ºæ¨¡ |
| æ¨ç†æ–¹æ³• | æšä¸¾ã€å‰å‘é“¾æ¥ã€åå‘é“¾æ¥ |
| ä¸€é˜¶é€»è¾‘ | è°“è¯ã€é‡è¯ã€åˆä¸€ã€GMP |
| å®é™…åº”ç”¨ | æ­¦å™¨è´©å–æ¡ˆä¾‹ã€å‰å‘/åå‘é“¾æ¥ |

---


# äººå·¥æ™ºèƒ½åŸºç¡€ â€” ä¸ç¡®å®šæ€§ä¸æ¦‚ç‡æ¨ç†ï¼ˆLecture 19â€“21ï¼‰

## ç›®å½• / Contents

1. æ¨ç†ä¸­çš„ä¸ç¡®å®šæ€§ï¼ˆReasoning Under Uncertaintyï¼‰
2. å†³ç­–è®ºä¸å†³ç­–-ç†è®ºä»£ç†ï¼ˆDecision Theory & Decision-Theoretic Agentï¼‰
3. æ¦‚ç‡è®ºåŸºç¡€ï¼ˆProbability Theory Basicsï¼‰

   * å…ˆéªŒï¼ˆPriorï¼‰ä¸åˆ†å¸ƒ
   * æ¡ä»¶æ¦‚ç‡ä¸ä¹˜æ³•è§„åˆ™ï¼ˆProduct Ruleï¼‰
   * è”åˆåˆ†å¸ƒä¸ç‹¬ç«‹æ€§ï¼ˆJoint Distribution & Independenceï¼‰
   * è¾¹ç¼˜åŒ–ï¼ˆMarginalisationï¼‰ä¸æ¡ä»¶åŒ–ï¼ˆConditioningï¼‰
4. è´å¶æ–¯è§„åˆ™ï¼ˆBayesâ€™ Ruleï¼‰ä¸ä¾‹é¢˜ï¼ˆCookie Problemï¼‰
5. é“¾å¼æ³•åˆ™ï¼ˆChain Ruleï¼‰ä¸ç®€åŒ–ï¼ˆConditional Independenceï¼‰
6. è´å¶æ–¯ç½‘ç»œï¼ˆBayesian Networksï¼‰

   * å®šä¹‰ä¸è¯­ä¹‰
   * æ¡ä»¶æ¦‚ç‡è¡¨ï¼ˆCPTï¼‰
   * Markov Blanket ä¸æ¡ä»¶ç‹¬ç«‹æ€§
   * æ¨ç†ï¼ˆInferenceï¼‰æ­¥éª¤ä¸ç¤ºä¾‹ï¼ˆToothacheã€Alarm networksï¼‰
7. å°ç»“ä¸å¤ä¹ é¢˜ï¼ˆSummary & Practice Questionsï¼‰

---

### 1. æ¨ç†ä¸­çš„ä¸ç¡®å®šæ€§ / Reasoning Under Uncertainty

**ä¸­æ–‡**ï¼šç°å®ä¸–ç•Œçš„é—®é¢˜å¸¸åŒ…å«ä¸ç¡®å®šæ€§ï¼Œæ¥æºåŒ…æ‹¬éƒ¨åˆ†å¯è§‚æµ‹ï¼ˆpartial observabilityï¼‰ã€éç¡®å®šæ€§ï¼ˆnondeterminismï¼‰å’Œå¯¹æ‰‹ï¼ˆadversariesï¼‰ã€‚ç”¨çº¯é€»è¾‘è¡¨è¾¾ï¼ˆâ€œå¦‚æœâ€¦â€¦åˆ™â€¦â€¦â€ï¼‰åœ¨è¿™ç§åœºæ™¯ä¸‹å¾€å¾€ä¸ç°å®æˆ–ä¼šéå¸¸è‡ƒè‚¿ï¼ˆä¾‹å¦‚ç‰™ç—›ä¸ä¸€å®šæ˜¯è›€ç‰™ï¼‰ã€‚
**English**: Real-world problems contain uncertainty due to partial observability, nondeterminism, or adversaries. Pure logical rules (ifâ†’then) often fail or become impractically large (e.g., toothache â†’ cavity is not exhaustive). 

**é€šä¿—è§£é‡Š / Plain English**ï¼šé€»è¾‘æƒ³æŠŠæ¯ç§å¯èƒ½éƒ½åˆ—å‡ºæ¥ï¼Œä½†ç°å®å¤ªå¤æ‚ï¼Œæˆ‘ä»¬ç”¨æ¦‚ç‡æ¥è¡¨è¾¾â€œç›¸ä¿¡ç¨‹åº¦â€ï¼ˆdegree of beliefï¼‰ã€‚
**Exampleï¼ˆä¾‹å­ï¼‰**ï¼šç‰™ç—›å¯èƒ½æ˜¯è›€ç‰™ã€ç‰™é¾ˆé—®é¢˜ã€è„“è‚¿ç­‰å¤šä¸ªåŸå› ä¸­çš„ä¸€ä¸ªï¼Œç”¨æ¦‚ç‡æ¯”ç”¨ä¸€é•¿ä¸²è§„åˆ™æ›´åˆé€‚ã€‚ 

---

### 2. å†³ç­–è®ºï¼ˆDecision Theoryï¼‰ä¸å†³ç­–-ç†è®ºä»£ç†ï¼ˆDecision-Theoretic Agentï¼‰

**ä¸­æ–‡**ï¼šå†³ç­–è®ºæŠŠæ¦‚ç‡ï¼ˆbeliefsï¼‰å’Œæ•ˆç”¨ï¼ˆutilityï¼‰ç»“åˆèµ·æ¥ï¼šç†æ€§ä»£ç†åº”è¯¥é€‰æ‹©ä½¿æœŸæœ›æ•ˆç”¨ï¼ˆexpected utilityï¼‰æœ€å¤§çš„åŠ¨ä½œï¼ˆåŸåˆ™ï¼šæœ€å¤§æœŸæœ›æ•ˆç”¨ MEUï¼‰ã€‚
**English**: Decision theory = probability theory + utility theory. A rational agent chooses the action with maximum expected utility (MEU). 

**ä»£ç†ä¼ªä»£ç  / Agent pseudocode**ï¼ˆä¸­æ–‡+è‹±æ–‡ï¼‰ï¼š

```text
function DT-Agent(percept) returns action
 persistent: beliefState  â–· agent's probabilistic beliefs about world
 update beliefState based on action and percept
 calculate outcome probabilities for actions given beliefState
 select action with highest expected utility
 return action
```

ï¼ˆä¸­æ–‡ï¼‰è§£é‡Šï¼šä»£ç†ç»´æŠ¤æ¦‚ç‡æ€§çš„ä¿¡å¿µçŠ¶æ€ï¼ˆbeliefStateï¼‰ï¼Œä»¥æ­¤è®¡ç®—å„åŠ¨ä½œå¯èƒ½äº§ç”Ÿç»“æœçš„æ¦‚ç‡ï¼Œå†ç»“åˆæ•ˆç”¨å‡½æ•°æŒ‘é€‰æœ€ä¼˜åŠ¨ä½œã€‚ 

---

### 3. æ¦‚ç‡è®ºåŸºç¡€ / Probability Theory Basics

#### 3.1 éšæœºå˜é‡ä¸å…ˆéªŒï¼ˆRandom variables & Priorsï¼‰

**ä¸­æ–‡**ï¼šæˆ‘ä»¬æŠŠå‘½é¢˜æˆ–é‡åŒ–å¯¹è±¡è§†ä¸ºéšæœºå˜é‡ï¼ˆBooleanã€ç¦»æ•£æˆ–è¿ç»­ï¼‰ã€‚å…ˆéªŒæ¦‚ç‡ï¼ˆpriorï¼‰æ˜¯åœ¨æ²¡æœ‰é¢å¤–ä¿¡æ¯æ—¶å¯¹äº‹ä»¶çš„ç›¸ä¿¡ç¨‹åº¦ï¼Œä¾‹å¦‚å…¬å¹³ç¡¬å¸ P(heads)=0.5ã€‚
**English**: We treat propositions as random variables (Boolean, discrete, continuous). Prior probabilities indicate degree of belief absent extra evidence (e.g., P(heads)=0.5). 

#### 3.2 æ¡ä»¶æ¦‚ç‡ä¸ä¹˜æ³•è§„åˆ™ï¼ˆConditional probability & Product ruleï¼‰

**ä¸­æ–‡**ï¼šæ¡ä»¶æ¦‚ç‡å®šä¹‰ä¸º (P(a \mid b) = \dfrac{P(a \land b)}{P(b)})ã€‚ä¹˜æ³•è§„åˆ™ï¼š(P(a \land b) = P(a \mid b)P(b))ã€‚
**English**: Conditional probability: (P(a|b)=P(a\land b)/P(b)). Product rule: (P(a\land b)=P(a|b)P(b)). 

**ä¾‹å­ / Example**ï¼šæŠ•ä¸¤æ¬¡ç¡¬å¸ï¼Œç‹¬ç«‹æ—¶ (P(H,H)=P(H)\times P(H))ã€‚å¦‚æœæ¯æ¬¡ (P(H)=0.5)ï¼Œé‚£ä¹ˆ (P(H,H)=0.25)ã€‚ 

## 3.3 è”åˆåˆ†å¸ƒä¸ç‹¬ç«‹æ€§ï¼ˆJoint distribution & Independenceï¼‰

**ä¸­æ–‡**ï¼šè”åˆæ¦‚ç‡åˆ†å¸ƒ (P(X_1,\dots,X_n)) ç»™å‡ºæ‰€æœ‰å˜é‡åŒæ—¶å–æŸå€¼çš„æ¦‚ç‡ã€‚è‹¥å˜é‡ç‹¬ç«‹ï¼Œè”åˆæ¦‚ç‡å¯å†™æˆå„è‡ªæ¦‚ç‡çš„ä¹˜ç§¯ï¼›å¦åˆ™ä¸èƒ½ã€‚ç‹¬ç«‹çš„å®šä¹‰æ˜¯ (P(b|a)=P(b))ã€‚
**English**: Joint distribution gives probability of all variables taking specific values. If variables are independent, joint equals product of marginals; independence means (P(b|a)=P(b)). 

#### 3.4 è¾¹ç¼˜åŒ–ï¼ˆMarginalisationï¼‰ä¸æ¡ä»¶åŒ–ï¼ˆConditioningï¼‰

**ä¸­æ–‡**ï¼šè¦å¾—åˆ°æŸäº›å˜é‡çš„æ¦‚ç‡ï¼Œå¯ä»¥å¯¹å…¶ä»–å˜é‡æ±‚å’Œï¼ˆè¾¹ç¼˜åŒ–ï¼‰ã€‚ç»™å®šè¯æ® (E=e)ï¼Œè¦è®¡ç®—æŸ¥è¯¢å˜é‡ (X) çš„åéªŒï¼Œä½¿ç”¨æ¡ä»¶åŒ–ï¼š
[
P(X=x \mid E=e) = \frac{P(X=x, E=e)}{P(E=e)} = \alpha P(X=x, E=e)
]
å…¶ä¸­ (\alpha = 1/P(E=e)) ä¸ºå½’ä¸€åŒ–å¸¸æ•°ã€‚
**English**: Marginalise by summing out irrelevant variables. Conditioning gives posterior (P(X|E)=P(X,E)/P(E)=\alpha P(X,E)), where (\alpha) normalises. 

**ä¾‹å­ï¼ˆç‰™ç—›/æ¢é’ˆ/è›€ç‰™è¡¨ï¼‰**ï¼šè¯¾ä»¶ç»™å‡ºå®Œæ•´è”åˆè¡¨å¹¶é€šè¿‡è¾¹ç¼˜åŒ–/æ¡ä»¶åŒ–å¾—åˆ° (P(\text{Cavity})=0.2) æˆ–è€… (P(\text{Cavity} \mid \text{Catch}=\text{true})=0.53) çš„è®¡ç®—ç¤ºä¾‹ï¼ˆè§è¯¾ä»¶æ•°æ®ï¼‰ã€‚ 

---

### 4. è´å¶æ–¯è§„åˆ™ï¼ˆBayesâ€™ Ruleï¼‰ä¸ Cookie é—®é¢˜ï¼ˆCookie Problemï¼‰

**è´å¶æ–¯å…¬å¼ / Bayes' Rule**ï¼š
[
P(a\mid b)=\frac{P(b\mid a)P(a)}{P(b)}
]
ï¼ˆä¸­æ–‡ï¼‰è§£é‡Šï¼šæŠŠâ€œæˆ‘ä»¬æƒ³è¦çš„åéªŒï¼ˆa|bï¼‰â€æ¢æˆâ€œå·²çŸ¥ a æ—¶ b çš„æ¦‚ç‡ Ã— a çš„å…ˆéªŒâ€ï¼Œå†é™¤ä»¥ b çš„è¾¹ç¼˜æ¦‚ç‡ã€‚
ï¼ˆEnglishï¼‰Interpretation: express posterior via likelihood Ã— prior normalized by evidence. 

**Cookie Problemï¼ˆé¥¼å¹²é—®é¢˜ï¼Œå®Œæ•´ç®—å¼ï¼‰**ï¼ˆä¸­æ–‡+è‹±æ–‡ï¼‰ï¼š

* Bowl1: 30 vanilla, 10 chocolate â†’ (P(\text{vanilla}|B1)=30/40=3/4)
* Bowl2: 20 vanilla, 20 chocolate â†’ (P(\text{vanilla}|B2)=20/40=1/2)
* é€‰ç¢—ç­‰æ¦‚ç‡ (P(B1)=P(B2)=1/2)
  æˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸€å—é¦™è‰é¥¼å¹²ï¼ˆvanillaï¼‰ï¼Œæ±‚æ¥è‡ª Bowl1 çš„æ¦‚ç‡ï¼š
  [
  P(B1\mid vanilla)=\frac{P(vanilla\mid B1)P(B1)}{P(vanilla)}
  =\frac{(3/4)\times(1/2)}{(3/4)\times(1/2)+(1/2)\times(1/2)}=\frac{3/8}{5/8}=\frac{3}{5}
  ]
  **English**: Using Bayes, (P(B1|V)=3/5). 

**å®ç”¨æç¤º / Practical note**ï¼šåœ¨å®é™…é—®é¢˜ä¸­é€šå¸¸æŠŠå³è¾¹æ‹†æˆï¼šå…ˆéªŒï¼ˆpriorï¼‰ã€ä¼¼ç„¶ï¼ˆlikelihoodï¼‰å’Œè¯æ®ï¼ˆevidenceï¼‰ï¼Œè¯æ®å¸¸ç”¨å…¨æ¦‚ç‡ï¼ˆlaw of total probabilityï¼‰è®¡ç®—ã€‚ 

---

### 5. é“¾å¼æ³•åˆ™ï¼ˆChain Ruleï¼‰ä¸æ¡ä»¶ç‹¬ç«‹çš„ç®€åŒ–ä½œç”¨

**é“¾å¼æ³•åˆ™ / Chain rule**ï¼š
[
P(X_1,\dots,X_n)=\prod_{i=1}^n P(X_i \mid X_{1},\dots,X_{i-1})
]
ï¼ˆä¸­æ–‡ï¼‰è§£é‡Šï¼šä»»æ„è”åˆå¯æŒ‰æ¡ä»¶æ¦‚ç‡åˆ†è§£ï¼Œä½†æ¯ä¸€é¡¹å¯èƒ½ä»ä¾èµ–å¾ˆå¤šå‰é©±å˜é‡ã€‚
**English**: Chain rule expresses any joint as product of conditional probabilities. 

**æ¡ä»¶ç‹¬ç«‹ (Conditional Independence)**ï¼šå¦‚æœåœ¨ç»™å®šæŸäº›å˜é‡ï¼ˆä¾‹å¦‚åŸå› ï¼‰æ—¶ï¼Œå¤šä¸ªå­å˜é‡ç›¸äº’ç‹¬ç«‹ï¼Œé‚£ä¹ˆå¯æŠŠå¤æ‚è¡¨æ ¼å¤§å¹…å‹ç¼©ï¼š
[
P(\text{Cause}, \text{Effect}_1,\dots,\text{Effect}_n)=P(\text{Cause})\prod_i P(\text{Effect}_i\mid \text{Cause})
]
ï¼ˆä¸­æ–‡ï¼‰è¿™ç§æ¨¡å¼ä½¿è¡¨ä» (O(2^n)) ç¼©å°åˆ° (O(n))ã€‚ 

---

### 6. è´å¶æ–¯ç½‘ç»œï¼ˆBayesian Networks, BNsï¼‰

#### 6.1 å®šä¹‰ä¸ç›´è§‚è¯­ä¹‰

**ä¸­æ–‡**ï¼šè´å¶æ–¯ç½‘ç»œæ˜¯æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ï¼ŒèŠ‚ç‚¹æ˜¯éšæœºå˜é‡ï¼Œè¾¹è¡¨ç¤ºç›´æ¥å½±å“ï¼ˆçˆ¶â†’å­ï¼‰ã€‚æ¯ä¸ªèŠ‚ç‚¹å¸¦æ¡ä»¶æ¦‚ç‡è¡¨ï¼ˆCPTï¼‰ï¼š(P(X_i \mid Parents(X_i)))ã€‚æ•´ç½‘çš„è”åˆæ¦‚ç‡å¯å†™ä¸ºï¼š
[
P(x_1,\dots,x_n)=\prod_{i=1}^n P(x_i \mid parents(x_i))
]
**English**: A BN is a DAG where nodes are random variables and edges encode direct influence. Joint is product of local conditionals. 

#### 6.2 CPT ä¸å‚æ•°æ•°é‡ä¸¾ä¾‹ï¼ˆToothache ç½‘ç»œï¼‰

ï¼ˆä¸­æ–‡ï¼‰ä¾‹å­ï¼šToothacheâ€”Cavityâ€”Catchâ€”Weather çš„å°ç½‘ç»œï¼Œè¯¾ä»¶ç»™å‡º P(Cavity)=0.2, P(Weather)=0.5ï¼Œä»¥åŠ (P(Toothache|Cavity)), (P(Catch|Cavity)) çš„è¡¨æ ¼ã€‚å¯¹äºå®Œå…¨æŒ‡å®šçš„æƒ…å†µï¼Œç›´æ¥æŠŠç›¸å…³æ¦‚ç‡ç›¸ä¹˜å³å¯ï¼Œä¾‹å¦‚ï¼š
[
P(Cav \land Cat \land T) = P(Cav)P(T\mid Cav)P(Cat\mid Cav)=0.2\times0.6\times0.9=0.108
]
ï¼ˆEnglishï¼‰Partially specified queries need marginalisation and normalization. 

#### 6.3 æ‹“æ‰‘è¯­ä¹‰ä¸æ¡ä»¶ç‹¬ç«‹ï¼ˆMarkov assumptionsï¼‰

**ä¸­æ–‡**ï¼šåœ¨ BN ä¸­ï¼Œæ¯ä¸ªèŠ‚ç‚¹åœ¨ç»™å®šå…¶çˆ¶èŠ‚ç‚¹æ¡ä»¶ä¸‹ï¼Œä¸å…¶éåä»£ï¼ˆnon-descendantsï¼‰æ¡ä»¶ç‹¬ç«‹ã€‚èŠ‚ç‚¹çš„ Markov blanketï¼ˆçˆ¶ã€å­ã€å­ä¹‹çˆ¶ï¼‰æŠŠè¯¥èŠ‚ç‚¹ä¸ç½‘ç»œå…¶å®ƒéƒ¨åˆ†éš”ç¦»ã€‚
**English**: Each node is conditionally independent of non-descendants given its parents. The Markov blanket (parents, children, children's parents) shields the node from the rest. 

#### 6.4 æ¨ç†ï¼ˆInferenceï¼‰åœ¨ BN ä¸­çš„æ­¥éª¤ï¼ˆä¸­æ–‡+è‹±æ–‡ï¼‰

**NaÃ¯ve/åŸºäºè”åˆè¡¨çš„åšæ³•**ï¼š

* æŠŠå˜é‡åˆ†æˆï¼šæŸ¥è¯¢å˜é‡ï¼ˆQueryï¼‰ã€è¯æ®å˜é‡ï¼ˆEvidenceï¼‰ã€éšè—å˜é‡ï¼ˆHiddenï¼‰
* æŠŠæ¶‰åŠçš„ CPT è¡Œé€‰å‡ºæ¥ï¼Œå›ºå®šè¯æ®è¡Œï¼ˆconditioningï¼‰ï¼Œå¯¹éšè—å˜é‡æ±‚å’Œï¼ˆmarginaliseï¼‰ï¼Œæœ€åå½’ä¸€åŒ–ï¼ˆnormalizeï¼‰å¾—åˆ°æ¦‚ç‡åˆ†å¸ƒã€‚
  **English**: Separate variables into Query, Evidence, Hidden. Sum out hidden variables, fix evidence, compute unnormalized probabilities for query values, then normalise.

**ç¤ºä¾‹ï¼šAlarm / Burglary ç½‘ç»œï¼ˆè¯¾ä»¶æ•°æ®ï¼‰**
è¯¾ä»¶ç»™å‡ºçš„ç½‘ç»œåŠ CPTï¼ˆéƒ¨åˆ†å…³é”®å€¼ï¼‰ï¼š

* (P(Burglary)=0.001, P(Earthquake)=0.002) ç­‰ï¼›
* (P(Alarm|B=true,E=true)=0.95) ç­‰ï¼›
* (P(JohnCalls|Alarm=true)=0.9, P(MaryCalls|Alarm=true)=0.7)ã€‚
  ï¼ˆä¸­æ–‡ï¼‰é—®ï¼šå½“è­¦æŠ¥å“ä½†æ²¡æœ‰åœ°éœ‡æ—¶ï¼ŒæŠ¥è­¦æ¥è‡ªå…¥å®¤ç›—çªƒçš„æ¦‚ç‡ï¼ˆè®¡ç®—ç¤ºä¾‹ï¼‰â€”â€”è¯¾ä»¶ç»™å‡ºé€é¡¹ä¹˜åŠ å¹¶å½’ä¸€åŒ–çš„è®¡ç®—ï¼Œæœ€ç»ˆè¿‘ä¼¼ (P(\text{Burglary} \mid Alarm) \approx \langle 0.97, 0.03\rangle)ï¼ˆåˆ†é…åˆ° true / false çš„æ¯”ä¾‹ï¼‰ã€‚
  **English**: The Alarm/Burglary worked example shows summing combinations of John/Mary call outcomes, multiplying by relevant CPT entries and priors, then normalizing to get posterior â‰ˆ âŸ¨0.97,0.03âŸ© for (alarm due to burglary vs not) in that scenario. 

**è®¡ç®—æ­¥éª¤é«˜å±‚æ¬¡åˆ—ä¸¾ï¼ˆä¸­æ–‡ï¼‰**ï¼š

1. å°†æ‰€æœ‰æ¶‰åŠé¡¹æŒ‰é“¾å¼æ³•åˆ™å±•å¼€ï¼ˆchain ruleï¼‰ã€‚
2. åˆ©ç”¨å·²çŸ¥çš„æ¡ä»¶ç‹¬ç«‹æ€§å°†æ¡ä»¶æ¦‚ç‡æ›¿æ¢ä¸ºå±€éƒ¨CPTï¼ˆå¤§é‡é¡¹è¢«å¿½ç•¥ï¼‰ã€‚
3. å¯¹éšè—å˜é‡æ±‚å’Œå¾—åˆ°éå½’ä¸€åŒ–çš„ query å€¼ã€‚
4. å½’ä¸€åŒ–ï¼ˆé™¤ä»¥è¯æ®æ¦‚ç‡æˆ–ç”¨ Î± å¸¸æ•°ï¼‰ã€‚
   ï¼ˆEnglishï¼‰This yields a correct posterior without enumerating an exponentially large joint table. 

---

### 7. å°ç»“ï¼ˆSummaryï¼‰

* ä¸ç¡®å®šæ€§æ˜¯AIä¸­çš„æ ¸å¿ƒé—®é¢˜ï¼Œæ¦‚ç‡è®ºæä¾›è¡¨è¾¾â€œç›¸ä¿¡ç¨‹åº¦â€çš„å·¥å…·ï¼›Decision theory æŠŠæ¦‚ç‡ä¸æ•ˆç”¨ç»“åˆåšå†³å®šã€‚ 
* æ¦‚ç‡åŸºç¡€åŒ…æ‹¬å…ˆéªŒã€æ¡ä»¶æ¦‚ç‡ã€è”åˆåˆ†å¸ƒã€è¾¹ç¼˜åŒ–ä¸æ¡ä»¶åŒ–ï¼Œè´å¶æ–¯è§„åˆ™æŠŠåéªŒä¸ä¼¼ç„¶å’Œå…ˆéªŒè”ç³»èµ·æ¥ã€‚
* è´å¶æ–¯ç½‘ç»œé€šè¿‡å›¾ç»“æ„+å±€éƒ¨ CPT å‹ç¼©è¡¨ç¤ºè”åˆåˆ†å¸ƒï¼Œæ¡ä»¶ç‹¬ç«‹æ€§æ˜¯å…³é”®çš„è¯­ä¹‰å·¥å…·ï¼›æ¨ç†é€šè¿‡æ±‚å’Œï¼ˆmarginaliseï¼‰ã€å›ºå®šè¯æ®ï¼ˆconditionï¼‰ã€å’Œå½’ä¸€åŒ–å®ç°ã€‚

---
# äººå·¥æ™ºèƒ½åŸºç¡€å­¦ä¹ ç¬”è®° - ç¬¬22è®²ï¼šæ—¶é—´ä¸ç¡®å®šæ€§ I
## Artificial Intelligence Foundation Study Notes - Lecture 22: Uncertainty over Time I

---

## 1. æ—¶é—´ä¸ä¸ç¡®å®šæ€§ | Time and Uncertainty

### 1.1 ç¦»æ•£æ—¶é—´æ¨¡å‹ | Discrete-time Models
- å°†ä¸–ç•Œè§†ä¸ºä¸€ç³»åˆ—å¿«ç…§æˆ–æ—¶é—´åˆ‡ç‰‡ | View the world as a series of snapshots or time slices
- æ—¶é—´é—´éš”Î”åœ¨æ¯ä¸ªåŒºé—´éƒ½ç›¸åŒ | The time interval Î” is the same for every interval
- **çŠ¶æ€å˜é‡** X_tï¼šåœ¨æ—¶é—´tçš„çŠ¶æ€å˜é‡ï¼ˆä¸å¯è§‚å¯Ÿï¼‰| State variables at time t (unobservable)
- **è¯æ®å˜é‡** E_tï¼šåœ¨æ—¶é—´tçš„è§‚å¯Ÿå˜é‡ï¼ˆå¯è§‚å¯Ÿï¼‰| Evidence variables at time t (observable)

### 1.2 è½¬ç§»æ¨¡å‹å’Œä¼ æ„Ÿå™¨æ¨¡å‹ | Transition and Sensor Models
**è½¬ç§»æ¨¡å‹**ï¼šP(X_t | X_{0:t-1}) - ç»™å®šå…ˆå‰çŠ¶æ€çš„æœ€æ–°çŠ¶æ€æ¦‚ç‡åˆ†å¸ƒ | Probability distribution over latest state given previous values

**é—®é¢˜**ï¼šå½“tå¢åŠ æ—¶ï¼ŒX_{0:t-1}é›†åˆçš„å¤§å°æ— é™åˆ¶ | Problem: unbounded size as t increases

**è§£å†³æ–¹æ¡ˆ**ï¼šé©¬å°”å¯å¤«å‡è®¾ | Solution: Markov Assumption
- å½“å‰çŠ¶æ€ä»…ä¾èµ–äºæœ‰é™å›ºå®šæ•°é‡çš„å…ˆå‰çŠ¶æ€ | Current state depends on only finite fixed number of previous states

**ä¼ æ„Ÿå™¨æ¨¡å‹**ï¼šP(E_t | X_t) - ä¼ æ„Ÿå™¨é©¬å°”å¯å¤«å‡è®¾ | Sensor Markov Assumption

### 1.3 é©¬å°”å¯å¤«å‡è®¾ | Markov Assumption
**ä¸€é˜¶é©¬å°”å¯å¤«è¿‡ç¨‹**ï¼šå½“å‰çŠ¶æ€ä»…ä¾èµ–äºå‰ä¸€ä¸ªçŠ¶æ€ | First-order: current state depends only on previous state
```
P(X_t | X_{0:t-1}) = P(X_t | X_{t-1})
```

**äºŒé˜¶é©¬å°”å¯å¤«è¿‡ç¨‹**ï¼šå½“å‰çŠ¶æ€ä¾èµ–äºå‰ä¸¤ä¸ªçŠ¶æ€ | Second-order: depends on previous two states

### 1.4 é›¨ä¼ä¸–ç•Œç¤ºä¾‹ | Umbrella World Example
- ä¸€é˜¶é©¬å°”å¯å¤«è¿‡ç¨‹ï¼šä¸‹é›¨æ¦‚ç‡ä»…å–å†³äºå‰ä¸€å¤©æ˜¯å¦ä¸‹é›¨ | Rain probability depends only on previous day's rain
- **è½¬ç§»æ¨¡å‹**ï¼šP(Rain_t | Rain_{t-1})
- **ä¼ æ„Ÿå™¨æ¨¡å‹**ï¼šP(Umbrella_t | Rain_t)

**æé«˜å‡†ç¡®æ€§çš„æ–¹æ³•** | Ways to improve accuracy:
- å¢åŠ é©¬å°”å¯å¤«è¿‡ç¨‹çš„é˜¶æ•° | Increase order of Markov process
- å¢åŠ çŠ¶æ€å˜é‡é›†åˆ | Increase set of state variables

---

## 2. æ—¶é—´æ¨¡å‹ä¸­çš„æ¨æ–­ | Inference in Temporal Models

### 2.1 åŸºæœ¬æ¨æ–­ä»»åŠ¡ | Basic Inference Tasks

#### 2.1.1 è¿‡æ»¤/çŠ¶æ€ä¼°è®¡ | Filtering/State Estimation
è®¡ç®—ä¿¡å¿µçŠ¶æ€ï¼šP(X_t | e_{1:t}) | Compute belief state given all evidence

#### 2.1.2 é¢„æµ‹ | Prediction
è®¡ç®—æœªæ¥çŠ¶æ€çš„åéªŒåˆ†å¸ƒï¼šP(X_{t+k} | e_{1:t}) | Posterior distribution over future state

#### 2.1.3 å¹³æ»‘ | Smoothing
è®¡ç®—è¿‡å»çŠ¶æ€çš„åéªŒåˆ†å¸ƒï¼šP(X_k | e_{1:t})ï¼Œå…¶ä¸­k < t | Posterior over past state given all evidence

#### 2.1.4 æœ€å¯èƒ½è§£é‡Š | Most Likely Explanation
æ‰¾åˆ°æœ€å¯èƒ½ç”Ÿæˆè§‚å¯Ÿåºåˆ—çš„çŠ¶æ€åºåˆ— | Find state sequence most likely to generate observations

#### 2.1.5 å­¦ä¹  | Learning
ä»è§‚å¯Ÿä¸­å­¦ä¹ è½¬ç§»å’Œä¼ æ„Ÿå™¨æ¨¡å‹ | Learn transition and sensor models from observations

---

## 3. è¿‡æ»¤å’Œé¢„æµ‹çš„é€’å½’å…¬å¼ | Recursive Formula for Filtering and Prediction

### 3.1 é—®é¢˜è¡¨è¿° | Problem Formulation
**ç›®æ ‡**ï¼šæ¨å¯¼æ—¶é—´t+1çš„ä¿¡å¿µçŠ¶æ€ï¼Œç»™å®šï¼š
- æ–°è¯æ®e_{t+1}
- å½“å‰çš„ä¿¡å¿µçŠ¶æ€P(X_t | e_{1:t})

**æ•°å­¦è¡¨è¾¾**ï¼šæˆ‘ä»¬è¦æ±‚P(X_{t+1} | e_{1:t+1})

### 3.2 å®Œæ•´æ¨å¯¼è¿‡ç¨‹ | Complete Derivation Process

#### æ­¥éª¤1ï¼šè¯æ®åˆ†å‰² | Step 1: Evidence Partitioning
```
P(X_{t+1} | e_{1:t+1}) = P(X_{t+1} | e_{1:t}, e_{t+1})
```
è¿™é‡Œæˆ‘ä»¬å°†è¯æ®åºåˆ—e_{1:t+1}åˆ†å‰²ä¸ºå·²çŸ¥éƒ¨åˆ†e_{1:t}å’Œæ–°è¯æ®e_{t+1}

#### æ­¥éª¤2ï¼šåº”ç”¨è´å¶æ–¯å®šç† | Step 2: Apply Bayes' Theorem
æ ¹æ®è´å¶æ–¯å®šç†çš„æ¡ä»¶å½¢å¼ï¼š
```
P(A | B, C) = [P(B | A, C) Ã— P(A | C)] / P(B | C)
```

åº”ç”¨åˆ°æˆ‘ä»¬çš„é—®é¢˜ï¼š
```
P(X_{t+1} | e_{1:t}, e_{t+1}) = [P(e_{t+1} | X_{t+1}, e_{1:t}) Ã— P(X_{t+1} | e_{1:t})] / P(e_{t+1} | e_{1:t})
```

ç”±äºåˆ†æ¯P(e_{t+1} | e_{1:t})æ˜¯å½’ä¸€åŒ–å¸¸æ•°ï¼Œæˆ‘ä»¬å†™ä½œÎ±ï¼š
```
P(X_{t+1} | e_{1:t+1}) = Î± Ã— P(e_{t+1} | X_{t+1}, e_{1:t}) Ã— P(X_{t+1} | e_{1:t})
```

#### æ­¥éª¤3ï¼šä¼ æ„Ÿå™¨é©¬å°”å¯å¤«å‡è®¾ | Step 3: Sensor Markov Assumption
ä¼ æ„Ÿå™¨é©¬å°”å¯å¤«å‡è®¾æŒ‡å‡ºï¼šç»™å®šå½“å‰çŠ¶æ€ï¼Œ**è§‚å¯Ÿå€¼ç‹¬ç«‹äºæ‰€æœ‰è¿‡å»çš„è§‚å¯Ÿå€¼**
```
P(e_{t+1} | X_{t+1}, e_{1:t}) = P(e_{t+1} | X_{t+1})
```

ä»£å…¥å…¬å¼ï¼š
```
P(X_{t+1} | e_{1:t+1}) = Î± Ã— P(e_{t+1} | X_{t+1}) Ã— P(X_{t+1} | e_{1:t})
```

#### æ­¥éª¤4ï¼šå±•å¼€é¢„æµ‹é¡¹ | Step 4: Expand Prediction Term
ç°åœ¨å±•å¼€P(X_{t+1} | e_{1:t})ï¼Œè¿™æ˜¯ä»æ—¶é—´tåˆ°t+1çš„é¢„æµ‹æ­¥éª¤ï¼š

```
P(X_{t+1} | e_{1:t}) = Î£_{x_t} P(X_{t+1} | x_t) Ã— P(x_t | e_{1:t})
```

è¿™é‡Œï¼š
- P(X_{t+1} | x_t)æ˜¯è½¬ç§»æ¨¡å‹
- P(x_t | e_{1:t})æ˜¯æ—¶é—´tçš„ä¿¡å¿µçŠ¶æ€
- æ±‚å’Œæ˜¯å¯¹æ‰€æœ‰å¯èƒ½çš„çŠ¶æ€x_t

#### æ­¥éª¤5ï¼šå®Œæ•´é€’å½’å…¬å¼ | Step 5: Complete Recursive Formula
å°†æ­¥éª¤4ä»£å…¥æ­¥éª¤3ï¼Œå¾—åˆ°å®Œæ•´çš„é€’å½’æ›´æ–°å…¬å¼ï¼ˆ**å…¨æ¦‚ç‡**ï¼‰ï¼š

```
P(X_{t+1} | e_{1:t+1}) = Î± Ã— P(e_{t+1} | X_{t+1}) Ã— Î£_{x_t} P(X_{t+1} | x_t) Ã— P(x_t | e_{1:t})
```

### 3.3 å…¬å¼ç»„ä»¶è§£é‡Š | Formula Components Explanation

**Î±**ï¼šå½’ä¸€åŒ–å¸¸æ•°ï¼Œç¡®ä¿æ¦‚ç‡å’Œä¸º1
```
Î± = 1 / Î£_{x_{t+1}} [P(e_{t+1} | x_{t+1}) Ã— Î£_{x_t} P(x_{t+1} | x_t) Ã— P(x_t | e_{1:t})]
```

**P(e_{t+1} | X_{t+1})**ï¼šä¼ æ„Ÿå™¨æ¨¡å‹ï¼Œæè¿°åœ¨ç»™å®šçŠ¶æ€ä¸‹è§‚å¯Ÿåˆ°è¯æ®çš„æ¦‚ç‡

**P(X_{t+1} | x_t)**ï¼šè½¬ç§»æ¨¡å‹ï¼Œæè¿°çŠ¶æ€å¦‚ä½•éšæ—¶é—´æ¼”å˜

**P(x_t | e_{1:t})**ï¼šå½“å‰ä¿¡å¿µçŠ¶æ€ï¼ŒåŒ…å«åˆ°ç›®å‰ä¸ºæ­¢çš„æ‰€æœ‰ä¿¡æ¯

**Î£_{x_t}**ï¼šå¯¹æ‰€æœ‰å¯èƒ½çš„å‰ä¸€çŠ¶æ€æ±‚å’Œï¼ˆè¾¹é™…åŒ–ï¼‰

---

## 4. é›¨ä¼ä¸–ç•Œè¯¦ç»†ç¤ºä¾‹ | Umbrella World Detailed Example

### 4.1 æ¨¡å‹å‚æ•°è®¾ç½® | Model Parameters
å‡è®¾ä»¥ä¸‹æ¦‚ç‡å€¼ï¼š

**å…ˆéªŒæ¦‚ç‡**ï¼š
- P(Rain_0 = true) = 0.5
- P(Rain_0 = false) = 0.5

**è½¬ç§»æ¨¡å‹**ï¼š
- P(Rain_t = true | Rain_{t-1} = true) = 0.7    // ä¸‹é›¨åç»§ç»­ä¸‹é›¨
- P(Rain_t = true | Rain_{t-1} = false) = 0.3   // ä¸ä¸‹é›¨åå¼€å§‹ä¸‹é›¨
- P(Rain_t = false | Rain_{t-1} = true) = 0.3  // ä¸‹é›¨ååœæ­¢ä¸‹é›¨  
- P(Rain_t = false | Rain_{t-1} = false) = 0.7 // ä¸ä¸‹é›¨åç»§ç»­ä¸ä¸‹é›¨

**ä¼ æ„Ÿå™¨æ¨¡å‹**ï¼š
- P(Umbrella_t = true | Rain_t = true) = 0.9    // ä¸‹é›¨æ—¶å¸¦ä¼
- P(Umbrella_t = true | Rain_t = false) = 0.2   // ä¸ä¸‹é›¨æ—¶å¸¦ä¼
- P(Umbrella_t = false | Rain_t = true) = 0.1   // ä¸‹é›¨æ—¶ä¸å¸¦ä¼
- P(Umbrella_t = false | Rain_t = false) = 0.8  // ä¸ä¸‹é›¨æ—¶ä¸å¸¦ä¼

### 4.2 ç¬¬1å¤©è¯¦ç»†è®¡ç®— | Day 1 Detailed Calculation

#### æ­¥éª¤1ï¼šåˆå§‹çŠ¶æ€ | Step 1: Initial State
```
P(Rain_0 = true) = 0.5
P(Rain_0 = false) = 0.5
```

#### æ­¥éª¤2ï¼šé¢„æµ‹ï¼ˆt=0â†’t=1ï¼‰| Step 2: Prediction (t=0â†’t=1)
è®¡ç®—P(Rain_1 | e_{1:0})ï¼Œç”±äºè¿˜æ²¡æœ‰è¯æ®ï¼Œe_{1:0}ä¸ºç©ºï¼š

```
P(Rain_1 = true) = P(Rain_1 = true | Rain_0 = true) Ã— P(Rain_0 = true)
                 + P(Rain_1 = true | Rain_0 = false) Ã— P(Rain_0 = false)
                 = 0.7 Ã— 0.5 + 0.3 Ã— 0.5 = 0.35 + 0.15 = 0.5

P(Rain_1 = false) = 1 - 0.5 = 0.5
```

#### æ­¥éª¤3ï¼šè§‚å¯Ÿè¯æ®ï¼ˆU_1 = trueï¼‰| Step 3: Observe Evidence
ç°åœ¨è§‚å¯Ÿåˆ°U_1 = trueï¼ˆå¸¦ä¼ï¼‰ï¼Œåº”ç”¨è´å¶æ–¯æ›´æ–°ï¼š

**è®¡ç®—æœªå½’ä¸€åŒ–çš„åéªŒ**ï¼š
```
P'(Rain_1 = true | U_1 = true) = P(U_1 = true | Rain_1 = true) Ã— P(Rain_1 = true)
                               = 0.9 Ã— 0.5 = 0.45

P'(Rain_1 = false | U_1 = true) = P(U_1 = true | Rain_1 = false) Ã— P(Rain_1 = false)
                                = 0.2 Ã— 0.5 = 0.10
```

**è®¡ç®—å½’ä¸€åŒ–å¸¸æ•°Î±**ï¼š
```
Î± = 1 / (0.45 + 0.10) = 1 / 0.55 â‰ˆ 1.8182
```

**è®¡ç®—å½’ä¸€åŒ–åéªŒ**ï¼š
```
P(Rain_1 = true | U_1 = true) = 0.45 Ã— 1.8182 â‰ˆ 0.8182
P(Rain_1 = false | U_1 = true) = 0.10 Ã— 1.8182 â‰ˆ 0.1818
```

### 4.3 ç¬¬2å¤©è¯¦ç»†è®¡ç®— | Day 2 Detailed Calculation

#### æ­¥éª¤1ï¼šé¢„æµ‹ï¼ˆt=1â†’t=2ï¼‰| Step 1: Prediction (t=1â†’t=2)
ä½¿ç”¨ç¬¬1å¤©çš„åéªŒæ¦‚ç‡è¿›è¡Œé¢„æµ‹ï¼š

```
P(Rain_2 = true | U_1 = true) = P(Rain_2 = true | Rain_1 = true) Ã— P(Rain_1 = true | U_1 = true)
                             + P(Rain_2 = true | Rain_1 = false) Ã— P(Rain_1 = false | U_1 = true)
                             = 0.7 Ã— 0.8182 + 0.3 Ã— 0.1818
                             = 0.5727 + 0.0545 = 0.6272

P(Rain_2 = false | U_1 = true) = 1 - 0.6272 = 0.3728
```

#### æ­¥éª¤2ï¼šè§‚å¯Ÿè¯æ®ï¼ˆU_2 = trueï¼‰| Step 2: Observe Evidence
å†æ¬¡è§‚å¯Ÿåˆ°å¸¦ä¼ï¼ŒU_2 = trueï¼š

**è®¡ç®—æœªå½’ä¸€åŒ–çš„åéªŒ**ï¼š
```
P'(Rain_2 = true | U_1 = true, U_2 = true) = P(U_2 = true | Rain_2 = true) Ã— P(Rain_2 = true | U_1 = true)
                                          = 0.9 Ã— 0.6272 = 0.5645

P'(Rain_2 = false | U_1 = true, U_2 = true) = P(U_2 = true | Rain_2 = false) Ã— P(Rain_2 = false | U_1 = true)
                                           = 0.2 Ã— 0.3728 = 0.0746
```

**è®¡ç®—å½’ä¸€åŒ–å¸¸æ•°**ï¼š
```
Î± = 1 / (0.5645 + 0.0746) = 1 / 0.6391 â‰ˆ 1.5647
```

**è®¡ç®—å½’ä¸€åŒ–åéªŒ**ï¼š
```
P(Rain_2 = true | U_1 = true, U_2 = true) = 0.5645 Ã— 1.5647 â‰ˆ 0.883
P(Rain_2 = false | U_1 = true, U_2 = true) = 0.0746 Ã— 1.5647 â‰ˆ 0.117
```

---

## 5. è¿‡æ»¤æ¶ˆæ¯æ¦‚å¿µ | Filtering Messages Concept

### 5.1 æ¶ˆæ¯ä¼ é€’è§†è§’ | Message Passing Perspective
æˆ‘ä»¬å¯ä»¥å°†è¿‡æ»¤è¿‡ç¨‹è§†ä¸ºæ¶ˆæ¯åœ¨å‰å‘ä¼ é€’ï¼š

**å®šä¹‰æ¶ˆæ¯**ï¼šf_{1:t} â‰¡ P(X_t | e_{1:t})

**é€’å½’æ›´æ–°**ï¼š
```
f_{1:t+1} = FORWARD(f_{1:t}, e_{t+1})
          = Î± Ã— P(e_{t+1} | X_{t+1}) Ã— Î£_{x_t} P(X_{t+1} | x_t) Ã— f_{1:t}(x_t)
```

### 5.2 ç®—æ³•æ­¥éª¤ | Algorithm Steps
1. **åˆå§‹åŒ–**ï¼šf_{1:0} = P(X_0)
2. **å¯¹äºæ¯ä¸ªæ—¶é—´æ­¥t=1,2,...**ï¼š
   - **é¢„æµ‹**ï¼šä»f_{1:t-1}è®¡ç®—P(X_t | e_{1:t-1})
   - **æ›´æ–°**ï¼šç»“åˆæ–°è¯æ®e_tå¾—åˆ°f_{1:t}
3. **é‡å¤**ç›´åˆ°å¤„ç†å®Œæ‰€æœ‰è¯æ®

è¿™ç§é€’å½’æ–¹æ³•çš„æ—¶é—´å¤æ‚åº¦æ˜¯çº¿æ€§çš„ï¼Œä½¿å¾—å®ƒèƒ½å¤Ÿé«˜æ•ˆå¤„ç†é•¿åºåˆ—æ•°æ®ã€‚

---

## 6. å…³é”®æ´è§ | Key Insights

### 6.1 è®¡ç®—æ•ˆç‡ | Computational Efficiency
é€’å½’å…¬å¼é¿å…äº†é‡å¤è®¡ç®—ï¼Œæ¯ä¸ªæ—¶é—´æ­¥åªéœ€è¦å¸¸æ•°æ—¶é—´æ“ä½œï¼Œä½¿å¾—ç®—æ³•èƒ½å¤Ÿå®æ—¶å¤„ç†æ•°æ®æµã€‚

### 6.2 æ¦‚ç‡è§£é‡Š | Probabilistic Interpretation
æ¯ä¸ªæ­¥éª¤éƒ½æœ‰æ¸…æ™°çš„æ¦‚ç‡æ„ä¹‰ï¼š
- é¢„æµ‹æ­¥éª¤ï¼šåŸºäºåŠ¨åŠ›å­¦æ¨¡å‹ä¼ æ’­ä¸ç¡®å®šæ€§
- æ›´æ–°æ­¥éª¤ï¼šåŸºäºè§‚å¯Ÿè¯æ®ä¿®æ­£ä¿¡å¿µ

### 6.3 æ‰©å±•æ€§ | Extensibility
è¿™ä¸ªæ¡†æ¶å¯ä»¥æ‰©å±•åˆ°æ›´å¤æ‚çš„æ¨¡å‹ï¼Œå¦‚é«˜é˜¶é©¬å°”å¯å¤«è¿‡ç¨‹ã€è¿ç»­çŠ¶æ€ç©ºé—´ï¼ˆä½¿ç”¨å¡å°”æ›¼æ»¤æ³¢ï¼‰ç­‰ã€‚

# å­¦ä¹ ç¬”è®°ï¼šUncertainty over Time IIï¼ˆä¿®è®¢ç‰ˆï¼‰

## 1. æ—¶é—´æ¨¡å‹ä¸­çš„æ¨ç† (Inference in Temporal Models)

### 1.1 å¹³æ»‘ (Smoothing)

**é€šä¿—è§£é‡Š**ï¼šå¹³æ»‘å°±åƒ"å›å¤´çœ‹"â€”â€”å½“æˆ‘ä»¬æœ‰äº†å®Œæ•´çš„è§‚æµ‹æ•°æ®åºåˆ—åï¼Œé‡æ–°è¯„ä¼°è¿‡å»æŸä¸ªæ—¶åˆ»çš„çŠ¶æ€æ¦‚ç‡ã€‚è¿™æ¯”ä»…åŸºäºå½“æ—¶ä¿¡æ¯çš„è¿‡æ»¤æ›´å‡†ç¡®ã€‚

**æ•°å­¦å®šä¹‰**ï¼šå¹³æ»‘è®¡ç®— $P(X_k \mid e_{1:t})$ï¼Œå³åœ¨ç»™å®šä»æ—¶åˆ»1åˆ°tçš„å®Œæ•´è§‚æµ‹åºåˆ—æ¡ä»¶ä¸‹ï¼Œè¿‡å»æŸä¸ªæ—¶åˆ»kçš„çŠ¶æ€åéªŒæ¦‚ç‡åˆ†å¸ƒã€‚

**å®Œæ•´æ•°å­¦æ¨å¯¼**ï¼š

**æ­¥éª¤1ï¼šåŸºç¡€è´å¶æ–¯åˆ†è§£**
$$P(X_k \mid e_{1:t}) = \frac{P(X_k, e_{1:t})}{P(e_{1:t})} = \alpha P(X_k, e_{1:t})$$

**æ­¥éª¤2ï¼šè¯æ®åºåˆ—åˆ†å‰²**
å°†è¯æ®åºåˆ—åˆ†ä¸ºä¸‰éƒ¨åˆ†ï¼š$e_{1:k-1}$, $e_k$, $e_{k+1:t}$
$$P(X_k, e_{1:t}) = P(X_k, e_{1:k}, e_{k+1:t})$$

**æ­¥éª¤3ï¼šæ¡ä»¶ç‹¬ç«‹æ€§åº”ç”¨**
åˆ©ç”¨é©¬å°”å¯å¤«æ€§è´¨ï¼š
$$P(e_{k+1:t} \mid X_k, e_{1:k}) = P(e_{k+1:t} \mid X_k)$$

å› æ­¤ï¼š
$$P(X_k, e_{1:t}) = P(X_k \mid e_{1:k}) \cdot P(e_{1:k}) \cdot P(e_{k+1:t} \mid X_k)$$

**æ­¥éª¤4ï¼šæœ€ç»ˆå¹³æ»‘å…¬å¼**
$$P(X_k \mid e_{1:t}) = \alpha P(X_k \mid e_{1:k}) \cdot P(e_{k+1:t} \mid X_k)$$

å…¶ä¸­ï¼š
- $\alpha$ æ˜¯å½’ä¸€åŒ–å¸¸æ•°
- $P(X_k \mid e_{1:k})$ æ˜¯å‰å‘æ¶ˆæ¯ï¼ˆé€šè¿‡è¿‡æ»¤å¾—åˆ°ï¼‰
- $P(e_{k+1:t} \mid X_k)$ æ˜¯åå‘æ¶ˆæ¯

### 1.2 åå‘æ¶ˆæ¯çš„é€’å½’æ¨å¯¼

**å®šä¹‰åå‘æ¶ˆæ¯**ï¼š$b_{k:t} = P(e_{k:t} \mid X_{k-1})$

**é€’å½’å…³ç³»æ¨å¯¼**ï¼š
$$b_{k:t} = P(e_{k:t} \mid X_{k-1})$$
$$= \sum_{x_k} P(e_k, e_{k+1:t}, x_k \mid X_{k-1})$$
$$= \sum_{x_k} P(e_k \mid x_k) \cdot P(e_{k+1:t} \mid x_k) \cdot P(x_k \mid X_{k-1})$$

**æœ€ç»ˆé€’å½’å…¬å¼**ï¼š
$$b_{k:t} = \sum_{x_k} P(e_k \mid x_k) \cdot b_{k+1:t} \cdot P(x_k \mid X_{k-1})$$

**è¾¹ç•Œæ¡ä»¶**ï¼š$b_{t+1:t} = P(\emptyset \mid X_t) = 1$

### 1.3 é›¨ä¼ä¾‹å­è¯¦ç»†æ•°å­¦è®¡ç®—

**é—®é¢˜å‚æ•°**ï¼š
- åˆå§‹æ¦‚ç‡ï¼š$P(R_0) = âŸ¨0.5, 0.5âŸ©$
- è½¬ç§»æ¦‚ç‡ï¼š$T = \begin{bmatrix} 0.7 & 0.3 \\ 0.3 & 0.7 \end{bmatrix}$
- è§‚æµ‹æ¦‚ç‡ï¼š$P(u|Rain) = 0.9$, $P(u|Â¬Rain) = 0.2$

**å·²çŸ¥å‰å‘è®¡ç®—ç»“æœ**ï¼š
$$P(R_1 \mid u_1) = âŸ¨0.818, 0.182âŸ©$$

**è®¡ç®—åå‘æ¶ˆæ¯** $b_{2:2} = P(u_2 \mid R_1)$ï¼š

å¯¹äº $R_1 = Rain$ï¼š
$$P(u_2 \mid Rain) = P(u_2 \mid R_2=Rain)P(R_2=Rain \mid Rain) + P(u_2 \mid R_2=Â¬Rain)P(R_2=Â¬Rain \mid Rain)$$
$$= 0.9 Ã— 0.7 + 0.2 Ã— 0.3 = 0.69$$

å¯¹äº $R_1 = Â¬Rain$ï¼š
$$P(u_2 \mid Â¬Rain) = P(u_2 \mid R_2=Rain)P(R_2=Rain \mid Â¬Rain) + P(u_2 \mid R_2=Â¬Rain)P(R_2=Â¬Rain \mid Â¬Rain)$$
$$= 0.9 Ã— 0.3 + 0.2 Ã— 0.7 = 0.41$$

**å¹³æ»‘è®¡ç®—**ï¼š
$$P(R_1 \mid u_1,u_2) = \alpha Ã— P(R_1 \mid u_1) Ã— P(u_2 \mid R_1)$$
$$= \alpha Ã— âŸ¨0.818, 0.182âŸ© Ã— âŸ¨0.69, 0.41âŸ©$$
$$= \alpha Ã— âŸ¨0.564, 0.075âŸ©$$

**å½’ä¸€åŒ–**ï¼š
$$\alpha = \frac{1}{0.564 + 0.075} = \frac{1}{0.639} â‰ˆ 1.565$$
$$P(R_1 \mid u_1,u_2) = âŸ¨0.564Ã—1.565, 0.075Ã—1.565âŸ© = âŸ¨0.883, 0.117âŸ©$$

### 1.4 ç»´ç‰¹æ¯”ç®—æ³•çš„æ•°å­¦æ¨å¯¼

**ç›®æ ‡**ï¼šæ‰¾åˆ°æœ€å¯èƒ½çš„çŠ¶æ€åºåˆ— $\arg\max_{x_{1:t}} P(x_{1:t} \mid e_{1:t})$

**æ¦‚ç‡åˆ†è§£**ï¼š
$$P(x_{1:t} \mid e_{1:t}) = \frac{P(x_{1:t}, e_{1:t})}{P(e_{1:t})} = \alpha P(x_{1:t}, e_{1:t})$$

**è”åˆæ¦‚ç‡åˆ†è§£**ï¼š
$$P(x_{1:t}, e_{1:t}) = P(x_1)P(e_1 \mid x_1) \prod_{k=2}^t P(x_k \mid x_{k-1})P(e_k \mid x_k)$$

**å®šä¹‰æœ€å¤§æ¦‚ç‡**ï¼š
ä»¤ $m_{1:t}(x_t) = \max_{x_{1:t-1}} P(x_{1:t}, e_{1:t})$

**é€’å½’å…³ç³»æ¨å¯¼**ï¼š
$$m_{1:t}(x_t) = \max_{x_{1:t-1}} \left[ P(x_{1:t-1}, e_{1:t-1}) \cdot P(x_t \mid x_{t-1}) \cdot P(e_t \mid x_t) \right]$$
$$= P(e_t \mid x_t) \cdot \max_{x_{t-1}} \left[ P(x_t \mid x_{t-1}) \cdot \max_{x_{1:t-2}} P(x_{1:t-1}, e_{1:t-1}) \right]$$
$$= P(e_t \mid x_t) \cdot \max_{x_{t-1}} \left[ P(x_t \mid x_{t-1}) \cdot m_{1:t-1}(x_{t-1}) \right]$$

**æœ€ç»ˆé€’å½’å…¬å¼**ï¼š
$$m_{1:t}(x_t) = P(e_t \mid x_t) \cdot \max_{x_{t-1}} \left[ P(x_t \mid x_{t-1}) \cdot m_{1:t-1}(x_{t-1}) \right]$$

**åˆå§‹æ¡ä»¶**ï¼š$m_{1:1}(x_1) = P(x_1)P(e_1 \mid x_1)$

## 2. éšé©¬å°”å¯å¤«æ¨¡å‹ (Hidden Markov Models, HMM)

### 2.1 HMMçš„çŸ©é˜µè¡¨ç¤ºæ•°å­¦æ¨å¯¼

**çŠ¶æ€è½¬ç§»çŸ©é˜µ**ï¼š
$$T_{i,j} = P(X_t = j \mid X_{t-1} = i)$$

**è§‚æµ‹æ¦‚ç‡çŸ©é˜µ**ï¼š
å¯¹äºç¦»æ•£è§‚æµ‹ï¼š$O_{i,k} = P(E_t = k \mid X_t = i)$
å¯¹äºè¿ç»­è§‚æµ‹ï¼šä½¿ç”¨æ¦‚ç‡å¯†åº¦å‡½æ•°

**å‰å‘ç®—æ³•çš„çŸ©é˜µå½¢å¼**ï¼š
$$f_{1:t} = \alpha \cdot O_t \cdot T^T \cdot f_{1:t-1}$$

å…¶ä¸­ï¼š
- $f_{1:t}$ æ˜¯æ—¶åˆ»tçš„å‰å‘æ¦‚ç‡å‘é‡
- $O_t$ æ˜¯ä»¥ $P(e_t \mid X_t=i)$ ä¸ºå¯¹è§’å…ƒç´ çš„å¯¹è§’çŸ©é˜µ
- $T$ æ˜¯è½¬ç§»çŸ©é˜µ

### 2.2 å›ºå®šæ»åå¹³æ»‘çš„æ•°å­¦æ¨å¯¼

**ç›®æ ‡**ï¼šè®¡ç®— $P(X_{t-d} \mid e_{1:t})$

**æ•°å­¦æ¨å¯¼**ï¼š
$$P(X_{t-d} \mid e_{1:t}) = \alpha P(X_{t-d} \mid e_{1:t-d}) \cdot P(e_{t-d+1:t} \mid X_{t-d})$$

**åå‘å˜æ¢çŸ©é˜µ**ï¼š
å®šä¹‰ $B_{t-d:t} = P(e_{t-d+1:t} \mid X_{t-d})$

**é€’å½’æ›´æ–°**ï¼š
$$B_{t-d:t} = P(e_{t-d+1} \mid X_{t-d+1}) \cdot P(X_{t-d+1} \mid X_{t-d}) \cdot B_{t-d+1:t}$$
$$= O_{t-d+1} \cdot T \cdot B_{t-d+1:t}$$

**çŸ©é˜µå½¢å¼**ï¼š
$$B_{t-d:t} = \prod_{k=t-d+1}^t O_k \cdot T$$

### 2.3 æœºå™¨äººå®šä½çš„æ•°å­¦æ¨¡å‹

**çŠ¶æ€ç©ºé—´**ï¼š42ä¸ªç½‘æ ¼ä½ç½®ï¼ŒçŠ¶æ€å˜é‡ $X_t \in \{1, 2, ..., 42\}$

**è§‚æµ‹æ¨¡å‹**ï¼š
$$P(E_t = e \mid X_t = x) = \prod_{d=1}^4 P(E_t^d = e^d \mid X_t = x)$$

å…¶ä¸­ $e^d$ è¡¨ç¤ºç¬¬dä¸ªæ–¹å‘çš„éšœç¢ç‰©ä¿¡æ¯ï¼ˆN, E, S, Wï¼‰

**ä¼ æ„Ÿå™¨è¯¯å·®æ¨¡å‹**ï¼š
$$P(E_t^d = \text{correct} \mid X_t) = 1 - \epsilon$$
$$P(E_t^d = \text{incorrect} \mid X_t) = \epsilon$$

## 3. æ ¸å¿ƒæ•°å­¦ç†è®ºæ€»ç»“

### 3.1 æ¡ä»¶ç‹¬ç«‹æ€§çš„å…³é”®ä½œç”¨

**ä¸€é˜¶é©¬å°”å¯å¤«å‡è®¾**ï¼š
$$P(X_t \mid X_{1:t-1}) = P(X_t \mid X_{t-1})$$

**è§‚æµ‹ç‹¬ç«‹æ€§**ï¼š
$$P(E_t \mid X_{1:t}, E_{1:t-1}) = P(E_t \mid X_t)$$

### 3.2 æ¦‚ç‡å½’ä¸€åŒ–çš„æ•°å­¦å¤„ç†

å¯¹äºä»»æ„éå½’ä¸€åŒ–æ¦‚ç‡å‘é‡ $v = âŸ¨v_1, v_2, ..., v_nâŸ©$ï¼Œå½’ä¸€åŒ–å…¬å¼ä¸ºï¼š
$$\text{Normalize}(v) = \left\langle \frac{v_1}{\sum v_i}, \frac{v_2}{\sum v_i}, ..., \frac{v_n}{\sum v_i} \right\rangle$$


# L25-L27 è‡ªåŠ¨åŒ–è§„åˆ’å­¦ä¹ ç¬”è®°  
**Artificial Intelligence Foundation - Automated Planning Study Notes**  

---

## ä¸€ã€æœç´¢ä¸è§„åˆ’çš„åŒºåˆ«ï¼ˆSearch vs. Planningï¼‰  
### 1.1 æ ¸å¿ƒå·®å¼‚ï¼ˆCore Differencesï¼‰  
- **æœç´¢ï¼ˆSearchï¼‰**  
  - çŠ¶æ€ç©ºé—´æœç´¢ï¼šé€šè¿‡æšä¸¾æ‰€æœ‰å¯èƒ½è·¯å¾„å¯»æ‰¾ç›®æ ‡ï¼ˆe.g. BFS, DFSï¼‰ã€‚  
  - *Search in state space: Enumerate all possible paths to reach the goal (e.g., BFS, DFS).*  
  - é—®é¢˜è¡¨ç¤ºä¸º**çŠ¶æ€è½¬ç§»å›¾**ï¼ˆState Transition Graphï¼‰ï¼Œæ¯ä¸ªèŠ‚ç‚¹æ˜¯å®Œæ•´çŠ¶æ€ï¼Œè¾¹æ˜¯åŠ¨ä½œã€‚  
    *Problems are represented as a state transition graph where each node is a full state and edges are actions.*  

- **è§„åˆ’ï¼ˆPlanningï¼‰**  
  - **é€»è¾‘å‘½é¢˜è¡¨ç¤º**ï¼šç”¨è°“è¯é€»è¾‘æè¿°çŠ¶æ€ã€åŠ¨ä½œå’Œç›®æ ‡ï¼ˆe.g. STRIPS, PDDLï¼‰ã€‚  
    *Logical representation: Use predicate logic to describe states, actions, and goals (e.g., STRIPS, PDDL).*  
  - å…³æ³¨**éƒ¨åˆ†å¯è§‚å¯Ÿæ€§**å’Œ**åŠ¨ä½œåˆ†è§£**ï¼ˆe.g. ä¹°ä¹¦é—®é¢˜åªéœ€ç›´æ¥æ‰§è¡ŒBuy(1234567890)ï¼‰ã€‚  
    *Focuses on partial observability and action decomposition (e.g., in the book-buying problem, only Buy(1234567890) is needed).*  

**ä¾‹å­**ï¼š  
- **æœç´¢é—®é¢˜**ï¼šè¶…å¸‚è´­ç‰©æ—¶ç©·ä¸¾æ‰€æœ‰å¯èƒ½è·¯å¾„ï¼ŒåŒ…æ‹¬æ— å…³åŠ¨ä½œï¼ˆå¦‚ä¹°å® ç‰©ç‹—ï¼‰ã€‚  
  *Search problem: Enumerate all possible paths in a supermarket, including irrelevant actions (e.g., buying a pet dog).*  
- **è§„åˆ’é—®é¢˜**ï¼šç›´æ¥æ¨ç†å‡ºâ€œä¹°ä¹¦â€åŠ¨ä½œæ»¡è¶³ç›®æ ‡ï¼Œæ— éœ€éå†æ— å…³è·¯å¾„ã€‚  
  *Planning problem: Directly infer that the "Buy" action satisfies the goal without exploring irrelevant paths.*  

---

## äºŒã€è§„åˆ’é—®é¢˜è¡¨ç¤ºï¼ˆPlanning Problem Representationï¼‰  
### 2.1 STRIPS è¡¨ç¤ºæ³•  
**å®šä¹‰**ï¼ˆDefinitionï¼‰ï¼š  
STRIPSï¼ˆStanford Research Institute Problem Solverï¼‰é€šè¿‡ä»¥ä¸‹ä¸‰è¦ç´ å®šä¹‰è§„åˆ’é—®é¢˜ï¼š  
1. **çŠ¶æ€**ï¼ˆStateï¼‰ï¼šæ­£æ–‡å­—åˆå–ï¼ˆe.g., `At(P1, JFK) âˆ§ At(C1, SFO)`ï¼‰ã€‚  
   *A conjunction of positive literals (e.g., `At(P1, JFK) âˆ§ At(C1, SFO)`).*  
2. **åŠ¨ä½œ**ï¼ˆActionï¼‰ï¼šåŒ…å«**å‰ææ¡ä»¶**ï¼ˆPreconditionï¼‰å’Œ**æ•ˆæœ**ï¼ˆEffectï¼‰ã€‚  
   *Actions include preconditions and effects.*  
3. **ç›®æ ‡**ï¼ˆGoalï¼‰ï¼šéƒ¨åˆ†æŒ‡å®šçš„çŠ¶æ€ï¼ˆe.g., `At(C1, JFK) âˆ§ At(C2, SFO)`ï¼‰ã€‚  
   *A partially specified state (e.g., `At(C1, JFK) âˆ§ At(C2, SFO)`).*  

**åŠ¨ä½œå…¬å¼**ï¼ˆAction Schemaï¼‰ï¼š  
```lisp
Action(Fly(p, from, to),  
       PRECOND: At(p, from) âˆ§ Plane(p) âˆ§ Airport(from) âˆ§ Airport(to),  
       EFFECT: Â¬At(p, from) âˆ§ At(p, to))
```

**è¯­ä¹‰**ï¼ˆSemanticsï¼‰ï¼š  
- **é€‚ç”¨æ€§**ï¼šå½“å‰çŠ¶æ€æ»¡è¶³å‰ææ¡ä»¶æ—¶ï¼ŒåŠ¨ä½œå¯æ‰§è¡Œã€‚  
  *Applicable if the current state satisfies the preconditions.*  
- **çŠ¶æ€è½¬æ¢**ï¼š  
  - æ·»åŠ æ•ˆæœä¸­çš„æ­£æ–‡å­—ï¼ˆAdd positive literalsï¼‰ã€‚  
  - åˆ é™¤æ•ˆæœä¸­çš„è´Ÿæ–‡å­—ï¼ˆRemove negative literalsï¼‰ã€‚  
  - **STRIPSå‡è®¾**ï¼šæœªæåŠçš„æ¡ä»¶ä¿æŒä¸å˜ï¼ˆè§£å†³å¸§é—®é¢˜ï¼‰ã€‚  
    *STRIPS assumption: Unmentioned conditions remain unchanged (solves the frame problem).*  

**ä¾‹å­**ï¼š  
- **åˆå§‹çŠ¶æ€**ï¼ˆInitial Stateï¼‰ï¼š  
  ```lisp
  At(C1, SFO) âˆ§ At(C2, JFK) âˆ§ At(P1, SFO) âˆ§ At(P2, JFK)
  ```  
- **ç›®æ ‡çŠ¶æ€**ï¼ˆGoal Stateï¼‰ï¼š  
  ```lisp
  At(C1, JFK) âˆ§ At(C2, SFO)
  ```  
- **åŠ¨ä½œåºåˆ—**ï¼ˆPlanï¼‰ï¼š  
  1. `Load(C1, P1, SFO)`  
  2. `Fly(P1, SFO, JFK)`  
  3. `Unload(C1, P1, JFK)`  
  4. `Load(C2, P2, JFK)`  
  5. `Fly(P2, JFK, SFO)`  
  6. `Unload(C2, P2, SFO)`  

---

### 2.2 PDDL è¡¨ç¤ºæ³•  
**å®šä¹‰**ï¼ˆDefinitionï¼‰ï¼š  
PDDLï¼ˆPlanning Domain Definition Languageï¼‰æ˜¯ç»å…¸è§„åˆ’çš„æ ‡å‡†è¯­è¨€ï¼Œåˆ†ä¸ºä¸¤ä¸ªæ–‡ä»¶ï¼š  
1. **Domain æ–‡ä»¶**ï¼šå®šä¹‰è°“è¯ã€å¸¸é‡å’ŒåŠ¨ä½œã€‚  
   *Defines predicates, constants, and actions.*  
2. **Problem æ–‡ä»¶**ï¼šå®šä¹‰å¯¹è±¡ã€åˆå§‹çŠ¶æ€å’Œç›®æ ‡ã€‚  
   *Defines objects, initial state, and goal.*  

**ç¤ºä¾‹**ï¼ˆExample - Gripper Taskï¼‰ï¼š  
```lisp
; Domain æ–‡ä»¶  
(define (domain gripper)  
  (:predicates (room ?r) (ball ?b) (at-robby ?r) (at ?b ?r) (free ?g) (carry ?o ?g))  
  (:action move  
    :parameters (?from ?to)  
    :precondition (and (room ?from) (room ?to) (at-robby ?from))  
    :effect (and (at-robby ?to) (not (at-robby ?from))))  
  (:action pick  
    :parameters (?obj ?room ?gripper)  
    :precondition (and (ball ?obj) (room ?room) (at ?obj ?room) (at-robby ?room) (free ?gripper))  
    :effect (and (carry ?obj ?gripper) (not (at ?obj ?room)) (not (free ?gripper))))  
  (:action drop  
    :parameters (?obj ?room ?gripper)  
    :precondition (and (ball ?obj) (room ?room) (carry ?obj ?gripper) (at-robby ?room))  
    :effect (and (at ?obj ?room) (free ?gripper) (not (carry ?obj ?gripper))))  
)

; Problem æ–‡ä»¶  
(define (problem gripper-four-balls)  
  (:domain gripper)  
  (:objects rooma roomb ball1 ball2 ball3 ball4 left right)  
  (:init (room rooma) (room roomb) (ball ball1) (ball ball2) (ball ball3) (ball ball4)  
         (gripper left) (gripper right) (at-robby rooma) (free left) (free right)  
         (at ball1 rooma) (at ball2 rooma) (at ball3 rooma) (at ball4 rooma))  
  (:goal (and (at ball1 roomb) (at ball2 roomb) (at ball3 roomb) (at ball4 roomb)))  
)
```

---

## ä¸‰ã€å‰å‘ä¸åå‘æœç´¢ï¼ˆForward and Backward Searchï¼‰  
### 3.1 å‰å‘æœç´¢ï¼ˆProgression Planningï¼‰  
- **è¿‡ç¨‹**ï¼šä»åˆå§‹çŠ¶æ€å‡ºå‘ï¼Œé€æ­¥åº”ç”¨åŠ¨ä½œç›´åˆ°æ»¡è¶³ç›®æ ‡ã€‚  
  *Start from the initial state and apply actions until the goal is met.*  
- **ä¼˜ç‚¹**ï¼šç›´è§‚ï¼Œé€‚åˆçŠ¶æ€ç©ºé—´è¾ƒå°çš„é—®é¢˜ã€‚  
  *Advantages: Intuitive, suitable for small state spaces.*  
- **ç¼ºç‚¹**ï¼šçŠ¶æ€ç©ºé—´çˆ†ç‚¸ï¼Œéœ€é«˜æ•ˆå¯å‘å¼å‡½æ•°ï¼ˆå¦‚A*ç®—æ³•ï¼‰ã€‚  
  *Disadvantages: State space explosion, requires efficient heuristics (e.g., A*).*  

**å¯å‘å¼å‡½æ•°**ï¼ˆHeuristic Functionï¼‰ï¼š  
- **å¿½ç•¥å‰ææ¡ä»¶**ï¼ˆIgnoring Precondition Heuristicï¼‰ï¼š  
  - å‡è®¾æ‰€æœ‰å‰ææ¡ä»¶å·²æ»¡è¶³ï¼Œè®¡ç®—æœ€å°‘åŠ¨ä½œæ•°ã€‚  
    *Assume all preconditions are satisfied, calculate the minimal number of actions.*  
- **æ›¼å“ˆé¡¿è·ç¦»**ï¼ˆManhattan Distanceï¼‰ï¼š  
  - é€‚ç”¨äºå—ä¸–ç•Œé—®é¢˜ï¼Œè®¡ç®—ç‰©ä½“ä½ç½®ä¸ç›®æ ‡çš„å·®å€¼ã€‚  
    *Used in the Blocks World, calculates the difference between object positions and goals.*  

---

### 3.2 åå‘æœç´¢ï¼ˆRegression Planningï¼‰  
- **è¿‡ç¨‹**ï¼šä»ç›®æ ‡çŠ¶æ€åå‘æ¨å¯¼ï¼Œä»…ä¿ç•™ç›¸å…³åŠ¨ä½œã€‚  
  *Start from the goal state and regress to the initial state, keeping only relevant actions.*  
- **å…³é”®æ­¥éª¤**ï¼ˆKey Stepsï¼‰ï¼š  
  1. **ç›®æ ‡å›å½’**ï¼ˆGoal Regressionï¼‰ï¼š  
     - å¯¹ç›®æ ‡ä¸­çš„æ¯ä¸ªå­ç›®æ ‡ï¼Œæ‰¾åˆ°å¯å®ç°çš„åŠ¨ä½œã€‚  
     *For each subgoal in the goal, find the actions that achieve it.*  
  2. **é¢„å¤„ç†çŠ¶æ€**ï¼ˆPredecessor Stateï¼‰ï¼š  
     - æ ¹æ®åŠ¨ä½œçš„å‰ææ¡ä»¶ç”Ÿæˆå‰é©±çŠ¶æ€ã€‚  
     *Generate predecessor states based on action preconditions.*  

**ä¾‹å­**ï¼ˆAir Cargo Transportationï¼‰ï¼š  
- **ç›®æ ‡**ï¼š`At(C1, JFK) âˆ§ At(C2, SFO)`  
- **åå‘æœç´¢æ­¥éª¤**ï¼š  
  1. é€‰æ‹© `Unload(C1, P1, JFK)` å®ç° `At(C1, JFK)`ã€‚  
  2. éœ€è¦å‰æ `In(C1, P1) âˆ§ At(P1, JFK)`ã€‚  
  3. åæ¨ `Load(C1, P1, SFO)` å’Œ `Fly(P1, SFO, JFK)`ã€‚  

---

## å››ã€è§„åˆ’å¤æ‚åº¦åˆ†æï¼ˆComplexity of Classical Planningï¼‰  
### 4.1 å¤æ‚åº¦ç±»ï¼ˆComplexity Classesï¼‰  
| å¤æ‚åº¦ç±» | æè¿° |  
|----------|------|  
| **NP-Hard** | éç¡®å®šæ€§å¤šé¡¹å¼æ—¶é—´éš¾é—®é¢˜ï¼ˆe.g., Plan-Existenceï¼‰ã€‚ |  
| **PSPACE-Complete** | å¤šé¡¹å¼ç©ºé—´å®Œå…¨é—®é¢˜ï¼ˆe.g., Plan-Length with ADLï¼‰ã€‚ |  
| **NEXPTIME** | éç¡®å®šæ€§æŒ‡æ•°æ—¶é—´é—®é¢˜ï¼ˆe.g., å¤šå±‚æ¡ä»¶æ•ˆæœè§„åˆ’ï¼‰ã€‚ |  

### 4.2 æ¡ä»¶å¯¹å¤æ‚åº¦çš„å½±å“ï¼ˆImpact of Featuresï¼‰  
| æ¡ä»¶ | å¤æ‚åº¦ |  
|------|--------|  
| å…è®¸è´Ÿæ•ˆæœï¼ˆNegative Effectsï¼‰ | NP-Hard â†’ PSPACE-Complete |  
| æ¡ä»¶æ•ˆæœï¼ˆConditional Effectsï¼‰ | PSPACE-Complete â†’ NEXPTIME |  
| å‡½æ•°ç¬¦å·ï¼ˆFunction Symbolsï¼‰ | ç»å…¸è§„åˆ’ â†’ éç»å…¸è§„åˆ’ï¼ˆéœ€æ›´å¤æ‚æ¨¡å‹ï¼‰ã€‚ |  

---

## äº”ã€è§„åˆ’å›¾ï¼ˆPlanning Graphï¼‰  
**å®šä¹‰**ï¼ˆDefinitionï¼‰ï¼š  
è§„åˆ’å›¾æ˜¯ä¸€ç§åˆ†å±‚æ•°æ®ç»“æ„ï¼Œç”¨äºé«˜æ•ˆè®¡ç®—å¯å‘å¼å‡½æ•°ï¼ˆå¦‚**å±‚çº§çŠ¶æ€è¯„ä¼°**ï¼‰ã€‚  
*A hierarchical data structure for efficiently computing heuristics (e.g., level-based state evaluation).*  

**æ„é€ è¿‡ç¨‹**ï¼ˆConstruction Stepsï¼‰ï¼š  
1. **åˆå§‹å±‚**ï¼ˆLevel 0ï¼‰ï¼šåŒ…å«åˆå§‹çŠ¶æ€çš„æ‰€æœ‰å‘½é¢˜ã€‚  
2. **åŠ¨ä½œå±‚**ï¼ˆAction Levelï¼‰ï¼šæ·»åŠ æ‰€æœ‰é€‚ç”¨åŠ¨ä½œã€‚  
3. **å‘½é¢˜å±‚**ï¼ˆProposition Levelï¼‰ï¼šç”±åŠ¨ä½œæ•ˆæœç”Ÿæˆæ–°å‘½é¢˜ã€‚  
4. **äº’æ–¥å…³ç³»**ï¼ˆMutual Exclusionï¼‰ï¼šæ ‡è®°å†²çªåŠ¨ä½œæˆ–å‘½é¢˜ã€‚  

**åº”ç”¨**ï¼ˆApplicationï¼‰ï¼š  
- **æœ€å¤§å±‚çº§æ•°**ï¼ˆMax Levelï¼‰ï¼šä¼°è®¡æœ€çŸ­è®¡åˆ’é•¿åº¦ã€‚  
  *The maximum level number estimates the minimal plan length.*  
- **äº’æ–¥è§£æ**ï¼ˆMutex Resolutionï¼‰ï¼šå‰ªæä¸å¯è¡Œè·¯å¾„ã€‚  

---

## å…­ã€æ€»ç»“ï¼ˆSummaryï¼‰  
1. **æœç´¢ vs. è§„åˆ’**ï¼šè§„åˆ’åˆ©ç”¨é€»è¾‘è¡¨ç¤ºå‡å°‘å†—ä½™æœç´¢ã€‚  
   *Planning reduces redundant search by using logical representations.*  
2. **è¡¨ç¤ºè¯­è¨€**ï¼šSTRIPS/PDDL æ˜¯ç»å…¸è§„åˆ’çš„æ ¸å¿ƒå·¥å…·ã€‚  
   *STRIPS/PDDL are core tools for classical planning.*  
3. **æœç´¢ç­–ç•¥**ï¼šå‰å‘æœç´¢é€‚åˆå°è§„æ¨¡é—®é¢˜ï¼Œåå‘æœç´¢æ›´é«˜æ•ˆã€‚  
   *Forward search suits small problems; backward search is more efficient.*  
4. **å¤æ‚åº¦**ï¼šè§„åˆ’é—®é¢˜çš„å¤æ‚åº¦å—å‰ææ¡ä»¶ã€æ•ˆæœç­‰æ¡ä»¶å½±å“ã€‚  
   *Complexity depends on preconditions, effects, etc.*  

--- 

**é™„å½•**ï¼ˆAppendixï¼‰ï¼š  
- **å—ä¸–ç•Œï¼ˆBlocks Worldï¼‰PDDL ç¤ºä¾‹**ï¼š  
  ```lisp
  (define (domain blocks)  
    (:predicates (on ?a ?b) (block ?b) (clear ?b))  
    (:action move  
      :parameters (?b ?x ?y)  
      :precondition (and (on ?b ?x) (clear ?y) (clear ?b) (block ?b) (block ?y))  
      :effect (and (on ?b ?y) (clear ?x) (not (on ?b ?x)) (not (clear ?y))))  
    (:action movetotable  
      :parameters (?b ?x)  
      :precondition (and (on ?b ?x) (clear ?b) (block ?b))  
      :effect (and (on ?b table) (clear ?x) (not (on ?b ?x))))  
  )
  ```


  